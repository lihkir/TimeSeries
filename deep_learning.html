
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>4. Deep Learning para la predicción de series temporales &#8212; Análisis y Predicción de Series de Tiempo</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css?v=b4b7a797" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'deep_learning';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/custom.js?v=14184634"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="5. Bibliografía" href="biblio.html" />
    <link rel="prev" title="3. Modelos autorregresivos integrados de media móvil" href="arima_model.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/fotolihki.jpg" class="logo__image only-light" alt="Análisis y Predicción de Series de Tiempo - Home"/>
    <script>document.write(`<img src="_static/fotolihki.jpg" class="logo__image only-dark" alt="Análisis y Predicción de Series de Tiempo - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Profesor: Dr. Lihki Rubio
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="statistical_background.html">1. Introducción a las Series de Tiempo</a></li>
<li class="toctree-l1"><a class="reference internal" href="exponential_smoothing.html">2. Métodos de Suavización Exponencial</a></li>
<li class="toctree-l1"><a class="reference internal" href="arima_model.html">3. Modelos autorregresivos integrados de media móvil</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">4. Deep Learning para la predicción de series temporales</a></li>
<li class="toctree-l1"><a class="reference internal" href="biblio.html">5. Bibliografía</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/deep_learning.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Deep Learning para la predicción de series temporales</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#redes-neuronales">4.1. Redes neuronales</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradiente-descendente">4.2. Gradiente descendente</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#el-perceptron">4.3. El perceptrón</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#redes-totalmente-conectadas">4.4. Redes Totalmente Conectadas</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#el-algoritmo-de-backpropagation">4.5. El Algoritmo De Backpropagation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#el-esquema-de-backpropagation-para-gradiente-descendente">4.6. El Esquema De Backpropagation Para Gradiente descendente</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#calculo-de-gradientes">4.7. Cálculo de gradientes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#calculo-de-delta-nj-r">4.8. Cálculo de <span class="math notranslate nohighlight">\(\delta_{nj}^{r}\)</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#las-capas-ocultas">4.9. Las capas ocultas</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#redes-neuronales-recurrentes">4.10. Redes Neuronales Recurrentes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#backpropagation-en-tiempo">4.10.1. Backpropagation en tiempo</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#desvanecimiento-y-explosion-de-gradientes">4.10.2. Desvanecimiento y explosión de gradientes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementacion">4.10.3. Implementación</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#red-de-memoria-a-largo-plazo-lstm">4.11. Red de memoria a largo plazo (LSTM)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#series-de-tiempo">4.12. Series de Tiempo</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#perceptrones-multicapa">4.13. Perceptrones multicapa</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#entrenamiento-de-mlp">4.14. Entrenamiento de MLP</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mlp-para-la-prediccion-de-series-temporales">4.15. MLP para la predicción de series temporales</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lstm-para-la-prediccion-de-series-de-tiempo">4.16. LSTM para la predicción de series de tiempo</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="deep-learning-para-la-prediccion-de-series-temporales">
<h1><span class="section-number">4. </span>Deep Learning para la predicción de series temporales<a class="headerlink" href="#deep-learning-para-la-prediccion-de-series-temporales" title="Link to this heading">#</a></h1>
<section id="redes-neuronales">
<h2><span class="section-number">4.1. </span>Redes neuronales<a class="headerlink" href="#redes-neuronales" title="Link to this heading">#</a></h2>
<div class="admonition-introduccion admonition">
<p class="admonition-title">Introducción</p>
<ul class="simple">
<li><p>Las redes neuronales son <em><strong>sistemas de aprendizaje compuestos por neuronas conectadas en capas que ajustan sus conexiones para aprender</strong></em>. Tras un período de <em><strong>25 años desde su inicio, las redes neuronales se convirtieron en la norma en el aprendizaje automático</strong></em>.</p></li>
<li><p>En un principio, dominaron durante una década, pero <em><strong>luego fueron superadas por máquinas de vectores de soporte</strong></em>. Sin embargo, <em><strong>desde 2010, las redes neuronales profundas se han vuelto populares gracias a mejoras en la tecnología y la disponibilidad de grandes conjuntos de datos</strong></em>, impulsando el campo del aprendizaje automático.</p></li>
</ul>
</div>
</section>
<section id="gradiente-descendente">
<h2><span class="section-number">4.2. </span>Gradiente descendente<a class="headerlink" href="#gradiente-descendente" title="Link to this heading">#</a></h2>
<ul>
<li><p>El <em><strong>método de gradiente descendente</strong></em> es uno de los mas ampliamente usados para la <em><strong>minimización iterativa de una función de costo diferenciable</strong></em>, <span class="math notranslate nohighlight">\(J(\boldsymbol{\theta}),~\boldsymbol{\theta}\in\mathbb{R}^{l}\)</span> (<em>por ejemplo MSE</em>). Como cualquier otra técnica iterativa, el método <em><strong>parte de una estimación inicial</strong></em>, <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(0)}\)</span>, <em><strong>y genera una sucesión</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(i)},~i=1,2,\dots,\)</span> tal que:</p>
<div class="math notranslate nohighlight">
\[
    \boldsymbol{\theta}^{(i)}=\boldsymbol{\theta}^{(i-1)}+\mu_{i}\Delta\boldsymbol{\theta}^{(i)},~ i &gt;0,~\mu_{i}&gt;0.
    \]</div>
</li>
<li><p>La diferencia entre cada método radica en la forma que <span class="math notranslate nohighlight">\(\mu_{i}\)</span> y <span class="math notranslate nohighlight">\(\Delta\boldsymbol{\theta}^{(i)}\)</span> son seleccionados. <span class="math notranslate nohighlight">\(\Delta\boldsymbol{\theta}^{(i)}\)</span> es conocido como la <em><strong>dirección de actualización o de búsqueda</strong></em>. La sucesión <span class="math notranslate nohighlight">\(\mu_{i}\)</span> es conocida como el <em><strong>tamaño o longitud de paso</strong></em> en la <span class="math notranslate nohighlight">\(i\)</span>-ésima iteración, estos valores pueden ser constantes o cambiar.</p></li>
<li><p>En el método de gradiente descendente, <em><strong>la selección de</strong></em> <span class="math notranslate nohighlight">\(\Delta\boldsymbol{\theta}^{(i)}\)</span> <em><strong>es realizada para garantizar que</strong></em> <span class="math notranslate nohighlight">\(J(\boldsymbol{\theta}^{(i)})&lt;J(\boldsymbol{\theta}^{(i-1)})\)</span>, excepto en el minimizador <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{\star}\)</span>.</p></li>
</ul>
<figure class="align-center" id="curva-nivel">
<a class="reference internal image-reference" href="_images/curva_nivel.png"><img alt="_images/curva_nivel.png" src="_images/curva_nivel.png" style="width: 504.8px; height: 386.40000000000003px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.1 </span><span class="caption-text">Función de coste en el espacio de parámetros bidimensional. Fuente <span id="id1">[<a class="reference internal" href="biblio.html#id31" title="S. Theodoridis. Machine Learning: A Bayesian and Optimization Perspective. Elsevier Science, 2020. ISBN 9780128188040. URL: https://books.google.com.co/books?id=l-nEDwAAQBAJ.">Theodoridis, 2020</a>]</span>.</span><a class="headerlink" href="#curva-nivel" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>Suponga que en la iteración <span class="math notranslate nohighlight">\(i-1\)</span> el valor <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(i-1)}\)</span> <em><strong>ha sido obtenido</strong></em></p></li>
</ul>
<div class="math notranslate nohighlight">
\[
J(\boldsymbol{\theta}^{(i)})=J(\boldsymbol{\theta}^{(i-1)}+\mu_{i}\Delta\boldsymbol{\theta}^{(i)})\approx J(\boldsymbol{\theta}^{(i-1)})+\mu_{i}\cdot\nabla^{T}J(\boldsymbol{\theta}^{(i-1)})\Delta\boldsymbol{\theta}^{(i-1)}.
\]</div>
<ul class="simple">
<li><p>Nótese que <em><strong>seleccionando la dirección tal que</strong></em> <span class="math notranslate nohighlight">\(\nabla^{T}J(\boldsymbol{\theta}^{(i-1)})\Delta\boldsymbol{\theta}^{(i)}&lt;0\)</span>, <em><strong>garantizará que</strong></em> <span class="math notranslate nohighlight">\(J(\boldsymbol{\theta}^{(i-1)}+\mu_{i}\Delta\boldsymbol{\theta}^{(i)})&lt;J(\boldsymbol{\theta}^{(i-1)})\)</span>. Tal selección de <span class="math notranslate nohighlight">\(\Delta\boldsymbol{\theta}^{(i)}\)</span> y <span class="math notranslate nohighlight">\(\nabla J(\boldsymbol{\theta}^{(i-1)})\)</span> debe formar un <em><strong>ángulo obtuso</strong></em>. Las curvas de nivel asociadas a <span class="math notranslate nohighlight">\(J(\boldsymbol{\theta})\)</span> pueden tomar cualquier forma, la cual va a <em><strong>depender de como está definido</strong></em> <span class="math notranslate nohighlight">\(J(\boldsymbol{\theta})\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(J(\boldsymbol{\theta})\)</span> se supone diferenciable, por lo tanto, las <em><strong>curvas de nivel o contornos deben ser suaves y aceptar un plano tangente en cualquier punto</strong></em>. Además, de los cursos de cálculo sabemos que el <em><strong>vector gradiente</strong></em> <span class="math notranslate nohighlight">\(\nabla J(\boldsymbol{\theta})\)</span> <em><strong>es perpendicular al plano tangente</strong></em> (recta tangente) <em><strong>a la correspondiente curva de nivel en el punto</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>.</p></li>
<li><p>Nótese que <em><strong>seleccionando la dirección de búsqueda</strong></em> <span class="math notranslate nohighlight">\(\Delta\boldsymbol{\theta}^{(i)}\)</span> <em><strong>que forma un angulo obtuso con el gradiente, se coloca a</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(i-1)}+\mu_{i}\Delta\boldsymbol{\theta}^{(i)}\)</span> <em><strong>en un punto sobre el contorno el cual corresponde a un valor menor que</strong></em> <span class="math notranslate nohighlight">\(J(\boldsymbol{\theta})\)</span>.</p></li>
</ul>
<ul class="simple">
<li><p>Dos problemas surgen ahora:</p>
<ol class="arabic simple">
<li><p>Escoger la <em><strong>mejor dirección de búsqueda</strong></em></p></li>
<li><p>Calcular <em><strong>que tan lejos es aceptable un movimiento a traves de esta dirección</strong></em>.</p></li>
</ol>
</li>
</ul>
<figure class="align-center" id="maximun-dec-cost-function">
<a class="reference internal image-reference" href="_images/maximun_dec_cost_function.png"><img alt="_images/maximun_dec_cost_function.png" src="_images/maximun_dec_cost_function.png" style="width: 495.20000000000005px; height: 365.6px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.2 </span><span class="caption-text">El vector gradiente en un punto <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> es perpendicular al plano tangente (línea punteada) en la curva de nivel que cruza <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>. La dirección de descenso forma un ángulo obtuso, <span class="math notranslate nohighlight">\(\phi\)</span>, con el vector gradiente.</span><a class="headerlink" href="#maximun-dec-cost-function" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>Nótese que <em><strong>si</strong></em> <span class="math notranslate nohighlight">\(\mu_{i}\|\Delta\boldsymbol{\theta}^{(i)}\|\)</span> <em><strong>es demasiado grande, entonces el nuevo punto puede ser colocado en un contorno correspondiente a un valor mayor al del actual</strong></em> contorno.</p></li>
</ul>
<figure class="align-center" id="curva-nivel-cost-function">
<a class="reference internal image-reference" href="_images/curva_nivel_cost_function.png"><img alt="_images/curva_nivel_cost_function.png" src="_images/curva_nivel_cost_function.png" style="width: 484.2px; height: 359.1px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.3 </span><span class="caption-text">Las correspondientes curvas de nivel para la función de coste, en el plano bidimensional. Nótese que a medida que nos alejamos del valor óptimo, <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{\star}\)</span>, los valores de <span class="math notranslate nohighlight">\(c\)</span> aumentan.</span><a class="headerlink" href="#curva-nivel-cost-function" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>Para abordar (1), <em><strong>supongamos que</strong></em> <span class="math notranslate nohighlight">\(\mu_{i}=1\)</span> y <em><strong>buscamos todos los vectores</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{z}\)</span> <em><strong>con norma Euclidiana unitaria, con inicio (cola) en</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(i-1)}\)</span>. Entonces, para todas las posibles direcciones, la que entrega el valor más negativo del producto interno, <span class="math notranslate nohighlight">\(\nabla^{T}J(\boldsymbol{\theta}^{(i-1)})z\)</span>, es aquella de gradiente negativo</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
z=-\frac{\nabla J(\boldsymbol{\theta}^{(i-1)})}{\|\nabla J(\boldsymbol{\theta}^{(i-1)}\|}
\]</div>
<ul class="simple">
<li><p>Centrando <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(i-1)}\)</span> en la bola con norma Euclideana uno. <em><strong>De todos los vectores con norma unitaria y origen en</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(i-1)}\)</span>, <em><strong>seleccionamos aquel que apunta en la dirección negativa del gradiente</strong></em>. Por lo tanto, para todos los vectores con norma Euclidiana 1, la <em><strong>dirección de descenso mas pronunciada coincide con la dirección del gradiente descendente, negativo</strong></em>, y la correspondiente actualización recursiva se convierte en</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\theta}^{(i)}=\boldsymbol{\theta}^{(i-1)}-\mu_{i}\nabla J(\boldsymbol{\theta}^{(i-1)}),\quad\text{Gradiente descendente}.
\]</div>
<figure class="align-center" id="fig-desc-gradient">
<a class="reference internal image-reference" href="_images/desc_gradient.png"><img alt="_images/desc_gradient.png" src="_images/desc_gradient.png" style="width: 480.6px; height: 352.8px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.4 </span><span class="caption-text">Representación del gradiente negativo, el cual conduce a la máxima disminución de la función de coste.</span><a class="headerlink" href="#fig-desc-gradient" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>La selección de <span class="math notranslate nohighlight">\(\mu_{i}\)</span> debe ser realizada de tal forma que <em><strong>garantice convergencia de la secuencia de minimización</strong></em>. Nótese que <em><strong>el algoritmo puede oscilar en torno al mínimo sin converger, si no seleccionamos la dirección correcta</strong></em>. La selección de <span class="math notranslate nohighlight">\(\mu_{i}\)</span> <em><strong>dependerá de la convergencia a cero del error entre</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(i)}\)</span> <em><strong>y el mínimo real en forma de serie geométrica</strong></em>.</p></li>
<li><p>Por ejemplo, para el caso de la función de coste del error cuadrático medio, la longitud de paso está dada por: <span class="math notranslate nohighlight">\(0&lt;\mu&lt;2/\lambda_{\max}\)</span>, donde <span class="math notranslate nohighlight">\(\lambda_{\max}\)</span> el máximo eigenvalor de la matriz de covarianza <span class="math notranslate nohighlight">\(\Sigma_{x}=\mathbb{E}[\boldsymbol{x}\boldsymbol{x}^{T}]\)</span>, donde <span class="math notranslate nohighlight">\(J(\boldsymbol{\theta})=\text{E}[(y-\boldsymbol{\theta}^{T}\boldsymbol{x})^{2}]\)</span> (ver <span id="id2">[<a class="reference internal" href="biblio.html#id31" title="S. Theodoridis. Machine Learning: A Bayesian and Optimization Perspective. Elsevier Science, 2020. ISBN 9780128188040. URL: https://books.google.com.co/books?id=l-nEDwAAQBAJ.">Theodoridis, 2020</a>]</span>) (<em><strong><code class="docutils literal notranslate"><span class="pre">bonus</span></code></strong></em>).</p></li>
</ul>
</section>
<section id="el-perceptron">
<h2><span class="section-number">4.3. </span>El perceptrón<a class="headerlink" href="#el-perceptron" title="Link to this heading">#</a></h2>
<ul>
<li><p>Nuestro punto de partida será considerar el problema simple de una <em><strong>tarea de clasificación conformada por dos clases linealmente separables</strong></em>. En otras palabras, dado un conjunto de muestras de entrenamiento, <span class="math notranslate nohighlight">\((y_{n}, \boldsymbol{x}_{n})\)</span>, <span class="math notranslate nohighlight">\(n=1,2,\dots,N\)</span>, con <span class="math notranslate nohighlight">\(y_{n}\in\{-1,+1\},~\boldsymbol{x}_{n}\in\mathbb{R}^{l}\)</span>, suponemos que existe un hiperplano</p>
<div class="math notranslate nohighlight">
\[
    \boldsymbol{\theta}_{\star}^{T}\boldsymbol{x}=0,
    \]</div>
<p>tal que,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{cases}
    \boldsymbol{\theta}_{\star}^{T}\boldsymbol{x}&amp;&gt;0,\quad\text{si}\quad\boldsymbol{x}\in\omega_{1}\\
    \boldsymbol{\theta}_{\star}^{T}\boldsymbol{x}&amp;&lt;0,\quad\text{si}\quad\boldsymbol{x}\in\omega_{2}
    \end{cases}
    \end{split}\]</div>
<p>en otras palabras, <em><strong>dicho hiperplano clasifica correctamente todos los puntos del conjunto de entrenamiento</strong></em>.</p>
</li>
<li><p>Para simplificar, el <em><strong>término de sesgo del hiperplano ha sido absorbido en</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{\star}\)</span> después de extender la dimensionalidad del espacio de entrada en uno. El objetivo ahora es <em><strong>desarrollar un algoritmo que calcule iterativamente un hiperplano que clasifique correctamente todos los patrones de ambas clases</strong></em>. Para ello, se adopta una función de costo.</p></li>
</ul>
<ul class="simple">
<li><p>Sea <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> la <em><strong>estimación del vector de parámetros desconocidos, disponible en la actual iteración</strong></em>. Entonces hay dos posibilidades. La primera es que <em><strong>todos los puntos estén clasificados correctamente</strong></em>; esto significa que se ha obtenido una solución. La otra alternativa es que <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> <em><strong>clasifique correctamente algunos de los puntos</strong></em> y <em><strong>el resto estén mal clasificados</strong></em>.</p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">Costo perceptrón</p>
<p>Sea <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> el conjunto de todas las muestras mal clasificadas. La <strong><code class="docutils literal notranslate"><span class="pre">función</span> <span class="pre">de</span> <span class="pre">costo,</span> <span class="pre">perceptrón</span></code></strong> se define como</p>
<div class="math notranslate nohighlight">
\[
J(\boldsymbol{\theta})=-\sum_{n:\boldsymbol{x}_{n}\in\mathcal{Y}}y_{n}\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}:\quad\textsf{Costo perceptrón},
\]</div>
<p>donde</p>
<div class="math notranslate nohighlight">
\[\begin{split}
y_{n}=
\begin{cases}
+1,&amp;\quad\text{si}~\boldsymbol{x}\in\omega_{1}\\
-1,&amp;\quad\text{si}~\boldsymbol{x}\in\omega_{2}.
\end{cases}
\end{split}\]</div>
</div>
<ul class="simple">
<li><p>Nótese que <em><strong>la función de costo es no negativa</strong></em>. Dado que la suma es sobre los puntos mal clasificados, si <span class="math notranslate nohighlight">\(\boldsymbol{x}_{n}\in\omega_{1}~(\omega_{2}),~\)</span> entonces <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}\leq (\geq)~0\)</span>, entregando así un producto <span class="math notranslate nohighlight">\(-y_{n}\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}\geq0\)</span>.</p></li>
<li><p><em><strong>La función de costo es cero, si no existen puntos mal clasificados</strong></em>, esto es, <span class="math notranslate nohighlight">\(\mathcal{Y}=\emptyset\)</span>. La función de costo perceptrón <em><strong>no es diferenciable en todos los puntos, es lineal por tramos</strong></em>. Si reescribimos <span class="math notranslate nohighlight">\(J(\boldsymbol{\theta})\)</span> en una forma ligeramente diferente:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
J(\boldsymbol{\theta})=\left(-\sum_{n:\boldsymbol{x}_{n}\in\mathcal{Y}}y_{n}\boldsymbol{x}_{n}^{T}\right)\boldsymbol{\theta}.
\]</div>
<ul class="simple">
<li><p><em><strong>Nótese que esta es una función lineal con respeto a</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>, siempre que el conjunto de puntos mal clasificados permanezca igual. Además, <em><strong>nótese que ligeros cambios del valor</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> <em><strong>corresponden a cambios de posición del respectivo hiperplano</strong></em>.</p></li>
<li><p>Como consecuencia, <em><strong>existirá un punto donde el número de muestras mal clasificadas en</strong></em> <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span>, <em><strong>repentinamente cambia</strong></em>; este es el tiempo donde <em><strong>una muestra en el conjunto de entrenamiento cambia su posición relativa con respecto al hiperplano en movimiento</strong></em>, y en consecuencia, el conjunto <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> es modificado. Después de este cambio, el conjunto, <span class="math notranslate nohighlight">\(J(\boldsymbol{\theta})\)</span>, corresponderá a una nueva  función lineal.</p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">El algoritmo perceptrón</p>
<p>A partir del <em><strong>método de subgradientes</strong></em> se puede verificar fácilmente que, iniciando desde un punto arbitrario, <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(0)}\)</span>, el siguiente método iterativo,</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\theta}^{(i)}=\boldsymbol{\theta}^{(i-1)}+\mu_{i}\sum_{n:\boldsymbol{x}_{n}\in\mathcal{Y}}y_{n}\boldsymbol{x}_{n}:\quad\textsf{Regla perceptrón}, 
\]</div>
<p><em><strong>converge después de un número finito de pasos</strong></em>. La sucesión de parámetros <span class="math notranslate nohighlight">\(\mu_{i}\)</span> es seleccionada adecuadamente para garantizar convergencia.</p>
</div>
<ul class="simple">
<li><p>Nótese que usando el <a class="reference external" href="https://en.wikipedia.org/wiki/Subgradient_method">método de subgradiente</a> se tiene que</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\boldsymbol{\theta}^{(i)}&amp;=\boldsymbol{\theta}^{(i-1)}-\mu_{i}\nabla_{\boldsymbol{\theta}} J(\boldsymbol{\theta}^{(i-1)})\\
&amp;=\boldsymbol{\theta}^{(i-1)}-\mu_{i}\left(-\sum_{n:\boldsymbol{x}_{n}\in\mathcal{Y}}y_{n}\boldsymbol{x}_{n}^{T}\right)\\
&amp;=\boldsymbol{\theta}^{(i-1)}+\mu_{i}\sum_{n:\boldsymbol{x}_{n}\in\mathcal{Y}}y_{n}\boldsymbol{x}_{n}.
\end{align*}
\end{split}\]</div>
<ul class="simple">
<li><p>Otra versión del algoritmo considera <em><strong>una muestra por iteración en un esquema cíclico, hasta que el algoritmo converge</strong></em>. Denotemos por <span class="math notranslate nohighlight">\(y_{(i)}\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{x}_{i},~i\in\{1,2,\dots,N\}\)</span> los <em><strong>pares de entrenamiento presentados al algoritmo en la iteración</strong></em> <span class="math notranslate nohighlight">\(i\)</span><em><strong>-ésima</strong></em>. Entonces, la iteración de actualización se convierte en:</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-eq-perceptron-algo2">
<span class="eqno">(4.1)<a class="headerlink" href="#equation-eq-perceptron-algo2" title="Link to this equation">#</a></span>\[\begin{split}
\boldsymbol{\theta}^{(i)}=
\begin{cases}
\boldsymbol{\theta}^{(i-1)}+\mu_{i}y_{(i)}\boldsymbol{x}_{(i)},&amp;\quad\text{si}\,\boldsymbol{x}_{(i)}\,\text{es mal clasificado por}\,\boldsymbol{\theta}^{(i-1)},\\
\boldsymbol{\theta}^{(i-1)},&amp;\quad\text{otro caso}.
\end{cases}
\end{split}\]</div>
<ul class="simple">
<li><p>Esto es, partiendo de una estimación inicial de forma random, <em><strong>inicializando</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(0)}\)</span> <em><strong>con algunos valores pequeños, testeamos cada una de las muestras</strong></em>, <span class="math notranslate nohighlight">\(\boldsymbol{x}_{n},~n=1,2,\dots,N\)</span>. Cada vez que una <em><strong>muestra es mal clasificada, se toma acción por medio de la regla perceptrón para una corrección</strong></em>. En otro caso, ninguna acción es requerida. <em><strong>Una vez que todas las muestras han sido consideradas, decimos que una <code class="docutils literal notranslate"><span class="pre">época</span> <span class="pre">(epoch)</span></code> ha sido completada</strong></em>.</p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">Observación</p>
<ul class="simple">
<li><p>Si <em><strong>no se obtiene convergencia, todas las muestras son reconsideradas en una segunda época</strong></em>, y así sucesivamente. La versión de este algoritmo es conocida como esquema <strong><code class="docutils literal notranslate"><span class="pre">pattern-by-pattern</span></code></strong>. Algunas veces también es referido como el <em><strong>algoritmo online</strong></em>. Nótese que el número total de datos muestrales es fijo, y que el algoritmo los considera en forma cíclica, época por época (<em><strong>epoch-by-epoch</strong></em>).</p></li>
<li><p>Después de un número finito de épocas, se garantiza que el algoritmo es convergente. <em><strong>Nótese que para obtener dicha convergencia, la sucesión</strong></em> <span class="math notranslate nohighlight">\(\mu_{i}\)</span> <em><strong>debe ser seleccionada apropiadamente</strong></em>. Sin embargo, para el caso del <em><strong>algoritmo perceptrón, la convergencia es garantizada</strong></em> (<span class="math notranslate nohighlight">\(J\)</span> convexa), aun cuando <span class="math notranslate nohighlight">\(\mu_{i}\)</span> es una constante positiva, <span class="math notranslate nohighlight">\(\mu_{i}=\mu&gt;0\)</span>, <em><strong>usualmente tomado igual a uno</strong></em>.</p></li>
<li><p>La formulación en <a class="reference internal" href="#equation-eq-perceptron-algo2">(4.1)</a> es conocida también como la <em><strong>filosofía de aprendizaje <code class="docutils literal notranslate"><span class="pre">reward-punishment</span></code></strong></em>. Si la actual estimación es exitosa en la predicción de la clase del respectivo patrón, ninguna acción es tomada (<code class="docutils literal notranslate"><span class="pre">reward</span></code>), en otro caso, el algoritmo es obligado a realizar una actualización (<code class="docutils literal notranslate"><span class="pre">punishment</span></code>).</p></li>
</ul>
</div>
<figure class="align-center" id="perceptron-rule">
<a class="reference internal image-reference" href="_images/perceptron_rule.png"><img alt="_images/perceptron_rule.png" src="_images/perceptron_rule.png" style="width: 332.8px; height: 346.40000000000003px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.5 </span><span class="caption-text">El punto <span class="math notranslate nohighlight">\(x\)</span> está mal clasificado por la línea roja. La regla perceptrón gira el hiperplano hacia el punto <span class="math notranslate nohighlight">\(x\)</span>, para intentar incluirlo en el lado correcto del nuevo hiperplano y clasificarlo correctamente. El nuevo hiperplano está definido por <span class="math notranslate nohighlight">\(θ^{(i)}\)</span> y se muestra con la línea negra.</span><a class="headerlink" href="#perceptron-rule" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul>
<li><p>La <a class="reference internal" href="#perceptron-rule"><span class="std std-numref">Fig. 4.5</span></a> ofrece una interpretación geométrica de la <em><strong>regla del perceptrón</strong></em>. Supongamos que la muestra <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> está mal clasificada por el hiperplano, <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(i-1)}\)</span>. Como sabemos, por geometría analítica, <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(i-1)}\)</span> <em><strong>corresponde a un vector que es perpendicular al hiperplano que está definido por este vector</strong></em>.</p></li>
<li><p>Como <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> se encuentra en el lado <span class="math notranslate nohighlight">\((-)\)</span> del hiperplano y está <em>mal clasificado, pertenece a la clase</em> <span class="math notranslate nohighlight">\(\omega_{1}\)</span>; asumiendo <span class="math notranslate nohighlight">\(\mu = 1\)</span>, la corrección aplicada por el algoritmo es</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \\[1mm]
    \boldsymbol{\theta}^{(i)}=\boldsymbol{\theta}^{(i-1)}+\boldsymbol{x},
    \end{split}\]</div>
<p>y su efecto es <em><strong>girar el hiperplano en dirección a</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> para colocarlo en el lado <span class="math notranslate nohighlight">\((+)\)</span> del nuevo hiperplano, que está definido por la estimación actualizada <span class="math notranslate nohighlight">\(\boldsymbol{\theta^{(i)}}\)</span>. El <em><strong>algoritmo perceptrón</strong></em> en su modo de funcionamiento patrón por patrón (<em><strong>pattern-by-pattern</strong></em>) se resume en el siguiente algoritmo.</p>
</li>
</ul>
<div class="proof algorithm admonition" id="my_algorithm_pattern_by_pattern">
<p class="admonition-title"><span class="caption-number">Algorithm 4.1 </span> (Algoritmo perceptrón <em>pattern-by-pattern</em>)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Inicialización</strong></p>
<ol class="arabic simple">
<li><p>Inicializar <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(0)}\)</span>; usualmente, de forma <code class="docutils literal notranslate"><span class="pre">random,</span> <span class="pre">pequeño</span></code></p></li>
<li><p>Seleccionar <span class="math notranslate nohighlight">\(\mu\)</span>; usualmente <code class="docutils literal notranslate"><span class="pre">establecido</span> <span class="pre">como</span> <span class="pre">1</span></code></p></li>
<li><p><span class="math notranslate nohighlight">\(i=1\)</span></p></li>
</ol>
<p><strong>Repeat</strong> Cada iteración corresponde a un <code class="docutils literal notranslate"><span class="pre">epoch</span></code></p>
<ol class="arabic">
<li><p><code class="docutils literal notranslate"><span class="pre">counter</span> <span class="pre">=</span> <span class="pre">0</span></code>; Contador del número de actualizaciones por <code class="docutils literal notranslate"><span class="pre">epoch</span></code></p></li>
<li><p><strong>For</strong> <span class="math notranslate nohighlight">\(n=1,2,\dots,N\)</span> <strong>Do</strong> Para cada <code class="docutils literal notranslate"><span class="pre">epoch</span></code>, <em>todas las muestras son presentadas una vez</em></p>
<p><strong>If</strong>(<span class="math notranslate nohighlight">\(y_{n}\boldsymbol{x}_{n}^{T}\leq0\)</span>) <strong>Then</strong></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(i)}=\boldsymbol{\theta}^{(i-1)}+\mu y_{n}\boldsymbol{x}_{n}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(i=i+1\)</span></p></li>
<li><p>counter = counter + 1</p></li>
</ol>
<p><strong>End For</strong></p>
</li>
<li><p><strong>Until</strong> counter = 0</p></li>
</ol>
</section>
</div><ul class="simple">
<li><p>Una vez que el <em><strong>algoritmo perceptrón se ha ejecutado y converge</strong></em>, tenemos los <em><strong>pesos</strong></em>, <span class="math notranslate nohighlight">\(\theta_{i},~i = 1,2,\dots,l\)</span>, <em><strong>de las sinapsis de la neurona/perceptrón</strong></em> asociada, así como el término de sesgo <span class="math notranslate nohighlight">\(\theta_{0}\)</span>. Ahora se pueden <em><strong>utilizar para clasificar patrones desconocidos</strong></em>. Las características <span class="math notranslate nohighlight">\(x_{i}, i = 1, 2,\dots,l\)</span>, se aplican a los nodos de entrada.</p></li>
<li><p>A su vez, <em><strong>cada característica se multiplica por la sinapsis respectiva (peso), y luego se añade el término de sesgo en su combinación lineal</strong></em>. El resultado de esta operación <em><strong>pasa por una función no lineal</strong></em>, <span class="math notranslate nohighlight">\(f\)</span>, conocida como <em><strong><a class="reference external" href="https://en.wikipedia.org/wiki/Activation_function">función de activación</a></strong></em>. Dependiendo de la forma de la no linealidad, se producen diferentes tipos de neuronas. La más clásica conocida como <em><strong>neurona McCulloch-Pitts</strong></em>, la función de <em><strong>activación es la de Heaviside</strong></em>, es decir,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
f(z)=
\begin{cases}
1,&amp;\quad\text{si}~z&gt;0,\\
0,&amp;\quad\text{si}~z\leq0.
\end{cases}
\end{split}\]</div>
<figure class="align-center" id="mcculloch-pitts">
<a class="reference internal image-reference" href="_images/mcculloch_pitts.png"><img alt="_images/mcculloch_pitts.png" src="_images/mcculloch_pitts.png" style="width: 532.0px; height: 167.20000000000002px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.6 </span><span class="caption-text">Arquitectura básica de neuronas/perceptrones.</span><a class="headerlink" href="#mcculloch-pitts" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>En la arquitectura básica de neuronas/perceptrones, las <em><strong>características de entrada se aplican a los nodos de entrada y se ponderan por los respectivos pesos que definen las sinapsis</strong></em>. A continuación <em><strong>se añade el término de sesgo en su combinación lineal y el resultado es empujado a través de la no linealidad</strong></em>.</p></li>
<li><p>En la neurona <em><strong>McCulloch-Pitts</strong></em>, la salida es 1 para los patrones de la clase <span class="math notranslate nohighlight">\(\omega_{1}\)</span> o 0 para la clase <span class="math notranslate nohighlight">\(\omega_{2}\)</span>. La suma y la operación no lineal se unen para simplificar el gráfico.</p></li>
</ul>
<figure class="align-center" id="hidden-layer-activationf-function-fig">
<a class="reference internal image-reference" href="_images/hidden_layer_activationf_function.png"><img alt="_images/hidden_layer_activationf_function.png" src="_images/hidden_layer_activationf_function.png" style="width: 508.9px; height: 256.2px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.7 </span><span class="caption-text">Selección de función de activación para <em><strong><code class="docutils literal notranslate"><span class="pre">hidden</span> <span class="pre">layers</span></code></strong></em>. (Fuente <span id="id3">[<a class="reference internal" href="biblio.html#id32" title="J. Brownlee and Machine Learning Mastery. Deep Learning with Python: Develop Deep Learning Models on Theano and TensorFlow Using Keras. Machine Learning Mastery, 2017. URL: https://books.google.com.co/books?id=eJw2nQAACAAJ.">Brownlee and Mastery, 2017</a>]</span>).</span><a class="headerlink" href="#hidden-layer-activationf-function-fig" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>Para las capas ocultas, la función de <em><strong>activación <a class="reference external" href="https://es.wikipedia.org/wiki/Tangente_hiperb%C3%B3lica">tangente hiperbólica</a> suele funcionar mejor que la <a class="reference external" href="https://en.wikipedia.org/wiki/Sigmoid_function">sigmoidea logística</a></strong></em>. Tanto la función <em><strong>sigmoid</strong></em> como <em><strong>tanh</strong></em> pueden hacer que el modelo sea más <em><strong>susceptible a los problemas durante el entrenamiento, a través del llamado problema de los <code class="docutils literal notranslate"><span class="pre">gradientes</span> <span class="pre">desvanecientes</span></code></strong></em>. Los modelos modernos de redes neuronales con arquitecturas comunes, como <em><strong>MLP y CNN, harán uso de la función de activación <a class="reference external" href="https://es.wikipedia.org/wiki/Rectificador_(redes_neuronales)">ReLU</a></strong></em>, o extensiones.</p></li>
<li><p>Las <em><strong>redes recurrentes</strong></em> suelen utilizar funciones de activación <em><strong>tanh</strong></em> o <em><strong>sigmoid</strong></em>, o incluso ambas. Por ejemplo, la <em><strong>LSTM suele utilizar la activación sigmoid para las conexiones recurrentes y la activación tanh para la salida</strong></em>.</p></li>
</ul>
<figure class="align-center" id="output-layer-activation-function">
<a class="reference internal image-reference" href="_images/output_layer_activation_function.png"><img alt="_images/output_layer_activation_function.png" src="_images/output_layer_activation_function.png" style="width: 485.79999999999995px; height: 301.7px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.8 </span><span class="caption-text">Selección de función de activación para <em><strong><code class="docutils literal notranslate"><span class="pre">output</span> <span class="pre">layers</span></code></strong></em>. (Fuente <span id="id4">[<a class="reference internal" href="biblio.html#id32" title="J. Brownlee and Machine Learning Mastery. Deep Learning with Python: Develop Deep Learning Models on Theano and TensorFlow Using Keras. Machine Learning Mastery, 2017. URL: https://books.google.com.co/books?id=eJw2nQAACAAJ.">Brownlee and Mastery, 2017</a>]</span>).</span><a class="headerlink" href="#output-layer-activation-function" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>Si su problema es de <em><strong>regresión, debe utilizar una función de <a class="reference external" href="https://en.wikipedia.org/wiki/Linearity">activación lineal</a></strong></em>. Si su problema es de <em><strong>clasificación</strong></em>, el modelo predice la <em><strong>probabilidad de pertenencia a una clase</strong></em>, que se puede convertir en una <em><strong>etiqueta de clase mediante redondeo (para sigmoid)</strong></em> o <em><strong>argmax (para <a class="reference external" href="https://en.wikipedia.org/wiki/Softmax_function">softmax</a>)</strong></em>.</p></li>
</ul>
<div class="admonition-activacion-lineal-y-problemas-de-regresion admonition">
<p class="admonition-title">Activación Lineal y Problemas de Regresión</p>
<p>En las <em><strong>Redes Neuronales Artificiales (ANNs)</strong></em>, la función de <em><strong>activación lineal</strong></em> se utiliza comúnmente en la capa de salida para <em><strong>problemas de regresión y series de tiempo</strong></em> debido a las siguientes razones:</p>
<ul>
<li><p><em><strong><code class="docutils literal notranslate"><span class="pre">Producción</span> <span class="pre">de</span> <span class="pre">Salidas</span> <span class="pre">Continuas</span></code></strong></em></p>
<p>La regresión y las predicciones de <em><strong>series de tiempo</strong></em> generalmente requieren que el modelo produzca <em><strong>salidas continuas, no discretas</strong></em>. Una función de <em><strong>activación lineal permite que la salida de la red neuronal sea cualquier valor real, lo que es adecuado para este tipo de problemas</strong></em>. <em>Por ejemplo, si estamos prediciendo precios de viviendas o temperaturas futuras, necesitamos que las salidas sean valores continuos, no limitados a un rango específico</em>.</p>
</li>
<li><p><em><strong><code class="docutils literal notranslate"><span class="pre">Evitar</span> <span class="pre">Saturación</span></code></strong></em></p>
<p>Funciones de activación no lineales como la <em><strong>sigmoid o tanh tienen salidas acotadas</strong></em> (<em>entre 0 y 1 para sigmoide, y entre -1 y 1 para tanh</em>). Estas funciones pueden saturarse, es decir, <em><strong>para valores grandes de entrada, los gradientes pueden volverse muy pequeños, lo que dificulta el entrenamiento del modelo debido al problema del gradiente desvaneciente</strong></em>. <em>Una función de activación lineal no tiene este problema, ya que la salida es directamente proporcional a la entrada</em>.</p>
</li>
<li><p><em><strong><code class="docutils literal notranslate"><span class="pre">Preservación</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">Escala</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">Salida</span></code></strong></em></p>
<p>En <em><strong>tareas de regresión y predicción de series de tiempo</strong></em>, a menudo <em><strong>es importante que la escala de las salidas se mantenga consistente con la escala de los valores objetivo</strong></em>. La función de activación lineal, definida como, <span class="math notranslate nohighlight">\(\boldsymbol{f(x)=x}\)</span> <em><strong>no cambia la escala de los valores</strong></em>, permitiendo que la red neuronal <em>modele relaciones directamente proporcionales entre las entradas y las salidas</em>.</p>
</li>
</ul>
</div>
</section>
<section id="redes-totalmente-conectadas">
<h2><span class="section-number">4.4. </span>Redes Totalmente Conectadas<a class="headerlink" href="#redes-totalmente-conectadas" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Para resumir de manera más formal el tipo de <em><strong>operaciones que tienen lugar en una red totalmente conectada</strong></em>, centrémonos en, por ejemplo, la <em><strong>capa</strong></em> <span class="math notranslate nohighlight">\(r\)</span> <em><strong>de una red neuronal multicapa y supongamos que está formada por</strong></em> <span class="math notranslate nohighlight">\(k_{r}\)</span> <em><strong>neuronas</strong></em>. El vector de <em><strong>entrada a esta capa está formado por las salidas de los nodos de la capa anterior, que se denomina</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{y}^{r-1}\)</span>.</p></li>
<li><p>Sea <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{j}^{r}\)</span> el vector de los <em><strong>pesos sinápticos, incluido el término de sesgo, asociado a la neurona</strong></em> <span class="math notranslate nohighlight">\(j\)</span> <em><strong>de la capa</strong></em> <span class="math notranslate nohighlight">\(r\)</span>, donde <span class="math notranslate nohighlight">\(j = 1,2,\dots, k_{r}\)</span>. La <em><strong>dimensión respectiva</strong></em> de este vector es <span class="math notranslate nohighlight">\(k_{r-1} + 1\)</span>, donde <span class="math notranslate nohighlight">\(k_{r-1}\)</span> es el <em><strong>número de neuronas de la capa anterior</strong></em>, <span class="math notranslate nohighlight">\(r-1\)</span>, <em><strong>y el aumento en 1 representa el término de sesgo</strong></em>. Entonces las operaciones realizadas, antes de la no linealidad, son los productos internos</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
z_{j}^{r}=\boldsymbol{\theta}_{j}^{rT}\boldsymbol{y}^{r-1},\quad j=1,2,\dots,k_{r}.
\]</div>
<ul class="simple">
<li><p>Colocando todos los valores de salida en un vector <span class="math notranslate nohighlight">\(\boldsymbol{z}^{r}=[z_{1}^{r}, z_{2}^{r},\dots,z_{k_{r}}^{r}]^{T}\)</span>, y <em><strong>agrupando todos los vectores sinápticos como filas</strong></em>, una debajo de la otra, en una matriz, podemos escribir colectivamente</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\boldsymbol{z}^{r}=\Theta\boldsymbol{y}^{r-1},\quad\text{donde}\quad\Theta:=[\boldsymbol{\theta}_{1}^{r}, \boldsymbol{\theta}_{2}^{r},\dots, \boldsymbol{\theta}_{k_{r}}^{r}].
\]</div>
<ul class="simple">
<li><p>El vector de las salidas de la <span class="math notranslate nohighlight">\(r\)</span>-ésima capa oculta, después de <em><strong>empujar cada</strong></em> <span class="math notranslate nohighlight">\(z_{i}^{r}\)</span> <em><strong>a través de la no linealidad</strong></em> <span class="math notranslate nohighlight">\(f\)</span>, está finalmente dado por</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{y}^{r}=
\begin{bmatrix}
1\\
f(\boldsymbol{z}^{r})
\end{bmatrix}
\end{split}\]</div>
<ul class="simple">
<li><p>La siguiente figura describe como es creada la <span class="math notranslate nohighlight">\(j\)</span>-ésima columna de la red neuronal full conectada</p></li>
</ul>
<figure class="align-center" id="id20">
<a class="reference internal image-reference" href="_images/fully_connected_net.png"><img alt="_images/fully_connected_net.png" src="_images/fully_connected_net.png" style="width: 600.0px; height: 317.20000000000005px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.9 </span><span class="caption-text"><span class="math notranslate nohighlight">\(j\)</span>-ésimo elemento de la capa <span class="math notranslate nohighlight">\(r\)</span> de la red totalmente conectada.</span><a class="headerlink" href="#id20" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="tip admonition">
<p class="admonition-title">Observación</p>
<ul class="simple">
<li><p>La notación anterior significa que <span class="math notranslate nohighlight">\(f\)</span> <em><strong>actúa sobre cada uno de los respectivos componentes del vector</strong></em>, individualmente, y la <em><strong>extensión del vector en uno es para dar cuenta de los términos de sesgo</strong></em> en la práctica estándar. Para redes grandes, con muchas capas y muchos nodos por capa, este tipo de conectividad resulta ser muy costoso en términos del número de parámetros (pesos), que es del orden de <span class="math notranslate nohighlight">\(k_{r}k_{r-1}\)</span>.</p></li>
<li><p>Por ejemplo, si <span class="math notranslate nohighlight">\(k_{r-1} = 1000\)</span> y <span class="math notranslate nohighlight">\(k_{r} = 1000\)</span>, <em><strong>esto equivale a un orden de 1 millón de parámetros</strong></em>. Tenga en cuenta que este número es la contribución de los parámetros de una sola de las capas. Sin embargo, <em><strong>un gran número de parámetros hace que una red sea vulnerable al sobreajuste</strong></em>, cuando se trata de entrenamiento</p></li>
</ul>
</div>
<ul class="simple">
<li><p>Se pueden emplear las llamadas <em><strong>técnicas de reparto de pesos</strong></em>, en las que un conjunto de parámetros es compartido entre un número de conexiones, a través de restricciones adecuadamente incorporadas. Las <em><strong>redes neuronales recurrentes y convolucionales</strong></em> pertenecen a esta familia de redes de peso compartido. En una red convolucional, las <em><strong>convoluciones sustituyen a las operaciones de producto interno</strong></em>, lo que permite un reparto de pesos importante que conduce a una reducción sustancial del número de parámetros.</p></li>
</ul>
</section>
<section id="el-algoritmo-de-backpropagation">
<h2><span class="section-number">4.5. </span>El Algoritmo De Backpropagation<a class="headerlink" href="#el-algoritmo-de-backpropagation" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Una red neuronal considera una <em><strong>función paramétrica no lineal</strong></em>, <span class="math notranslate nohighlight">\(\hat{y} = f_{\boldsymbol{\theta}}(\boldsymbol{x})\)</span>, donde <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> representa todos los pesos/sesgo presentes en la red. Por lo tanto, el entrenamiento de una red neuronal no parece ser diferente del entrenamiento de cualquier otro modelo de predicción paramétrico.</p></li>
<li><p>Todo lo que se necesita es <em><strong>(a)</strong></em> un <em><strong>conjunto de muestras de entrenamiento</strong></em>, <em><strong>(b)</strong></em> una <em><strong>función de pérdida</strong></em> <span class="math notranslate nohighlight">\(\mathcal{L}(y, \hat{y})\)</span>, y <em><strong>(c)</strong></em> un <em><strong>esquema iterativo</strong></em>, por ejemplo, el <em><strong>gradiente descendente</strong></em>, para realizar la optimización de la función de coste asociada (<em><strong>pérdida empírica</strong></em>).</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
J(\boldsymbol{\theta})=\sum_{n=1}^{N}\mathcal{L}(y_{n}, f_{\boldsymbol{\theta}}(\boldsymbol{x}_{n})).
\]</div>
<ul class="simple">
<li><p>La dificultad del entrenamiento de las redes neuronales radica en su <em><strong>estructura multicapa que complica el cálculo de los gradientes, que intervienen en la optimización</strong></em>. Además, la neurona <em><strong>McCulloch-Pitts</strong></em> se basa en la función de activación discontinua de <em><strong>Heaviside, que no es diferenciable</strong></em>.</p></li>
</ul>
<ul class="simple">
<li><p>La <em><strong><code class="docutils literal notranslate"><span class="pre">neurona</span> <span class="pre">sigmoidea</span> <span class="pre">logística</span></code></strong></em>: Una posibilidad es adoptar la función <em><strong>sigmoidea logística</strong></em>, es decir,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
f(z)=\sigma(z):=\frac{1}{1+\exp(-az)}.
\]</div>
<ul class="simple">
<li><p>Nótese que <em><strong>cuanto mayor sea el valor del parámetro</strong></em> <span class="math notranslate nohighlight">\(a\)</span>, <em><strong>la gráfica correspondiente se acerca más a la de la función de Heaviside</strong></em> (ver <a class="reference internal" href="#sigmoid-act-function"><span class="std std-numref">Fig. 4.10</span></a>).</p></li>
</ul>
<figure class="align-center" id="sigmoid-act-function">
<a class="reference internal image-reference" href="_images/sigmoid_act_function.png"><img alt="_images/sigmoid_act_function.png" src="_images/sigmoid_act_function.png" style="width: 507.20000000000005px; height: 471.20000000000005px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.10 </span><span class="caption-text">La función sigmoidea logística para diferentes valores del parámetro <span class="math notranslate nohighlight">\(a\)</span>.</span><a class="headerlink" href="#sigmoid-act-function" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul>
<li><p>Otra posibilidad sería <em><strong>utilizar la función</strong></em>,</p>
<div class="math notranslate nohighlight">
\[
    f(z)=a\tanh\left(\frac{cz}{2}\right),
    \]</div>
<p>donde <span class="math notranslate nohighlight">\(c\)</span> y <span class="math notranslate nohighlight">\(a\)</span> son parámetros de control. El gráfico de esta función se muestra en la <a class="reference internal" href="#tanh-act-function"><span class="std std-numref">Fig. 4.11</span></a>. Nótese que a diferencia de la sigmoidea logística, ésta es una <em><strong>función no simétrica</strong></em>, es decir, <span class="math notranslate nohighlight">\(f(-z)=-f(z)\)</span>. Ambas son también conocidas como <em><strong>funciones de reducción, porque limitan la salida a un rango finito de valores</strong></em>.</p>
</li>
</ul>
<figure class="align-center" id="tanh-act-function">
<a class="reference internal image-reference" href="_images/tanh_act_function.png"><img alt="_images/tanh_act_function.png" src="_images/tanh_act_function.png" style="width: 693.6px; height: 435.20000000000005px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.11 </span><span class="caption-text">Función de reducción de la tangente hiperbólica para <span class="math notranslate nohighlight">\(a = 1.7\)</span> y <span class="math notranslate nohighlight">\(c = 4/3\)</span>.</span><a class="headerlink" href="#tanh-act-function" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul>
<li><p>Recordemos que la regla de <em><strong>actualización del algoritmo gradiente descendente</strong></em>, en su versión unidimensional se convierte en</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \\[1mm]
    \theta(new)=\theta(old)-\mu\left.\frac{d J}{d\theta}\right|_{\theta(old)},
    \end{split}\]</div>
<p>y las iteraciones parten de un punto inicial arbitrario, <span class="math notranslate nohighlight">\(\theta^{(0)}\)</span>. Si en la iteración actual el algoritmo está digamos, en el punto <span class="math notranslate nohighlight">\(\theta(old) = \theta_{1}\)</span>, entonces se moverá hacia el mínimo local, <span class="math notranslate nohighlight">\(\theta_{l}\)</span>. <em><strong>Esto se debe a que la derivada del coste en</strong></em> <span class="math notranslate nohighlight">\(\theta_{1}\)</span> <em><strong>es igual a la tangente de</strong></em> <span class="math notranslate nohighlight">\(\phi_{1}\)</span>, <em><strong>que es negativa</strong></em> (<em>el ángulo es obtuso</em>) y la actualización, <span class="math notranslate nohighlight">\(\theta(new)\)</span>, se moverá a la derecha, hacia el mínimo local, <span class="math notranslate nohighlight">\(\theta_{l}\)</span>.</p>
</li>
</ul>
<figure class="align-center" id="convex-function-saddle-point">
<a class="reference internal image-reference" href="_images/convex_function_saddle_point.png"><img alt="_images/convex_function_saddle_point.png" src="_images/convex_function_saddle_point.png" style="width: 635.2px; height: 354.40000000000003px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.12 </span><span class="caption-text">Función no convexa global, con mínimos locales y puntos de silla.</span><a class="headerlink" href="#convex-function-saddle-point" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>La <em><strong>elección del tamaño del paso</strong></em>, <span class="math notranslate nohighlight">\(\mu\)</span>, <em><strong>es crítica para la convergencia del algoritmo</strong></em>. En problemas reales en espacios multidimensionales, el número de mínimos locales puede ser grande, por lo que <em><strong>el algoritmo puede converger a uno local</strong></em>. Sin embargo, esto no es necesariamente una mala noticia.</p></li>
<li><p>Si este <em><strong>mínimo local es lo suficientemente profundo</strong></em>, es decir, si el valor de la función de coste en este punto, por ejemplo, <span class="math notranslate nohighlight">\(J(\theta_{l})\)</span>, <em><strong>no es mucho mayor que el alcanzado en el mínimo global</strong></em>, es decir, <span class="math notranslate nohighlight">\(J(\theta_{g})\)</span>, la <em><strong>convergencia a dicho mínimo local puede corresponder a una buena solución</strong></em>.</p></li>
</ul>
</section>
<section id="el-esquema-de-backpropagation-para-gradiente-descendente">
<h2><span class="section-number">4.6. </span>El Esquema De Backpropagation Para Gradiente descendente<a class="headerlink" href="#el-esquema-de-backpropagation-para-gradiente-descendente" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Habiendo adoptado una función de activación diferenciable, estamos listos para proceder a desarrollar el <em><strong>esquema iterativo de gradiente descendente para la minimización de la función de coste</strong></em>. Formularemos la tarea en un marco general.</p></li>
</ul>
<ul class="simple">
<li><p>Sea <span class="math notranslate nohighlight">\((\boldsymbol{y}_{n}, \boldsymbol{x}_{n}), n = 1, 2,\dots, N\)</span>, el conjunto de muestras de entrenamiento. <em><strong>Nótese que hemos asumido output multivariado</strong></em>. Suponemos que la red consta de <span class="math notranslate nohighlight">\(L\)</span> capas, <span class="math notranslate nohighlight">\(L-1\)</span> capas ocultas y una capa de salida. Cada capa consta de <span class="math notranslate nohighlight">\(k_{r}, r = 1, 2,\dots, L\)</span>, neuronas. Así, los vectores de salida (objetivo/deseado) son</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\boldsymbol{y}_{n}=[y_{n1}, y_{n2},\dots, y_{nk_{L}}]^{T}\in\mathbb{R}^{k_{L}},\quad n=1,2,\dots,N.
\]</div>
<ul class="simple">
<li><p>Para ciertas derivaciones matemáticas, también denotamos el número de nodos de entrada como <span class="math notranslate nohighlight">\(k_{0}\)</span>; es decir <span class="math notranslate nohighlight">\(k_{0} = l\)</span>, donde <span class="math notranslate nohighlight">\(l\)</span> es la <em><strong>dimensionalidad del espacio de características de entrada</strong></em>.</p></li>
<li><p>Sea <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{j}^{r}\)</span> el <em><strong>vector de los pesos sinápticos asociados a la</strong></em> <span class="math notranslate nohighlight">\(j\)</span><em><strong>-ésima neurona de la</strong></em> <span class="math notranslate nohighlight">\(r\)</span><em><strong>-ésima capa</strong></em>, con <span class="math notranslate nohighlight">\(j = 1, 2,\dots, k_{r}\)</span> y <span class="math notranslate nohighlight">\(r = 1, 2,\dots,L\)</span>, donde el término de sesgo se incluye en <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{j}^{r}\)</span> , es decir,</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-parameters-vector-def">
<span class="eqno">(4.2)<a class="headerlink" href="#equation-parameters-vector-def" title="Link to this equation">#</a></span>\[
\boldsymbol{\theta}_{j}^{r}:=[\theta_{j0}^{r}, \theta_{j1}^{r},\dots, \theta_{jk_{r-1}}^{r}]^{T}.
\]</div>
<ul class="simple">
<li><p>Los <em><strong>pesos sinápticos enlazan la neurona respectiva con todas las neuronas de la capa</strong></em> <span class="math notranslate nohighlight">\(k_{r-1}\)</span> (véase la <a class="reference internal" href="#synaptic-weights-link"><span class="std std-numref">Fig. 4.13</span></a>). El paso iterativo básico para el esquema de gradiente descendente se escribe como</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-update-equations-gd">
<span class="eqno">(4.3)<a class="headerlink" href="#equation-update-equations-gd" title="Link to this equation">#</a></span>\[\begin{split}
\begin{align*}
\boldsymbol{\theta}_{j}^{r}(\text{new})&amp;=\boldsymbol{\theta}_{j}^{r}(old)+\Delta\boldsymbol{\theta}_{j}^{r},\\
\Delta\boldsymbol{\theta}_{j}^{r}&amp;:=-\mu\left.\frac{\partial J}{\partial\boldsymbol{\theta}_{j}^{r}}\right|_{\boldsymbol{\theta}_{j}^{r}(old)}.
\end{align*}
\end{split}\]</div>
<ul class="simple">
<li><p>El parámetro <span class="math notranslate nohighlight">\(\mu\)</span> es el <em><strong>tamaño de paso definido por el usuario</strong></em> (<em>también puede depender de la iteración</em>) y <span class="math notranslate nohighlight">\(J\)</span> denota la función de coste.</p></li>
</ul>
<figure class="align-center" id="synaptic-weights-link">
<a class="reference internal image-reference" href="_images/synaptic_weights_link.png"><img alt="_images/synaptic_weights_link.png" src="_images/synaptic_weights_link.png" style="width: 529.6px; height: 355.20000000000005px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.13 </span><span class="caption-text">Enlaces y las variables asociadas de la <span class="math notranslate nohighlight">\(j\)</span>-ésima neurona en la <span class="math notranslate nohighlight">\(r\)</span>-ésima capa. <span class="math notranslate nohighlight">\(y_{k}^{r-1}\)</span> es la salida de la <span class="math notranslate nohighlight">\(k\)</span>-ésima neurona de la <span class="math notranslate nohighlight">\((r - 1)\)</span>-ésima capa y <span class="math notranslate nohighlight">\(\theta_{jk}^{r}\)</span> es el peso respectivo que conecta estas dos neuronas.</span><a class="headerlink" href="#synaptic-weights-link" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="tip admonition">
<p class="admonition-title">Forward/Backward</p>
<p>Las <em><strong>Ecuaciones de actualización</strong></em> <a class="reference internal" href="#equation-update-equations-gd">(4.3)</a> comprenden el <em><strong>esquema de gradiente descendente</strong></em> para la optimización. Como se ha dicho anteriormente, la dificultad de las redes neuronales <em><strong>feed-forward</strong></em> surge de su estructura multicapa. Para calcular los gradientes en la Ecuación <a class="reference internal" href="#equation-update-equations-gd">(4.3)</a>, para todas las neuronas, en todas las capas, se deben seguir dos pasos en su cálculo</p>
<ul>
<li><p><em><strong><code class="docutils literal notranslate"><span class="pre">Forward</span> <span class="pre">computations</span></code></strong></em>: Para un vector de entrada dado <span class="math notranslate nohighlight">\(\boldsymbol{x}_{n}, n = 1, 2,\dots, N\)</span>, se utilizan las estimaciones actuales de los parámetros (<em>pesos sinápticos</em>) (<span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{j}^{r}(old)\)</span>) y <em><strong>calcula todas las salidas de todas las neuronas en todas las capas</strong></em>, denotadas como <span class="math notranslate nohighlight">\(y_{nj}^{r}\)</span>; en la <a class="reference internal" href="#synaptic-weights-link"><span class="std std-numref">Fig. 4.13</span></a>, se ha suprimido el índice <span class="math notranslate nohighlight">\(n\)</span> para no afectar la notación.</p></li>
<li><p><em><strong><code class="docutils literal notranslate"><span class="pre">Backward</span> <span class="pre">computations</span></code></strong></em>: Utilizando las salidas neuronales calculadas anteriormente junto con los valores objetivo conocidos, <span class="math notranslate nohighlight">\(y_{nk}\)</span>, de la capa de salida, se <em><strong>calculan los gradientes de la función de coste</strong></em>. Esto implica <span class="math notranslate nohighlight">\(L\)</span> pasos, es decir, tantos como el número de capas. La secuencia de los pasos algorítmicos se indica a continuación:</p>
<ul>
<li><p>Calcular el gradiente de la función de coste con respecto a los parámetros de las neuronas de la última capa, es decir, <span class="math notranslate nohighlight">\(\displaystyle{\frac{\partial J}{\partial\boldsymbol{\theta}_{j}^{L}}, j = 1, 2,\dots, k_{L}}\)</span>.</p></li>
<li><p><strong>For</strong> <span class="math notranslate nohighlight">\(r = L-1\)</span> to <span class="math notranslate nohighlight">\(1\)</span>, <strong>Do</strong></p>
<p>Calcular los gradientes con respecto a los parámetros asociados a las neuronas de la <span class="math notranslate nohighlight">\(r\)</span>-ésima capa, es decir, <span class="math notranslate nohighlight">\(\displaystyle{\frac{\partial J}{\partial\boldsymbol{\theta}_{k}^{r}}, k= 1, 2,\dots, k_{r}}\)</span> basado en todos los gradientes <span class="math notranslate nohighlight">\(\displaystyle{\frac{\partial J}{\partial\boldsymbol{\theta}_{j}^{r+1}}}\)</span>, <span class="math notranslate nohighlight">\(j= 1, 2,\dots, k_{r+1}\)</span>, con respecto a los parámetros de la capa <span class="math notranslate nohighlight">\(r + 1\)</span> que se han calculado en el paso anterior.</p>
</li>
<li><p><strong>End For</strong></p></li>
</ul>
</li>
</ul>
</div>
<ul class="simple">
<li><p>El esquema de cálculo hacia atrás <em><strong>backpropagation</strong></em> es una aplicación directa de la <em><strong>regla de la cadena para las derivadas</strong></em>, y comienza con el paso inicial de <em><strong>calcular las derivadas asociadas a la última capa (de salida)</strong></em>, que resulta ser sencillo.</p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">Observación</p>
<ul class="simple">
<li><p>La última derivada en el algoritmo backpropagation es sencilla porque <em><strong>se calcula directamente a partir de la función de pérdida y las salidas de la red</strong></em>, sin necesidad de propagar errores hacia atrás a través de múltiples capas</p></li>
</ul>
</div>
<ul>
<li><p>A continuación, el algoritmo “fluye” hacia atrás en la jerarquía de capas. Esto se debe a la naturaleza de la red multicapa, donde las <em><strong>salidas, capa tras capa, se forman como funciones de funciones</strong></em>. En efecto, centrémonos en la salida <span class="math notranslate nohighlight">\(y_{k}^{r}\)</span> de la neurona <span class="math notranslate nohighlight">\(k\)</span> en la capa <span class="math notranslate nohighlight">\(r\)</span>. Entonces tenemos</p>
<div class="math notranslate nohighlight">
\[
    y_{k}^{r}=f(\boldsymbol{\theta}_{k}^{r^T}\boldsymbol{y}^{r-1}),\quad k=1,2,\dots, k_{r},
    \]</div>
<p>donde <span class="math notranslate nohighlight">\(\boldsymbol{y}^{r-1}\)</span> es el vector (ampliado) que comprende todas las salidas de la capa anterior, <span class="math notranslate nohighlight">\(r-1\)</span>, y <span class="math notranslate nohighlight">\(f\)</span> denota la no-linealidad.</p>
</li>
</ul>
<ul>
<li><p>De acuerdo con lo anterior, la <em><strong>salida de la</strong></em> <span class="math notranslate nohighlight">\(j\)</span><em><strong>-ésima neurona en la siguiente capa</strong></em> viene dada por</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    y_{j}^{r+1}=f(\boldsymbol{\theta}_{j}^{r+1^T}\boldsymbol{y}^{r})=f\left(\boldsymbol{\theta}_{j}^{r+1^{T}}
    \begin{bmatrix}
    1\\
    f(\Theta^{r}\boldsymbol{y}^{r-1})
    \end{bmatrix}
    \right),
    \end{split}\]</div>
<p>donde <span class="math notranslate nohighlight">\(\Theta^{r}:=[\boldsymbol{\theta}_{1}^{r}, \boldsymbol{\theta}_{2}^{r},\dots,\boldsymbol{\theta}_{k_{r}}^{r}]^{T}\)</span> denota la matriz cuyas columnas corresponden al vector de pesos en el layer <span class="math notranslate nohighlight">\(r\)</span>.</p>
</li>
</ul>
<ul class="simple">
<li><p>Nótese que obtuvimos <em><strong>evaluación de “una función interna bajo una función externa”</strong></em>. Claramente, esto continúa a medida que avanzamos en la jerarquía. Esta <em><strong>estructura de evaluación de funciones internas por funciones externas</strong></em>, es el subproducto de la <em><strong>naturaleza multicapa de las redes neuronales, la cual es una operación altamente no lineal</strong></em>, que da lugar a la dificultad de calcular los gradientes, a diferencia de otros modelos, como por ejemplo <em><strong>SVM</strong></em>.</p></li>
<li><p>Sin embargo, se puede observar fácilmente que <em><strong><code class="docutils literal notranslate"><span class="pre">el</span> <span class="pre">cálculo</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">gradientes</span> <span class="pre">con</span> <span class="pre">respecto</span> <span class="pre">a</span> <span class="pre">los</span> <span class="pre">parámetros</span> <span class="pre">que</span> <span class="pre">definen</span> <span class="pre">la</span> <span class="pre">capa</span> <span class="pre">de</span> <span class="pre">salida</span> <span class="pre">no</span> <span class="pre">plantea</span> <span class="pre">ninguna</span> <span class="pre">dificultad</span></code></strong></em>. En efecto, la salida de la <span class="math notranslate nohighlight">\(j\)</span>-ésima neurona de la última capa (que es en realidad la respectiva estimación de salida actual) se escribe como:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\hat{y}_{j}:=y_{j}^{L}=f(\boldsymbol{\theta}_{j}^{L^{T}}\boldsymbol{y}^{L-1}).
\]</div>
<ul class="simple">
<li><p>Dado que <span class="math notranslate nohighlight">\(\boldsymbol{y}^{L-1}\)</span> <em><strong>es conocido, después de los cálculos durante el paso adelante</strong></em>, tomando la derivada con respecto a <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{j}^{L}\)</span> es sencillo; <em><strong>no hay ninguna operación de función sobre función</strong></em>. Por esto es que <em><strong>empezamos por la capa superior y luego nos movemos hacia atrás</strong></em>. Debido a su <em><strong>importancia histórica</strong></em>, se dará la derivación completa del algoritmo <em><strong>backpropagation</strong></em>.</p></li>
</ul>
<ul>
<li><p>Para la derivación detallada del algoritmo backpropagation, <em><strong>se adopta como ejemplo la función de pérdida del error cuadrático</strong></em>, es decir</p>
<div class="math notranslate nohighlight" id="equation-gradient-desc-scheme">
<span class="eqno">(4.4)<a class="headerlink" href="#equation-gradient-desc-scheme" title="Link to this equation">#</a></span>\[
    J(\boldsymbol{\theta})=\sum_{n=1}^{N}J_{n}(\boldsymbol{\theta})\quad\text{y}\quad J_{n}(\boldsymbol{\theta})=\frac{1}{2}\sum_{k=1}^{k_{L}}(\hat{y}_{nk}-y_{nk})^{2},
    \]</div>
<p>donde <span class="math notranslate nohighlight">\(\hat{y}_{nk},~k=1,2,\dots,k_{L}\)</span>, son las <em><strong>estimaciones proporcionadas en los correspondientes nodos de salida de la red</strong></em>. Las consideraremos como los elementos de un vector correspondiente, <span class="math notranslate nohighlight">\(\hat{\boldsymbol{y}}_{n}\)</span>.</p>
</li>
</ul>
</section>
<section id="calculo-de-gradientes">
<h2><span class="section-number">4.7. </span>Cálculo de gradientes<a class="headerlink" href="#calculo-de-gradientes" title="Link to this heading">#</a></h2>
<ul>
<li><p>Sea <span class="math notranslate nohighlight">\(z_{nj}^{r}\)</span> la <em><strong>salida del combinador lineal</strong></em> de la <span class="math notranslate nohighlight">\(j\)</span>-ésima neurona en la capa <span class="math notranslate nohighlight">\(r\)</span> en el <em><strong>instante de tiempo</strong></em> <span class="math notranslate nohighlight">\(n\)</span>, cuando se aplica el patrón <span class="math notranslate nohighlight">\(\boldsymbol{x}_{n}\)</span> en los nodos de entrada (véase la <a class="reference internal" href="#synaptic-weights-link"><span class="std std-numref">Fig. 4.13</span></a>). Entonces, para <span class="math notranslate nohighlight">\(n, j\)</span> fijos, podemos escribir</p>
<div class="math notranslate nohighlight" id="equation-eq-znj">
<span class="eqno">(4.5)<a class="headerlink" href="#equation-eq-znj" title="Link to this equation">#</a></span>\[
    z_{nj}^{r}=\sum_{m=1}^{k_{r-1}}\theta_{jm}^{r}y_{nm}^{r-1}+\theta_{j0}^{r}=\sum_{m=0}^{k_{r-1}}\theta_{jm}^{r}y_{nm}^{r-1}=\boldsymbol{\theta}_{j}^{r^{T}}\boldsymbol{y}_{n}^{r-1},
    \]</div>
<p>donde por definición</p>
<div class="math notranslate nohighlight">
\[
    \boldsymbol{y}_{n}^{r-1}:=[1, y_{n1}^{r-1},\dots, y_{nk_{r-1}}^{r-1}]^{T},
    \]</div>
<p>y <span class="math notranslate nohighlight">\(y_{n0}^{r}\equiv 1,~\forall~r, n\)</span> y <span class="math notranslate nohighlight">\(\theta_{j}^{r}\)</span> ha sido definido en la Ecuación <a class="reference internal" href="#equation-parameters-vector-def">(4.2)</a>.</p>
</li>
</ul>
<ul class="simple">
<li><p>Para las neuronas de la capa de salida <span class="math notranslate nohighlight">\(r=L,~y_{nm}^{L}=\hat{y}_{nm},~m=1,2,\dots, k_{L}\)</span>, y para <span class="math notranslate nohighlight">\(r=1\)</span>, tenemos <span class="math notranslate nohighlight">\(y_{nm}^{1}=x_{nm},~m=1,2,\dots, k_{1}\)</span>; esto es, <span class="math notranslate nohighlight">\(y_{nm}^{1}\)</span> se fijan iguales a los <em><strong>valores de las características de entrada</strong></em>.</p></li>
</ul>
<ul class="simple">
<li><p>Por lo tanto, podemos escribir ahora</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\frac{\partial J_{n}}{\partial\boldsymbol{\theta}_{j}^{r}}=\frac{\partial J_{n}}{\partial z_{nj}^{r}}\frac{\partial z_{nj}^{r}}{\partial\boldsymbol{\theta}_{j}^{r}}=\frac{\partial J_{n}}{\partial z_{nj}^{r}}\boldsymbol{y}_{n}^{r-1}.
\]</div>
<ul class="simple">
<li><p>Definamos</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\delta_{nj}^{r}:=\frac{\partial J_{n}}{\partial z_{nj}^{r}}.
\]</div>
<ul class="simple">
<li><p>Entonces la Ecuación <a class="reference internal" href="#equation-update-equations-gd">(4.3)</a> puede escribirse como</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-eq-delta-theta-jr">
<span class="eqno">(4.6)<a class="headerlink" href="#equation-eq-delta-theta-jr" title="Link to this equation">#</a></span>\[
\Delta\boldsymbol{\theta}_{j}^{r}=\left.-\mu\frac{\partial J}{\partial\boldsymbol{\theta}_{j}^{r}}\right|_{\boldsymbol{\theta}_{j}^{r}(\text{old})}=-\mu\frac{\partial}{\partial\boldsymbol{\theta}_{j}^{r}}\left.\sum_{n=1}^{N}J_{n}\right|_{\boldsymbol{\theta}_{j}^{r}(\text{old})}=-\mu\sum_{n=1}^{N}\delta_{nj}^{r}\boldsymbol{y}_{n}^{r-1},\quad r=1,2,\dots,L.
\]</div>
</section>
<section id="calculo-de-delta-nj-r">
<h2><span class="section-number">4.8. </span>Cálculo de <span class="math notranslate nohighlight">\(\delta_{nj}^{r}\)</span><a class="headerlink" href="#calculo-de-delta-nj-r" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Este es el <em><strong>cálculo principal del algoritmo backpropagation</strong></em>. Para el cálculo de los gradientes, <span class="math notranslate nohighlight">\(\delta_{nj}^{r}\)</span>, se <em><strong>comienza en la última capa</strong></em>, <span class="math notranslate nohighlight">\(r = L\)</span>, y se <em><strong>procede hacia atrás</strong></em>, hacia <span class="math notranslate nohighlight">\(r = 1\)</span>; esta “filosofía” justifica el nombre dado al algoritmo.</p></li>
</ul>
<ol class="arabic">
<li><p><span class="math notranslate nohighlight">\(r=L\)</span>: Tenemos que</p>
<div class="math notranslate nohighlight">
\[
    \delta_{nj}^{L}:=\frac{\partial J_{n}}{\partial z_{nj}^{L}}.
    \]</div>
<p>Para la función de pérdida del <em><strong>error al cuadrado</strong></em>,</p>
<div class="math notranslate nohighlight">
\[
    J_{n}=\frac{1}{2}\sum_{k=1}^{k_{L}}\left(\hat{y}_{nk}-y_{nk}\right)^{2}=\frac{1}{2}\sum_{k=1}^{k_{L}}\left(f(z_{nk}^{L})-y_{nk}\right)^{2}.
    \]</div>
<p>Por lo tanto,</p>
<div class="math notranslate nohighlight" id="equation-eq-delta-njl">
<span class="eqno">(4.7)<a class="headerlink" href="#equation-eq-delta-njl" title="Link to this equation">#</a></span>\[\begin{split}
    \begin{align*}
    \delta_{nj}^{L}=\frac{\partial}{\partial z_{nj}^{L}}\left(\frac{1}{2}\sum_{k=1}^{k_{L}}\left(f(z_{nk}^{L})-y_{nk}\right)^{2}\right)&amp;=(f(z_{nj}^{L})-y_{nj})f'(z_{nj}^{L})\\
    &amp;=(\hat{y}_{nj}-y_{nj})f'(z_{nj}^{L})=e_{nj}f'(z_{nj}^{L}),
    \end{align*}
    \end{split}\]</div>
<p>donde <span class="math notranslate nohighlight">\(j=1,2,\dots, k_{L}\)</span>, <span class="math notranslate nohighlight">\(f'\)</span> denota la derivada de <span class="math notranslate nohighlight">\(f\)</span> y <span class="math notranslate nohighlight">\(e_{nj}\)</span> es el error asociado con el <span class="math notranslate nohighlight">\(j\)</span>-ésima output en el tiempo <span class="math notranslate nohighlight">\(n\)</span>. <em><strong>Nótese que para el último layer, el cálculo del gradiente</strong></em>, <span class="math notranslate nohighlight">\(\delta_{nj}^{L}\)</span> <em><strong>es sencillo</strong></em>.</p>
</li>
</ol>
<ol class="arabic" start="2">
<li><p><span class="math notranslate nohighlight">\(r&lt;L\)</span>: Debido a la <em><strong>dependencia sucesiva entre las capas</strong></em>, el valor de <span class="math notranslate nohighlight">\(z_{nj}^{r-1}\)</span> <em><strong>influye en todos los valores</strong></em> <span class="math notranslate nohighlight">\(z_{nk}^{r},~k = 1, 2,\dots, k_{r}\)</span>, <em><strong>de la capa siguiente</strong></em>. Empleando la <em><strong>regla de la cadena</strong></em> para la diferenciación, obtenemos, para <span class="math notranslate nohighlight">\(r=L, L-1,\dots, 2\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-partial-der-znk1">
<span class="eqno">(4.8)<a class="headerlink" href="#equation-partial-der-znk1" title="Link to this equation">#</a></span>\[
    \delta_{nj}^{r-1}=\frac{\partial J_{n}}{\partial z_{nj}^{r-1}}=\sum_{k=1}^{k_{r}}\frac{\partial J_{n}}{\partial z_{nk}^{r}}\frac{\partial z_{nk}^{r}}{\partial z_{nj}^{r-1}},
    \]</div>
<p>o</p>
<div class="math notranslate nohighlight" id="equation-partial-der-znk2">
<span class="eqno">(4.9)<a class="headerlink" href="#equation-partial-der-znk2" title="Link to this equation">#</a></span>\[
    \delta_{nj}^{r-1}=\sum_{k=1}^{k_{r}}\delta_{nk}^{r}\frac{\partial z_{nk}^{r}}{\partial z_{nj}^{r-1}}.
    \]</div>
<p>Además, <em><strong>usando Ecuación</strong></em> <a class="reference internal" href="#equation-eq-znj">(4.5)</a> se tiene,</p>
<div class="math notranslate nohighlight">
\[
    \frac{\partial z_{nk}^{r}}{\partial z_{nj}^{r-1}}=\frac{\partial}{\partial z_{nj}^{r-1}}\left(\sum_{m=0}^{k_{r-1}}\theta_{km}^{r}y_{nm}^{r-1}\right),
    \]</div>
<p>donde <span class="math notranslate nohighlight">\(\textcolor{blue}{y_{nm}^{r-1}=f(z_{nm}^{r-1})}=f(\boldsymbol{\theta}_{m}^{r-1^{T}}\boldsymbol{y}_{n}^{r-2})\)</span>. Entonces,</p>
<div class="math notranslate nohighlight">
\[
    \frac{\partial z_{nk}^{r}}{\partial z_{nj}^{r-1}}=\theta_{kj}^{r}f'(z_{nj}^{r-1}),
    \]</div>
<p>y combinando las Ecuaciones <a class="reference internal" href="#equation-partial-der-znk1">(4.8)</a> y <a class="reference internal" href="#equation-partial-der-znk2">(4.9)</a>, obtenemos la regla recursiva</p>
<div class="math notranslate nohighlight">
\[
    \delta_{nj}^{r-1}=\left(\sum_{k=1}^{k_{r}}\delta_{nk}^{r}\theta_{kj}^{r}\right)f'(z_{nj}^{r-1}).
    \]</div>
</li>
</ol>
<ul>
<li><p>Manteniendo la misma notación en la Ecuación <a class="reference internal" href="#equation-eq-delta-njl">(4.7)</a>, definimos</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \\[1mm]
    e_{nj}^{r-1}:=\sum_{k=1}^{k_{r}}\delta_{nk}^{r}\theta_{kj}^{r},
    \end{split}\]</div>
<p>y finalmente obtenemos,</p>
<div class="math notranslate nohighlight" id="equation-eq-delta-njr-1">
<span class="eqno">(4.10)<a class="headerlink" href="#equation-eq-delta-njr-1" title="Link to this equation">#</a></span>\[
    \delta_{nj}^{r-1}=e_{nj}^{r-1}f'(z_{nj}^{r-1}).
    \]</div>
</li>
</ul>
<ul class="simple">
<li><p>El único cálculo que queda es la derivada de <span class="math notranslate nohighlight">\(f\)</span>. Para el caso de la <em><strong>función sigmoidea logística</strong></em> se demuestra fácilmente que es igual a</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
f'(z)=af(z)(1-f(z)).
\]</div>
<ul class="simple">
<li><p>La derivación se ha completado y el esquema <em><strong>backpropagation neural network</strong></em> se resume en el siguiente algoritmo</p></li>
</ul>
<div class="proof algorithm admonition" id="my_algorithm_backpropagation">
<p class="admonition-title"><span class="caption-number">Algorithm 4.2 </span> (Algoritmo Backpropagation Gradiente descendente)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Inicialización</strong></p>
<ol class="arabic simple">
<li><p><em><strong>Inicializar todos los pesos y sesgos sinápticos al azar con valores pequeños</strong></em>, pero no muy pequeños.</p></li>
<li><p><em><strong>Seleccione el tamaño del paso</strong></em> <span class="math notranslate nohighlight">\(\mu\)</span>.</p></li>
<li><p>Fije <span class="math notranslate nohighlight">\(y_{nj}^{1}=x_{nj},\quad j=1,2,\dots,k_{1}:=l,\quad n=1,2,\dots,N\)</span></p></li>
</ol>
<p><strong>Repeat</strong> Cada repetición completa un <em><strong>epoch</strong></em></p>
<ol class="arabic">
<li><p><strong>For</strong> <span class="math notranslate nohighlight">\(n=1,2,\dots,N\)</span> <strong>Do</strong></p>
<ol class="arabic">
<li><p><strong>For</strong> <span class="math notranslate nohighlight">\(r=1,2,\dots,L\)</span> <strong>Do</strong> Cálculo <em><strong>Forward</strong></em></p>
<ol class="arabic">
<li><p><strong>For</strong> <span class="math notranslate nohighlight">\(j=1,2,\dots,k_{r}\)</span> <strong>Do</strong></p>
<p>Calcule <span class="math notranslate nohighlight">\(z_{nj}^{r}\)</span> a partir de la Ecuación <a class="reference internal" href="#equation-eq-znj">(4.5)</a>
Calcule <span class="math notranslate nohighlight">\(y_{nj}^{r}=f(z_{nj}^{r})\)</span></p>
</li>
<li><p><strong>End For</strong></p></li>
</ol>
</li>
<li><p><strong>End For</strong></p></li>
<li><p><strong>For</strong> <span class="math notranslate nohighlight">\(j = 1, 2,\dots, k_{L}\)</span>, <strong>Do</strong>; Cálculo <em><strong>Backward</strong></em> (<em><strong>output layer</strong></em>)</p>
<p>Calcule <span class="math notranslate nohighlight">\(\delta_{nj}^{L}\)</span> a partir de la Ecuación <a class="reference internal" href="#equation-eq-delta-njr-1">(4.10)</a></p>
</li>
<li><p><strong>End For</strong></p></li>
<li><p><strong>For</strong> <span class="math notranslate nohighlight">\(r=L, L-1,\dots, 2\)</span>, <strong>Do</strong>; Cálculo <em><strong>Backward</strong></em> (<em><strong>hidden layers</strong></em>)</p>
<ol class="arabic">
<li><p><strong>For</strong> <span class="math notranslate nohighlight">\(j=1,2,\dots, k_{r}\)</span>, <strong>Do</strong></p>
<p>Calcule <span class="math notranslate nohighlight">\(\delta_{nj}^{r-1}\)</span> a partir de la Ecuación <a class="reference internal" href="#equation-eq-delta-njr-1">(4.10)</a></p>
</li>
<li><p><strong>End For</strong></p></li>
</ol>
</li>
<li><p><strong>End For</strong></p></li>
</ol>
</li>
<li><p><strong>End For</strong></p></li>
<li><p><strong>For</strong> <span class="math notranslate nohighlight">\(r=1,2,\dots,L\)</span>, <strong>Do</strong>: Actualice los pesos</p>
<ol class="arabic">
<li><p><strong>For</strong> <span class="math notranslate nohighlight">\(j=1,2,\dots,k_{r}\)</span>, <strong>Do</strong></p>
<p>Calcule <span class="math notranslate nohighlight">\(\Delta\boldsymbol{\theta}_{j}^{r}\)</span> a partir de la Ecuación <a class="reference internal" href="#equation-eq-delta-theta-jr">(4.6)</a></p>
<p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{j}^{r}=\boldsymbol{\theta}_{j}^{r}+\Delta\boldsymbol{\theta}_{j}^{r}\)</span></p>
</li>
<li><p><strong>End For</strong></p></li>
</ol>
</li>
<li><p><strong>End For</strong></p></li>
<li><p><strong>Until</strong> Un criterio de parada se cumpla.</p></li>
</ol>
</section>
</div><ul class="simple">
<li><p>El algoritmo de <em><strong>backpropagation</strong></em> puede reivindicar una serie de padres. La popularización del algoritmo se asocia con el artículo clásico <span id="id5">[<a class="reference internal" href="biblio.html#id33" title="David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-propagating errors. nature, 323(6088):533–536, 1986.">Rumelhart <em>et al.</em>, 1986</a>]</span>, donde se proporciona la derivación del algoritmo. La idea de <em><strong>backpropagation</strong></em> también aparece en <span id="id6">[<a class="reference internal" href="biblio.html#id34" title="Arthur E Bryson Jr, Walter F Denham, and Stewart E Dreyfus. Optimal programming problems with inequality constraints. AIAA journal, 1(11):2544–2550, 1963.">Bryson Jr <em>et al.</em>, 1963</a>]</span> en el contexto del control óptimo.</p></li>
<li><p>Existen diferentes variaciones del algoritmo <em><strong>backpropagation</strong></em>, tales como: <em><strong>Gradiende descendente con término de momento, Algoritmo de momentos de Nesterov’s, Algoritmo AdaGrad, RMSProp con momento de Nesterov, Algortimo de estimación de momentos adaptativo</strong></em> los cuales pueden ser utlizados para resolver la tarea de optimización (ver <span id="id7">[<a class="reference internal" href="biblio.html#id31" title="S. Theodoridis. Machine Learning: A Bayesian and Optimization Perspective. Elsevier Science, 2020. ISBN 9780128188040. URL: https://books.google.com.co/books?id=l-nEDwAAQBAJ.">Theodoridis, 2020</a>]</span>).</p></li>
</ul>
</section>
<section id="las-capas-ocultas">
<h2><span class="section-number">4.9. </span>Las capas ocultas<a class="headerlink" href="#las-capas-ocultas" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><em><strong>¿Cuántas capas ocultas?</strong></em>. Si los datos son <em><strong>linealmente separables (lo que a menudo se sabe cuando se empieza a codificar una ANN, SVM puede servir de test)</strong></em>, entonces no se necesita ninguna capa oculta. Por supuesto, tampoco se necesita una ANN para resolver los datos, pero está seguirá haciendo su trabajo.</p></li>
<li><p>Sobre la configuración de las capas ocultas en las ANNs, existe un consenso dentro de este tema, y es la diferencia de rendimiento al añadir capas ocultas adicionales: <em><strong>las situaciones en las que el rendimiento mejora con una segunda (o tercera, etc.) capa oculta son muy pocas</strong></em>. Una capa oculta es suficiente para la gran mayoría de los problemas.</p></li>
<li><p>Entonces, <em><strong>¿qué pasa con el tamaño de la(s) capa(s) oculta(s), cuántas neuronas?</strong></em>. Existen algunas reglas empíricas; de ellas, la más utilizada es <strong><code class="docutils literal notranslate"><span class="pre">'The</span> <span class="pre">optimal</span> <span class="pre">size</span> <span class="pre">of</span> <span class="pre">the</span> <span class="pre">hidden</span> <span class="pre">layer</span> <span class="pre">is</span> <span class="pre">usually</span> <span class="pre">between</span> <span class="pre">the</span> <span class="pre">size</span> <span class="pre">of</span> <span class="pre">the</span> <span class="pre">input</span> <span class="pre">and</span> <span class="pre">size</span> <span class="pre">of</span> <span class="pre">the</span> <span class="pre">output</span> <span class="pre">layers'</span></code></strong>. <code class="docutils literal notranslate"><span class="pre">Jeff</span> <span class="pre">Heaton,</span> <span class="pre">the</span> <span class="pre">author</span> <span class="pre">of</span> <span class="pre">Introduction</span> <span class="pre">to</span> <span class="pre">Neural</span> <span class="pre">Networks</span> <span class="pre">in</span> <span class="pre">Java</span></code>.</p></li>
</ul>
<ul>
<li><p>Hay una regla empírica adicional que ayuda en los problemas de aprendizaje supervisado. Normalmente <em><strong>se puede evitar el sobreajuste si se mantiene el número de neuronas por debajo de</strong></em>:</p>
<div class="math notranslate nohighlight">
\[
    N_{h}=\frac{N_{s}}{(\alpha\cdot(N_{i}+N_{o}))}
    \]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(N_{i}=\)</span> número de neuronas de entrada</p></li>
<li><p><span class="math notranslate nohighlight">\(N_{o}=\)</span> número de neuronas de salida</p></li>
<li><p><span class="math notranslate nohighlight">\(N_{s}=\)</span> número de muestras en el conjunto de datos de entrenamiento</p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha=\)</span> un factor de escala arbitrario, normalmente 2-10</p></li>
</ul>
</li>
<li><p>Un valor de <span class="math notranslate nohighlight">\(\alpha=2\)</span> suele funcionar <em><strong>sin sobreajustar</strong></em>. Se puede pensar en <span class="math notranslate nohighlight">\(\alpha\)</span> como el <em><strong>factor de ramificación efectivo o el número de pesos distintos de cero para cada neurona</strong></em>. Las capas de salida harán que el factor de ramificación “efectivo” sea muy inferior al factor de ramificación medio real de la red. Para <em><strong>profundizar mas en el diseño de redes neuronales, ver el siguiente texto de</strong></em> <a class="reference external" href="https://hagan.okstate.edu/nnd.html">Martin Hagan</a>.</p></li>
</ul>
<ul class="simple">
<li><p>En resumen, para la mayoría de los problemas, probablemente se podría obtener un rendimiento decente (incluso sin un segundo paso de optimización) estableciendo la configuración de la capa oculta utilizando sólo dos reglas:</p>
<ul>
<li><p><em><strong>el número de capas ocultas es igual a uno</strong></em></p></li>
<li><p><em><strong>el número de neuronas de esa capa es la media de las neuronas de las capas de entrada y salida.</strong></em></p></li>
</ul>
</li>
</ul>
</section>
<section id="redes-neuronales-recurrentes">
<h2><span class="section-number">4.10. </span>Redes Neuronales Recurrentes<a class="headerlink" href="#redes-neuronales-recurrentes" title="Link to this heading">#</a></h2>
<div class="tip admonition">
<p class="admonition-title">Introducción</p>
<ul class="simple">
<li><p>Nuestro interés en esta sección se centra en el caso de los <em><strong>datos secuenciales</strong></em>. Es decir, los <em><strong>vectores de entrada no son independientes, sino que aparecen en secuencia</strong></em>. Además, el <em><strong>orden específico en que se producen encierra información importante</strong></em>. Por ejemplo, este tipo de secuencias se dan en el <strong><code class="docutils literal notranslate"><span class="pre">reconocimiento</span> <span class="pre">del</span> <span class="pre">habla</span> <span class="pre">y</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">procesamiento</span> <span class="pre">del</span> <span class="pre">lenguaje,</span> <span class="pre">como</span> <span class="pre">la</span> <span class="pre">traducción</span> <span class="pre">automática,</span> <span class="pre">así</span> <span class="pre">como</span> <span class="pre">también</span> <span class="pre">el</span> <span class="pre">pronóstico</span> <span class="pre">de</span> <span class="pre">series</span> <span class="pre">de</span> <span class="pre">tiempo</span> <span class="pre">financieras</span></code></strong>. Sin duda, <em><strong>la secuencia en la que se producen las palabras es de suma importancia</strong></em>.</p></li>
</ul>
</div>
<div class="important admonition">
<p class="admonition-title">Redes Neuronales Recurrentes</p>
<ul>
<li><p>Las variables que intervienen en una <em><strong>RNN</strong></em> son:</p>
<ul class="simple">
<li><p><em><strong>Vector de estado en el tiempo</strong></em> <span class="math notranslate nohighlight">\(n\)</span><em><strong>, denotado como</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{h}_{n}\)</span>. El símbolo nos recuerda que <span class="math notranslate nohighlight">\(\boldsymbol{h}\)</span> es un vector de variables ocultas (<em><code class="docutils literal notranslate"><span class="pre">hidden</span> <span class="pre">layer</span></code></em>); <em><strong>el vector de estado constituye la memoria del sistema</strong></em>,</p></li>
<li><p><em><strong>Vector de entrada en el momento</strong></em> <span class="math notranslate nohighlight">\(n\)</span>, denominado <span class="math notranslate nohighlight">\(\boldsymbol{x}_{n}\)</span>,</p></li>
<li><p><em><strong>Vector de salida en el momento</strong></em> <span class="math notranslate nohighlight">\(n\)</span>, <span class="math notranslate nohighlight">\(\hat{\boldsymbol{y}}_{n}\)</span>, y el vector de salida objetivo, <span class="math notranslate nohighlight">\(\boldsymbol{y}_{n}\)</span>.</p></li>
</ul>
</li>
<li><p>El modelo se describe mediante un <em><strong>conjunto de matrices y vectores de parámetros desconocidos</strong></em>, a saber, <span class="math notranslate nohighlight">\(U, W, V , \boldsymbol{b}\)</span> y <span class="math notranslate nohighlight">\(\boldsymbol{c}\)</span>, que <em><strong>deben aprenderse durante el entrenamiento</strong></em>.</p></li>
<li><p>Las <em><strong>ecuaciones que describen un modelo RNN</strong></em> son</p>
<div class="math notranslate nohighlight" id="equation-rnn-system-eq">
<span class="eqno">(4.11)<a class="headerlink" href="#equation-rnn-system-eq" title="Link to this equation">#</a></span>\[\begin{split}
    \begin{align*}
    \boldsymbol{h}_{n}&amp;=f(U\boldsymbol{x}_{n}+W\boldsymbol{h}_{n-1}+\boldsymbol{b})\\
    \hat{\boldsymbol{y}}_{n}&amp;=g(V\boldsymbol{h}_{n}+\boldsymbol{c}).
    \end{align*}
    \end{split}\]</div>
<p>donde <em><strong>las funciones no lineales</strong></em> <span class="math notranslate nohighlight">\(f\)</span> y <span class="math notranslate nohighlight">\(g\)</span> <em>actúan elemento a elemento (element-wise)</em> y <em><strong>se aplican individualmente a cada elemento de sus argumentos vectoriales</strong></em>.</p>
</li>
<li><p>En otras palabras, <em><strong>una vez que se ha observado un nuevo vector de entrada, se actualiza el vector de estado</strong></em>. Su nuevo valor <em><strong>depende de la información más reciente, transmitida por la entrada</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{x}_{n}\)</span> así como de la <em><strong>historia pasada, ya que esta se ha acumulado en</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{h}_{n-1}\)</span>. La salida depende del <em><strong>vector de estado actualizado</strong></em>, <span class="math notranslate nohighlight">\(\boldsymbol{h}_{n}\)</span>. Es decir, <em><strong>depende de la “historia” hasta el instante actual</strong></em> <span class="math notranslate nohighlight">\(n\)</span>, tal y como se expresa en <span class="math notranslate nohighlight">\(\boldsymbol{h}_{n}\)</span>.</p></li>
<li><p>Las opciones típicas para <span class="math notranslate nohighlight">\(f\)</span> son la <em><strong>tangente hiperbólica</strong></em>, <code class="docutils literal notranslate"><span class="pre">tanh</span></code>, o las <em><strong>no linealidades</strong></em> <code class="docutils literal notranslate"><span class="pre">ReLU</span></code>. El valor inicial <span class="math notranslate nohighlight">\(\boldsymbol{h}_{0}\)</span> <em><strong>suele ser igual al vector cero</strong></em>. La <em><strong>no linealidad de salida,</strong></em> <span class="math notranslate nohighlight">\(g\)</span><em><strong>, se elige a menudo para ser la función</strong></em> <code class="docutils literal notranslate"><span class="pre">softmax</span></code>.</p></li>
</ul>
</div>
<figure class="align-center" id="recurrent-neural-network-arch-numref">
<a class="reference internal image-reference" href="_images/recurrent_neural_network_arch.png"><img alt="_images/recurrent_neural_network_arch.png" src="_images/recurrent_neural_network_arch.png" style="width: 552.0px; height: 304.8px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.14 </span><span class="caption-text">Arquitectura de una <em><strong>Red Neuronal Recurrente</strong></em>. Fuente <span id="id8">[<a class="reference internal" href="biblio.html#id31" title="S. Theodoridis. Machine Learning: A Bayesian and Optimization Perspective. Elsevier Science, 2020. ISBN 9780128188040. URL: https://books.google.com.co/books?id=l-nEDwAAQBAJ.">Theodoridis, 2020</a>]</span>.</span><a class="headerlink" href="#recurrent-neural-network-arch-numref" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>De las ecuaciones anteriores se deduce que las <em><strong>matrices y vectores de parámetros se comparten en todos los instantes temporales</strong></em>. Durante el entrenamiento, <em><strong>se inicializan mediante números aleatorios</strong></em>. El modelo asociado con la Ecuación <a class="reference internal" href="#equation-rnn-system-eq">(4.11)</a> se muestra en la <a class="reference internal" href="#recurrent-neural-network-arch-numref"><span class="std std-numref">Fig. 4.14</span></a>A.</p></li>
<li><p>En la <a class="reference internal" href="#recurrent-neural-network-arch-numref"><span class="std std-numref">Fig. 4.14</span></a>B, <em><strong>el gráfico se despliega sobre los distintos instantes de tiempo</strong></em> para los que se dispone de observaciones. Por ejemplo, <em><strong>si la secuencia de interés es una frase de 10 palabras</strong></em>, entonces <span class="math notranslate nohighlight">\(N\)</span> se establece igual a 10, mientras que <span class="math notranslate nohighlight">\(\boldsymbol{x}_{n}\)</span> es el <em><strong>vector que codifica las respectivas palabras de entrada</strong></em>.</p></li>
</ul>
<section id="backpropagation-en-tiempo">
<h3><span class="section-number">4.10.1. </span>Backpropagation en tiempo<a class="headerlink" href="#backpropagation-en-tiempo" title="Link to this heading">#</a></h3>
<div class="important admonition">
<p class="admonition-title">Introducción</p>
<ul class="simple">
<li><p>El entrenamiento de las <em><strong>RNN</strong></em> sigue una <em><strong>lógica similar a la del algoritmo backpropagation</strong></em> para el entrenamiento de redes neuronales de avance. Después de todo, <em><strong>una RNN puede verse como una red feed-forward con</strong></em> <span class="math notranslate nohighlight">\(N\)</span> <em><strong>capas</strong></em>. La <em><strong>capa superior es la del instante de tiempo</strong></em> <span class="math notranslate nohighlight">\(N\)</span> y la <em><strong>primera capa corresponde al instante de tiempo</strong></em> <span class="math notranslate nohighlight">\(n = 1\)</span>. Una diferencia radica en que las <em><strong>capas ocultas en una RNN también producen salidas</strong></em>, es decir, <span class="math notranslate nohighlight">\(\hat{\boldsymbol{y}}_{n}\)</span>, y se alimentan directamente con entradas. Sin embargo, <em><strong>en lo que respecta al entrenamiento, estas diferencias no afectan al razonamiento principal</strong></em>.</p></li>
</ul>
</div>
<ul class="simple">
<li><p>El <em><strong>aprendizaje de las matrices y vectores de parámetros desconocidos</strong></em> se consigue mediante un esquema de gradiente descendente, de acuerdo con Eq. <a class="reference internal" href="#equation-update-equations-gd">(4.3)</a>. Resulta que los <em><strong>gradientes requeridos de la función de coste</strong></em>, con respecto a los parámetros desconocidos, <em><strong>tienen lugar recursivamente, comenzando en el último instante de tiempo,</strong></em> <span class="math notranslate nohighlight">\(N\)</span>, y retrocediendo en el tiempo, <span class="math notranslate nohighlight">\(n = N-1, N-2,\dots,1\)</span>. Esta es la razón por la que el algoritmo se conoce como <em><strong>bakpropagation a traves del tiempo (BPTT)</strong></em>.</p></li>
</ul>
<ul class="simple">
<li><p>La <em><strong>función de coste es la suma a lo largo del tiempo,</strong></em> <span class="math notranslate nohighlight">\(n\)</span>, de las correspondientes <em><strong>contribuciones a la función de pérdida</strong></em>, que dependen de los valores respectivos de <span class="math notranslate nohighlight">\(\boldsymbol{h}_{n}, \boldsymbol{x}_{n}\)</span>, es decir,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
J(U, W, V, \boldsymbol{b}, \boldsymbol{c})=\sum_{n=1}^{N}J_{n}(U, W, V, \boldsymbol{b}, \boldsymbol{c}).
\]</div>
<ul>
<li><p>Por ejemplo, tomar <span class="math notranslate nohighlight">\(J_{n}\)</span> como <code class="docutils literal notranslate"><span class="pre">MSE</span></code> es la elección más común en series de tiempo con <code class="docutils literal notranslate"><span class="pre">BPTT</span></code> debido a su <em><strong>simplicidad y compatibilidad con problemas de predicción continua</strong></em>. Sin embargo, si necesitas una mayor robustez ante outliers, podrías considerar <code class="docutils literal notranslate"><span class="pre">MAE</span></code> o <a class="reference external" href="https://lihkir.github.io/MachineLearning/svm_model.html#regresion-de-vectores-de-soporte">Huber Loss</a></p>
<div class="math notranslate nohighlight">
\[
    J_{n}(U, W, V, \boldsymbol{b}, \boldsymbol{c}):=\frac{1}{K}\sum_{k}(y_{nk}-\hat{y}_{nk})^{2},
    \]</div>
<p>donde <em><strong>la suma es sobre la dimensionalidad de</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span>, y</p>
<div class="math notranslate nohighlight">
\[
    \hat{\boldsymbol{y}}_{n}=g(\boldsymbol{h}_{n}, V, \boldsymbol{c})~\text{y}~\boldsymbol{h}_{n}=f(\boldsymbol{x}_{n}, \boldsymbol{h}_{n-1}, U, W, \boldsymbol{b}).
    \]</div>
</li>
</ul>
<ul>
<li><p>En el corazón del <em><strong>cálculo de los gradientes de la función de coste con respecto a las diversas matrices y vectores de parámetros</strong></em> se encuentra el cálculo de los <em><strong>gradientes de</strong></em> <span class="math notranslate nohighlight">\(J\)</span> <em><strong>con respecto a los vectores de estado,</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{h}_{n}\)</span>. Una vez calculados estos últimos, el resto de los gradientes, con respecto a las <em><strong>matrices y vectores de parámetros desconocidos</strong></em>, es una tarea sencilla. Para ello, nótese que cada <span class="math notranslate nohighlight">\(h_{n},~n=1, 2,\dots,N-1\)</span>, afecta a <span class="math notranslate nohighlight">\(J\)</span> de dos maneras:</p>
<ul class="simple">
<li><p>Directamente, <em><strong>a traves de</strong></em> <span class="math notranslate nohighlight">\(J_{n}\)</span></p></li>
<li><p>Indirectamente, <em><strong>a través de la cadena que impone la estructura RNN</strong></em>, es decir,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    \boldsymbol{h}_{n}\rightarrow\boldsymbol{h}_{n+1}\rightarrow\cdots\rightarrow\boldsymbol{h}_{N}.
    \]</div>
<p>Es decir, <span class="math notranslate nohighlight">\(\boldsymbol{h}_{n}\)</span>, además de <span class="math notranslate nohighlight">\(J_{n}\)</span>, <em><strong>también afecta a todos los valores de coste posteriores</strong></em>, <span class="math notranslate nohighlight">\(J_{n+1},\dots, J_{N}\)</span>.</p>
</li>
</ul>
<ul>
<li><p>Nótese que, a través de la cadena, <span class="math notranslate nohighlight">\(\boldsymbol{h}_{n+1}=f(\boldsymbol{x}_{n+1}, \boldsymbol{h}_{n}, U, W, \boldsymbol{b})\)</span>, empleando la <em><strong>regla de la cadena</strong></em> para las derivadas, las <em><strong>dependencias anteriores conducen al siguiente cálculo recursivo</strong></em>:</p>
<div class="math notranslate nohighlight" id="equation-indirect-direct-rec-part">
<span class="eqno">(4.12)<a class="headerlink" href="#equation-indirect-direct-rec-part" title="Link to this equation">#</a></span>\[
    \frac{\partial J}{\partial\boldsymbol{h}_{n}}=\underbrace{{\left(\frac{\partial\boldsymbol{h}_{n+1}}{\partial\boldsymbol{h}_{n}}\right)^{T}}\frac{\partial J}{\partial\boldsymbol{h}_{n+1}}}_{\text{parte recursiva indirecta}}+\underbrace{\left(\frac{\partial\hat{\boldsymbol{y}}_{n}}{\partial\boldsymbol{h}_{n}}\right)^{T}\frac{\partial J}{\partial\hat{\boldsymbol{y}}_{n}}}_{\text{parte directa}},
    \]</div>
<p>donde, por definición, la <em><strong>derivada de un vector</strong></em>, digamos, <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span>, <em><strong>con respecto a otro vector</strong></em>, digamos, <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>, se define como la matriz</p>
<div class="math notranslate nohighlight">
\[
    \left[\frac{\partial\boldsymbol{y}}{\partial\boldsymbol{x}}\right]_{ij}:=\frac{\partial y_{i}}{\partial x_{j}}.
    \]</div>
</li>
</ul>
<ul class="simple">
<li><p>Nótese que el <em><strong>gradiente de la función de coste</strong></em>, con respecto a los <em><strong>parámetros ocultos (vector de estado)</strong></em> en la capa “<span class="math notranslate nohighlight">\(n\)</span>”, se da como una <em><strong>función del gradiente respectivo en la capa anterior</strong></em>, es decir, con respecto al <em><strong>vector de estado en el tiempo</strong></em> <span class="math notranslate nohighlight">\(n+1\)</span>. Las dos <em><strong>pasadas requeridas por backpropagation en el tiempo</strong></em> se resumen a continuación.</p></li>
</ul>
<div class="admonition-pasadas-de-backpropagation-en-tiempo admonition">
<p class="admonition-title">Pasadas de Backpropagation en Tiempo</p>
<ul>
<li><p><em><strong>Paso hacia adelante</strong></em>:</p>
<ul class="simple">
<li><p>Iniciando en <span class="math notranslate nohighlight">\(n=1\)</span> y <em><strong>utilizando las estimaciones actuales de las matrices y vectores de parámetros implicados</strong></em>, calcular en secuencia,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    (\boldsymbol{h}_{1}, \hat{\boldsymbol{y}}_{1})\rightarrow(\boldsymbol{h}_{2}, \hat{\boldsymbol{y}}_{2})\rightarrow\cdots\rightarrow(\boldsymbol{h}_{N}, \hat{\boldsymbol{y}}_{N}).
    \]</div>
</li>
<li><p><em><strong>Paso hacia atrás</strong></em>:</p>
<ul class="simple">
<li><p>Empezando en <span class="math notranslate nohighlight">\(n = N\)</span>, a calcular en secuencia,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    \frac{\partial J}{\partial\boldsymbol{h}_{N}}\rightarrow\frac{\partial J}{\partial\boldsymbol{h}_{N-1}}\rightarrow\cdots\rightarrow\frac{\partial J}{\partial\boldsymbol{h}_{1}}.
    \]</div>
</li>
</ul>
</div>
<ul class="simple">
<li><p>Nótese que el <em><strong>cálculo del gradiente</strong></em> <span class="math notranslate nohighlight">\(\partial J/\partial\boldsymbol{h}_{N}\)</span> es sencillo, y <em><strong>solo involucra la parte directa en</strong></em> Eq. <a class="reference internal" href="#equation-indirect-direct-rec-part">(4.12)</a>.</p></li>
<li><p>Para la implementación de la <code class="docutils literal notranslate"><span class="pre">BPTT</span></code>, se procede a</p>
<ol class="arabic simple">
<li><p><em><strong>inicializar aleatoriamente las matrices y vectores desconocidos implicados,</strong></em></p></li>
<li><p><em><strong>calcular todos los gradientes requeridos, siguiendo los pasos indicados anteriormente</strong></em>, y</p></li>
<li><p><em><strong>realizar las actualizaciones según el esquema de gradiente descendente</strong></em>.</p></li>
</ol>
</li>
<li><p>Los pasos <em><strong>(2) y (3) se realizan de forma iterativa hasta que se cumple un criterio de convergencia</strong></em>, de forma análoga al <em><strong>algoritmo estándar de backpropagation</strong></em>.</p></li>
</ul>
</section>
<section id="desvanecimiento-y-explosion-de-gradientes">
<h3><span class="section-number">4.10.2. </span>Desvanecimiento y explosión de gradientes<a class="headerlink" href="#desvanecimiento-y-explosion-de-gradientes" title="Link to this heading">#</a></h3>
<div class="information admonition">
<p class="admonition-title">Introducción</p>
<ul class="simple">
<li><p>La tarea de <em><strong>desvanecimiento y explosión de gradientes</strong></em> se ha introducido y discutido en secciones anteriores, en el contexto del algoritmo de backpropagation. <em><strong>Los mismos problemas se presentan en el algoritmo BPTT</strong></em>, dado que, este último es una <em><strong>forma específica del concepto de backpropagation</strong></em> y, como se ha dicho, una <em><strong>RNN</strong></em> puede considerarse como una <em><strong>red multicapa, donde cada instante de tiempo corresponde a una capa diferente</strong></em>. De hecho, en las <em><strong>RNN</strong></em>, el fenómeno de <em><strong>desvanecimiento/explosión de gradiente aparece de una forma bastante “agresiva”</strong></em>, teniendo en cuenta que <span class="math notranslate nohighlight">\(N\)</span> puede alcanzar valores grandes.</p></li>
</ul>
</div>
<ul class="simple">
<li><p>La naturaleza multiplicativa de la propagación de gradientes puede verse fácilmente en la Eq. <a class="reference internal" href="#equation-indirect-direct-rec-part">(4.12)</a>. Para ayudar a comprender el concepto principal, simplifiquemos el escenario y <em><strong>supongamos que sólo interviene una variante de estado</strong></em>. Entonces los vectores de estado se convierten en escalares, <span class="math notranslate nohighlight">\(h_{n}\)</span> , y la matriz <span class="math notranslate nohighlight">\(W\)</span> en un escalar <span class="math notranslate nohighlight">\(w\)</span>. Además, <em><strong>supongamos que las salidas también son escalares</strong></em>. Entonces la recursión en la Eq. <a class="reference internal" href="#equation-indirect-direct-rec-part">(4.12)</a> se simplifica como:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\frac{\partial J}{\partial h_{n}}=\frac{\partial h_{n+1}}{\partial h_{n}}\frac{\partial J}{\partial h_{n+1}}+\frac{\partial\hat{y}_{n}}{\partial h_{n}}\frac{\partial J}{\partial\hat{y}_{n}}.
\]</div>
<ul class="simple">
<li><p>Suponiendo en la Eq. <a class="reference internal" href="#equation-rnn-system-eq">(4.11)</a> que <span class="math notranslate nohighlight">\(f\)</span> <em><strong>es la función</strong></em> <span class="math notranslate nohighlight">\(\tanh(\cdot)\)</span> <em><strong>estándar</strong></em>, teniendo en cuenta que, <span class="math notranslate nohighlight">\(\text{sech}^2(\cdot)=1-\tanh^2(\cdot)\)</span> y <span class="math notranslate nohighlight">\(|\tanh(\cdot)|&lt;1\)</span>, sobre su dominio, se ve fácilmente que</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-partial-deriv-hn">
<span class="eqno">(4.13)<a class="headerlink" href="#equation-partial-deriv-hn" title="Link to this equation">#</a></span>\[
\frac{\partial h_{n+1}}{\partial h_{n}}=w(1-h_{n+1}^{2}).
\]</div>
<ul class="simple">
<li><p>Escribiendo la <em><strong>recursión para dos pasos sucesivos</strong></em>, repitiendo el proceso en Eq. <a class="reference internal" href="#equation-partial-deriv-hn">(4.13)</a>, por ejemplo, para <span class="math notranslate nohighlight">\(\partial h_{n+2}/\partial h_{n+1}\)</span> obtenemos que,</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-partialj-hn-eq">
<span class="eqno">(4.14)<a class="headerlink" href="#equation-partialj-hn-eq" title="Link to this equation">#</a></span>\[\begin{split}
\begin{align*}
\frac{\partial J}{\partial h_{n}}&amp;=\frac{\partial h_{n+1}}{\partial h_{n}}\frac{\partial J}{\partial h_{n+1}}+\frac{\partial\hat{y}_{n}}{\partial h_{n}}\frac{\partial J}{\partial\hat{y}_{n}}\\
&amp;=w^{2}(1-h_{n+1}^{2})(1-h_{n+2}^{2})\frac{\partial J}{\partial h_{n+2}}+\text{otro términos}
\end{align*}
\end{split}\]</div>
<ul class="simple">
<li><p>No es difícil ver que la <em><strong>multiplicación de los términos menores que uno puede llevar a valores de desvanecimiento</strong></em>, sobre todo si tenemos en cuenta que, en la práctica, las <em><strong>secuencias pueden ser bastante grandes, por ejemplo,</strong></em> <span class="math notranslate nohighlight">\(N = 100\)</span>. Por lo tanto <em><strong>para instantes de tiempo cercanos a</strong></em> <span class="math notranslate nohighlight">\(n = 1\)</span>, la <em><strong>contribución al gradiente del primer término del lado derecho en la</strong></em> Eq. <a class="reference internal" href="#equation-partialj-hn-eq">(4.14)</a> implicará un <em><strong>gran número de productos de números menores que uno en magnitud</strong></em>. Por otra parte, el valor de <span class="math notranslate nohighlight">\(w\)</span> estará contribuyendo en <span class="math notranslate nohighlight">\(w^{n}\)</span> potencia. Por lo tanto, <em><strong>si su valor es mayor que uno, puede conducir a valores explosivos de los gradientes respectivos</strong></em>.</p></li>
</ul>
<ul class="simple">
<li><p>En varios casos, <em><strong>se puede truncar el algoritmo de backpropagation a unos pocos pasos de tiempo</strong></em>. Otra forma, es <em><strong>sustituir la no linealidad</strong></em> <code class="docutils literal notranslate"><span class="pre">tanh</span></code> <em><strong>por</strong></em> <code class="docutils literal notranslate"><span class="pre">ReLU</span></code>. Para el caso del <em><strong>valor explosivo</strong></em>, se puede <em><strong>introducir una técnica que recorte los valores a un umbral predeterminado</strong></em>, una vez que los <em><strong>valores superen ese umbral</strong></em>. Sin embargo, otra técnica que suele emplearse en la práctica es <em><strong>sustituir la formulación RNN</strong></em> estándar descrita anteriormente por una <em><strong>estructura alternativa, que puede hacer mejor frente a estos fenómenos causados por la dependencia a largo plazo de la RNN</strong></em>.</p></li>
</ul>
<div class="proof observation admonition" id="observation_ann9">
<p class="admonition-title"><span class="caption-number">Observation 4.1 </span></p>
<section class="observation-content" id="proof-content">
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">RNN</span> <span class="pre">profundas</span></code></strong>: Además de la red <em><strong>RNN básica que comprende una sola capa de estados</strong></em>, se han propuesto <em><strong>extensiones que implican múltiples capas de estados</strong></em>, una encima de otra (ver <span id="id9">[<a class="reference internal" href="biblio.html#id36" title="Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. How to construct deep recurrent neural networks. arXiv preprint arXiv:1312.6026, 2013.">Pascanu <em>et al.</em>, 2013</a>]</span>).</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">RNN</span> <span class="pre">bidireccionales</span></code></strong>: Como su nombre indica, en las <em><strong>RNN bidireccionales hay dos variables de estado</strong></em>, es decir, una denotada como <span class="math notranslate nohighlight">\(\overset{\rightarrow}{\boldsymbol{h}}\)</span><em><strong>, que se propaga hacia adelante</strong></em>, y otra, <span class="math notranslate nohighlight">\(\overset{\leftarrow}{\boldsymbol{h}}\)</span><em><strong>, que se propaga hacia atrás</strong></em>. De este modo, <em><strong>las salidas dependen tanto del pasado como del futuro</strong></em> (ver <span id="id10">[<a class="reference internal" href="biblio.html#id37" title="Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent neural networks. In 2013 IEEE international conference on acoustics, speech and signal processing, 6645–6649. Ieee, 2013.">Graves <em>et al.</em>, 2013</a>]</span>).</p></li>
</ul>
</section>
</div></section>
<section id="implementacion">
<h3><span class="section-number">4.10.3. </span>Implementación<a class="headerlink" href="#implementacion" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Funciones de activación y su derivada (<code class="docutils literal notranslate"><span class="pre">sigmoide</span></code>)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">sigmoid_derivative</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Función de activación de salida (<code class="docutils literal notranslate"><span class="pre">lineal</span></code>)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">linear</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">linear_derivative</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Clase para el modelo <code class="docutils literal notranslate"><span class="pre">RNN</span></code> con <code class="docutils literal notranslate"><span class="pre">BPTT</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">RNN_BPTT</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
        
        <span class="c1"># Inicialización de los pesos</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">U</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span>
        
        <span class="c1"># Inicialización de los sesgos</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_size</span><span class="p">))</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">truncation</span> <span class="o">=</span> <span class="n">truncation</span>  <span class="c1"># Truncación de BPTT para evitar largos horizontes de tiempo</span>
        
    <span class="c1"># Forward pass</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">T</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">T</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>  <span class="c1"># Inicializamos el estado oculto</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>  <span class="c1"># Salida predicha</span>

        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
            <span class="c1"># Estado oculto en el tiempo t</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">t</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">U</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>
            <span class="c1"># Salida predicha en el tiempo t</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">y_hat</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">c</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_hat</span>

    <span class="c1"># Backward pass (BPTT)</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">T</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="c1"># Gradientes inicializados a cero</span>
        <span class="n">dU</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">U</span><span class="p">)</span>
        <span class="n">dW</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span>
        <span class="n">dV</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">)</span>
        <span class="n">db</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>
        <span class="n">dc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">c</span><span class="p">)</span>
        
        <span class="c1"># Inicialización del gradiente del estado oculto</span>
        <span class="n">dh_next</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">)):</span>
            
            <span class="c1"># Gradiente de la pérdida con respecto a la salida (derivada externa)</span>
            <span class="n">dy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_hat</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
            
            <span class="c1"># Gradientes para V y c</span>
            <span class="n">dV</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="p">],</span> <span class="n">dy</span><span class="p">)</span>
            <span class="n">dc</span> <span class="o">+=</span> <span class="n">dy</span>
            
            <span class="c1"># Gradiente del estado oculto</span>
            <span class="n">dh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dy</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">+</span> <span class="n">dh_next</span>
            <span class="n">dh_raw</span> <span class="o">=</span> <span class="n">dh</span> <span class="o">*</span> <span class="n">sigmoid_derivative</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="p">])</span>
            
            <span class="c1"># Gradientes para U, W y b</span>
            <span class="n">dU</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">t</span><span class="p">],</span> <span class="n">dh_raw</span><span class="p">)</span>
            <span class="n">dW</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dh_raw</span><span class="p">)</span>
            <span class="n">db</span> <span class="o">+=</span> <span class="n">dh_raw</span>

            <span class="c1"># Gradiente para la siguiente iteración (truncación de BPTT)</span>
            <span class="n">dh_next</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dh_raw</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        
        <span class="c1"># Actualizamos los pesos con el gradiente descendente</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">U</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">dU</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">dW</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">V</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">dV</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">db</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">dc</span>

    <span class="c1"># Entrenamiento</span>
    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">output</span><span class="p">))</span>  <span class="c1"># MSE</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">, Loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Dataset ejemplo (<code class="docutils literal notranslate"><span class="pre">senoidal</span></code>)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_dataset</span><span class="p">(</span><span class="n">series</span><span class="p">,</span> <span class="n">look_back</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">series</span><span class="p">)</span> <span class="o">-</span> <span class="n">look_back</span><span class="p">):</span>
        <span class="n">X</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">series</span><span class="p">[</span><span class="n">i</span><span class="p">:(</span><span class="n">i</span> <span class="o">+</span> <span class="n">look_back</span><span class="p">)])</span>
        <span class="n">y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">series</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="n">look_back</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Función para graficar la serie de tiempo original y las predicciones usando <code class="docutils literal notranslate"><span class="pre">BPTT</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_predictions</span><span class="p">(</span><span class="n">time_series</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">look_back</span><span class="p">):</span>
    <span class="c1"># Recortamos las predicciones para que coincidan con el tamaño de la serie original</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">predictions</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>  <span class="c1"># Convertir a 1D</span>
    <span class="n">prediction_shifted</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">time_series</span><span class="p">)</span>
    <span class="n">prediction_shifted</span><span class="p">[</span><span class="n">look_back</span><span class="p">:]</span> <span class="o">=</span> <span class="n">predictions</span>  <span class="c1"># Desplazamos las predicciones para alinearlas con la serie</span>

    <span class="c1"># Crear la figura</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">time_series</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Serie de tiempo original&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">prediction_shifted</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Predicciones&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>

    <span class="c1"># Añadir etiquetas y leyenda</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Comparación de la serie de tiempo y las predicciones&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Tiempo&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Valor&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

    <span class="c1"># Mostrar la figura</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Ejemplo de uso</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="c1"># Generamos una serie de tiempo simple</span>
    <span class="n">time_series</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>  <span class="c1"># Onda senoidal</span>
    <span class="n">look_back</span> <span class="o">=</span> <span class="mi">3</span>

    <span class="c1"># Preparamos los datos</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">time_series</span><span class="p">,</span> <span class="n">look_back</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">look_back</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Normalizamos los datos</span>
    <span class="n">X</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">X</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
    <span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">y</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>

    <span class="c1"># Definimos la red RNN</span>
    <span class="n">input_size</span> <span class="o">=</span> <span class="n">look_back</span>
    <span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">output_size</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>

    <span class="n">rnn</span> <span class="o">=</span> <span class="n">RNN_BPTT</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>
    <span class="n">rnn</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

    <span class="n">predictions</span> <span class="o">=</span> <span class="n">rnn</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    
    <span class="n">plot_predictions</span><span class="p">(</span><span class="n">time_series</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">look_back</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 0, Loss: 1.071363085545681
Epoch 100, Loss: 0.01064924295283284
Epoch 200, Loss: 0.006468968051710293
Epoch 300, Loss: 0.006329409041480851
Epoch 400, Loss: 0.0062034322090852
Epoch 500, Loss: 0.006080923382791597
Epoch 600, Loss: 0.005961086868890104
Epoch 700, Loss: 0.005843252688915223
Epoch 800, Loss: 0.0057268349439890676
Epoch 900, Loss: 0.0056113096615199995
</pre></div>
</div>
<img alt="_images/a2bf8d724727bfd98e7fd1831804d9cc79907cf07e3deec0ea72b1ad7b3a91f9.png" src="_images/a2bf8d724727bfd98e7fd1831804d9cc79907cf07e3deec0ea72b1ad7b3a91f9.png" />
</div>
</div>
</section>
</section>
<section id="red-de-memoria-a-largo-plazo-lstm">
<h2><span class="section-number">4.11. </span>Red de memoria a largo plazo (LSTM)<a class="headerlink" href="#red-de-memoria-a-largo-plazo-lstm" title="Link to this heading">#</a></h2>
<div class="proof observation admonition" id="observation_ann10">
<p class="admonition-title"><span class="caption-number">Observation 4.2 </span></p>
<section class="observation-content" id="proof-content">
<ul class="simple">
<li><p>La <em><strong>idea clave de la red LSTM</strong></em>, propuesta en el artículo seminal <span id="id11">[<a class="reference internal" href="biblio.html#id43" title="Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.">Hochreiter and Schmidhuber, 1997</a>]</span>, es el llamado <em><strong>estado de celda</strong></em>, que ayuda a <em><strong>superar los problemas asociados con los fenómenos de desvanecimiento/explosión</strong></em> que son causados por las <em><strong>dependencias largo plazo</strong></em> dentro de la red. Las <em><strong>redes LSTM</strong></em> tienen la capacidad incorporada de <em><strong>controlar el flujo de información que entra y sale de la memoria del sistema mediante algoritmos no lineales</strong></em>.</p></li>
<li><p>Estas puertas se implementan <em><strong>mediante la no linealidad sigmoidea y un multiplicador</strong></em>. Desde un punto de vista algorítmico, las puertas equivalen a <em><strong>aplicar una ponderación al flujo de información correspondiente</strong></em>. Los <em><strong>pesos</strong></em> se sitúan en el intervalo <span class="math notranslate nohighlight">\([0,1]\)</span> y <em><strong>dependen de los valores de las variables implicadas que activan la no linealidad sigmoidea</strong></em>.</p></li>
</ul>
</section>
</div><figure class="align-center" id="lstm-arch-rnn-numref">
<a class="reference internal image-reference" href="_images/lstm_arch_rnn.png"><img alt="_images/lstm_arch_rnn.png" src="_images/lstm_arch_rnn.png" style="width: 664.8000000000001px; height: 332.0px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.15 </span><span class="caption-text">Arquitectura de unidad LSTM. Fuente <span id="id12">[<a class="reference internal" href="biblio.html#id31" title="S. Theodoridis. Machine Learning: A Bayesian and Optimization Perspective. Elsevier Science, 2020. ISBN 9780128188040. URL: https://books.google.com.co/books?id=l-nEDwAAQBAJ.">Theodoridis, 2020</a>]</span>.</span><a class="headerlink" href="#lstm-arch-rnn-numref" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>En otras palabras, <em><strong>la ponderación (control) de la información</strong></em> tiene lugar en el contexto. Según este razonamiento, <em><strong>la red tiene la agilidad de olvidar la información que ya ha sido utilizada y ya no es necesaria</strong></em>. La <em><strong>célula/unidad LSTM básica</strong></em> se se muestra en la <a class="reference internal" href="#lstm-arch-rnn-numref"><span class="std std-numref">Fig. 4.15</span></a>. Se construye en torno a <em><strong>dos conjuntos de variables, apiladas en el vector</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{s}\)</span> (<code class="docutils literal notranslate"><span class="pre">cell</span> <span class="pre">state</span></code> <em>long-term memory</em>), que se conoce como el <em><strong>estado de la célula o unidad</strong></em>, y el vector <span class="math notranslate nohighlight">\(\boldsymbol{h}\)</span> (<code class="docutils literal notranslate"><span class="pre">hidden</span> <span class="pre">state</span></code> <em>short-term memory</em>), que se conoce como el <em><strong>vector de variables ocultas</strong></em>. Una <em><strong>red LSTM</strong></em> se construye a partir de la <em><strong>concatenación sucesiva de esta unidad básica</strong></em>. La unidad correspondiente al tiempo <span class="math notranslate nohighlight">\(n\)</span>, además del vector de entrada, <span class="math notranslate nohighlight">\(\boldsymbol{x}_{n}\)</span>, recibe <span class="math notranslate nohighlight">\(\boldsymbol{s}_{n-1}\)</span> y <span class="math notranslate nohighlight">\(\boldsymbol{h}_{n-1}\)</span> de la etapa anterior y pasa <span class="math notranslate nohighlight">\(\boldsymbol{s}_{n}\)</span> y <span class="math notranslate nohighlight">\(\boldsymbol{h}_{n}\)</span> a la siguiente.</p></li>
</ul>
<ul>
<li><p>A continuación se resumen las <em><strong>ecuaciones de actualización asociadas</strong></em>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{align*}
    \boldsymbol{f}&amp;=\sigma(U^{f}\boldsymbol{x}_{n}+W^{f}\boldsymbol{h}_{n-1}+\boldsymbol{b}^{f}),\\
    \boldsymbol{i}&amp;=\sigma(U^{i}\boldsymbol{x}_{n}+W^{i}\boldsymbol{h}_{n-1}+\boldsymbol{b}^{i}),\\
    \tilde{\boldsymbol{s}}&amp;=\tanh(U^{s}\boldsymbol{x}_{n}+W^{s}\boldsymbol{h}_{n-1}+\boldsymbol{b}^{s}),\\
    \boldsymbol{s}_{n}&amp;=\boldsymbol{s}_{n-1}\circ f+\boldsymbol{i}\circ\tilde{\boldsymbol{s}},\\
    \boldsymbol{o}&amp;=\sigma(U^{o}\boldsymbol{x}_{n}+W^{o}\boldsymbol{h}_{n-1}+\boldsymbol{b}^{o}),\\
    \boldsymbol{h}_{n}&amp;=\boldsymbol{o}\circ\tanh(\boldsymbol{s}_{n}),
    \end{align*}
    \end{split}\]</div>
<p>donde <span class="math notranslate nohighlight">\(\circ\)</span> denota el <em><strong>producto elemento a elemento entre vectores o matrices</strong></em> (producto de <em><strong>Hadamard</strong></em>), es decir, <span class="math notranslate nohighlight">\((s\circ f)_{i} = s_{i}f_{i}\)</span>, y <span class="math notranslate nohighlight">\(\sigma\)</span> denota la <em><strong>función sigmoidea logística</strong></em>.</p>
</li>
</ul>
<ol class="arabic">
<li><p><em><strong><code class="docutils literal notranslate"><span class="pre">Puerta</span> <span class="pre">de</span> <span class="pre">olvido</span></code></strong></em> (<span class="math notranslate nohighlight">\(\boldsymbol{f}\)</span>)</p>
<p>Esta puerta decide <em><strong>qué información se debe olvidar de la celda</strong></em>. Se calcula de la siguiente manera:</p>
<div class="math notranslate nohighlight">
\[
   \boldsymbol{f} = \sigma(U^{f} \boldsymbol{x}_{n} + W^{f} \boldsymbol{h}_{n-1} + \boldsymbol{b}^{f})
   \]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( U^{f} \)</span>: pesos de la puerta de olvido aplicados a la entrada actual <span class="math notranslate nohighlight">\( \boldsymbol{x}_{n} \)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\( W^{f} \)</span>: pesos de la puerta de olvido aplicados al estado oculto anterior <span class="math notranslate nohighlight">\( \boldsymbol{h}_{n-1} \)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\( \boldsymbol{b}^{f} \)</span>: sesgo de la puerta de olvido.</p></li>
<li><p><span class="math notranslate nohighlight">\( \sigma \)</span>: función sigmoide.</p></li>
</ul>
</li>
<li><p><em><strong><code class="docutils literal notranslate"><span class="pre">Puerta</span> <span class="pre">de</span> <span class="pre">entrada</span></code></strong></em> (<span class="math notranslate nohighlight">\(\boldsymbol{i}\)</span>)</p>
<p>Esta puerta decide <em><strong>qué nueva información se almacenará en la celda</strong></em>:</p>
<div class="math notranslate nohighlight">
\[
   \boldsymbol{i} = \sigma(U^{i} \boldsymbol{x}_{n} + W^{i} \boldsymbol{h}_{n-1} + \boldsymbol{b}^{i})
   \]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( U^{i} \)</span>: pesos de la puerta de entrada aplicados a la entrada actual <span class="math notranslate nohighlight">\( \boldsymbol{x}_{n} \)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\( W^{i} \)</span>: pesos de la puerta de entrada aplicados al estado oculto anterior <span class="math notranslate nohighlight">\( \boldsymbol{h}_{n-1} \)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\( \boldsymbol{b}^{i} \)</span>: sesgo de la puerta de entrada.</p></li>
</ul>
</li>
<li><p><em><strong><code class="docutils literal notranslate"><span class="pre">Candidato</span> <span class="pre">de</span> <span class="pre">estado</span></code></strong></em> (<span class="math notranslate nohighlight">\( \tilde{\boldsymbol{s}} \)</span>)</p>
<p>Genera <em><strong>nuevas candidaturas para el estado de la celda</strong></em>:</p>
<div class="math notranslate nohighlight">
\[
   \tilde{\boldsymbol{s}} = \tanh(U^{s} \boldsymbol{x}_{n} + W^{s} \boldsymbol{h}_{n-1} + \boldsymbol{b}^{s})
   \]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( U^{s} \)</span>: pesos del candidato de estado aplicados a la entrada actual <span class="math notranslate nohighlight">\( \boldsymbol{x}_{n} \)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\( W^{s} \)</span>: pesos del candidato de estado aplicados al estado oculto anterior <span class="math notranslate nohighlight">\( \boldsymbol{h}_{n-1} \)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\( \boldsymbol{b}^{s} \)</span>: sesgo del candidato de estado.</p></li>
<li><p><span class="math notranslate nohighlight">\( \tanh \)</span>: función tangente hiperbólica.</p></li>
</ul>
</li>
<li><p><em><strong><code class="docutils literal notranslate"><span class="pre">Estado</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">celda</span></code></strong></em> (<span class="math notranslate nohighlight">\( \boldsymbol{s}_{n} \)</span>)</p>
<p><em><strong>Actualiza el estado de la celda</strong></em> utilizando la puerta de olvido y la puerta de entrada:</p>
<div class="math notranslate nohighlight">
\[
   \boldsymbol{s}_{n} = \boldsymbol{s}_{n-1} \circ \boldsymbol{f} + \boldsymbol{i} \circ \tilde{\boldsymbol{s}}
   \]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( \circ \)</span>: multiplicación elemento a elemento.</p></li>
<li><p><span class="math notranslate nohighlight">\( \boldsymbol{s}_{n-1} \)</span>: estado de la celda en el paso temporal anterior.</p></li>
</ul>
</li>
<li><p><em><strong><code class="docutils literal notranslate"><span class="pre">Puerta</span> <span class="pre">de</span> <span class="pre">salida</span></code></strong></em> (<span class="math notranslate nohighlight">\( \boldsymbol{o} \)</span>)</p>
<p>Decide <em><strong>qué parte del estado de la celda se enviará a la salida</strong></em>:</p>
<div class="math notranslate nohighlight">
\[
   \boldsymbol{o} = \sigma(U^{o} \boldsymbol{x}_{n} + W^{o} \boldsymbol{h}_{n-1} + \boldsymbol{b}^{o})
   \]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( U^{o} \)</span>: pesos de la puerta de salida aplicados a la entrada actual <span class="math notranslate nohighlight">\( \boldsymbol{x}_{n} \)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\( W^{o} \)</span>: pesos de la puerta de salida aplicados al estado oculto anterior <span class="math notranslate nohighlight">\( \boldsymbol{h}_{n-1} \)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\( \boldsymbol{b}^{o} \)</span>: sesgo de la puerta de salida.</p></li>
</ul>
</li>
<li><p><em><strong><code class="docutils literal notranslate"><span class="pre">Estado</span> <span class="pre">oculto</span></code></strong></em> (<span class="math notranslate nohighlight">\(\boldsymbol{h}_{n}\)</span>)</p>
<p>Finalmente, se <em><strong>calcula el estado oculto, que es la salida de la celda</strong></em> LSTM:</p>
<div class="math notranslate nohighlight">
\[
    \boldsymbol{h}_{n} = \boldsymbol{o} \circ \tanh(\boldsymbol{s}_{n})
    \]</div>
</li>
<li><p><em><strong><code class="docutils literal notranslate"><span class="pre">Resumen</span> <span class="pre">de</span> <span class="pre">salida</span> <span class="pre">de</span> <span class="pre">una</span> <span class="pre">red</span></code></strong></em> <code class="docutils literal notranslate"><span class="pre">LSTM</span></code></p>
<ul class="simple">
<li><p><strong>Estado de la Celda (<span class="math notranslate nohighlight">\( \boldsymbol{s}_{n} \)</span>)</strong>: almacena información a largo plazo, afectada por las puertas de olvido y entrada.</p></li>
<li><p><strong>Estado Oculto (<span class="math notranslate nohighlight">\( \boldsymbol{h}_{n} \)</span>)</strong>: representa la salida de la celda LSTM en el tiempo <span class="math notranslate nohighlight">\( n \)</span>, que se utiliza como entrada para el siguiente paso temporal y puede usarse como salida de la red.</p></li>
</ul>
</li>
</ol>
<div class="proof observation admonition" id="observation_ann11">
<p class="admonition-title"><span class="caption-number">Observation 4.3 </span></p>
<section class="observation-content" id="proof-content">
<ul class="simple">
<li><p>Nótese que el <em><strong>estado de la célula</strong></em>, <span class="math notranslate nohighlight">\(\boldsymbol{s}\)</span>, <em><strong>pasa información directa del instante anterior al siguiente</strong></em>. Esta información es <em><strong>controlada primero por la primera puerta</strong></em>, según los elementos en <span class="math notranslate nohighlight">\(f\)</span>, que toman valores en el rango <span class="math notranslate nohighlight">\([0, 1]\)</span>, <em><strong>dependiendo de la entrada actual y de las variables ocultas que se reciben de la etapa anterior</strong></em>. Esto es lo que decíamos antes, es decir, que <em><strong>la ponderación se ajusta en “contexto”</strong></em>. A continuación, <em><strong>se añade nueva información</strong></em>, es decir, <span class="math notranslate nohighlight">\(\tilde{\boldsymbol{s}}\)</span>, a <span class="math notranslate nohighlight">\(\boldsymbol{s}_{n-1}\)</span>, que también está <em><strong>controlada por la segunda red de puertas sigmoidales</strong></em> (es decir, <span class="math notranslate nohighlight">\(\boldsymbol{i}\)</span>). Así se garantiza que la <em><strong>información del pasado se transmita al futuro de forma directa, lo cual, ayuda a la red a memorizar información.</strong></em>.</p></li>
<li><p>Resulta que este tipo de memoria <em><strong>explota mejor las dependencias de largo alcance en los datos</strong></em>, en comparación con la estructura <em><strong>RNN básica</strong></em>. El vector de variables ocultas <span class="math notranslate nohighlight">\(\boldsymbol{h}\)</span> <em><strong>está controlado tanto por el estado de la célula como por los valores actuales de las variables de entrada y de estados anteriores</strong></em>. Todas las <em><strong>matrices y vectores implicados se aprenden en la fase de entrenamiento</strong></em>. Nótese que hay dos líneas asociadas a <span class="math notranslate nohighlight">\(\boldsymbol{h}_{n}\)</span>. La de <em><strong>la derecha conduce a la siguiente etapa</strong></em> y la de <em><strong>la parte superior se utiliza para proporcionar la salida</strong></em>, <span class="math notranslate nohighlight">\(\hat{\boldsymbol{y}}_{n}\)</span>, en el tiempo <span class="math notranslate nohighlight">\(n\)</span>, a <em><strong>través de, digamos, la no linealidad softmax</strong></em>, como en las RNN estándar en Eq. <a class="reference internal" href="#equation-rnn-system-eq">(4.11)</a>.</p></li>
</ul>
</section>
</div><div class="admonition-variantes-y-aplicaciones admonition">
<p class="admonition-title">Variantes y Aplicaciones</p>
<ul class="simple">
<li><p>Además de la estructura LSTM ya comentada, <em><strong>se han propuesto diversas variantes</strong></em>. Un extenso <em><strong>estudio comparativo entre diferentes arquitecturas LSTM y RNN</strong></em> se puede encontrar en <span id="id13">[<a class="reference internal" href="biblio.html#id44" title="Klaus Greff, Rupesh K Srivastava, Jan Koutník, Bas R Steunebrink, and Jürgen Schmidhuber. Lstm: a search space odyssey. IEEE transactions on neural networks and learning systems, 28(10):2222–2232, 2016.">Greff <em>et al.</em>, 2016</a>, <a class="reference internal" href="biblio.html#id45" title="Rafal Jozefowicz, Wojciech Zaremba, and Ilya Sutskever. An empirical exploration of recurrent network architectures. In International conference on machine learning, 2342–2350. PMLR, 2015.">Jozefowicz <em>et al.</em>, 2015</a>]</span>. Las <em><strong>RNNs y las LSTMs</strong></em> se han utilizado con éxito en una <em><strong>amplia gama de aplicaciones</strong></em>, tales como:</p>
<ul>
<li><p>el <code class="docutils literal notranslate"><span class="pre">modelado</span> <span class="pre">del</span> <span class="pre">lenguaje</span></code> <span id="id14">[<a class="reference internal" href="biblio.html#id38" title="Ilya Sutskever, James Martens, and Geoffrey E Hinton. Generating text with recurrent neural networks. In Proceedings of the 28th international conference on machine learning (ICML-11), 1017–1024. 2011.">Sutskever <em>et al.</em>, 2011</a>]</span>,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">traducción</span> <span class="pre">de</span> <span class="pre">máquinas</span></code> <span id="id15">[<a class="reference internal" href="biblio.html#id39" title="Shujie Liu, Nan Yang, Mu Li, and Ming Zhou. A recursive recurrent neural network for statistical machine translation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 1491–1500. 2014.">Liu <em>et al.</em>, 2014</a>]</span>,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">reconocimiento</span> <span class="pre">del</span> <span class="pre">habla</span></code> <span id="id16">[<a class="reference internal" href="biblio.html#id40" title="Alex Graves and Navdeep Jaitly. Towards end-to-end speech recognition with recurrent neural networks. In International conference on machine learning, 1764–1772. PMLR, 2014.">Graves and Jaitly, 2014</a>]</span>,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">visión</span> <span class="pre">artificial</span></code> para la generación de descriptores de imágenes <span id="id17">[<a class="reference internal" href="biblio.html#id46" title="Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, 3128–3137. 2015.">Karpathy and Fei-Fei, 2015</a>]</span>,</p></li>
<li><p>análisis de datos de fMRI para comprender la <code class="docutils literal notranslate"><span class="pre">dinámica</span> <span class="pre">temporal</span> <span class="pre">de</span> <span class="pre">las</span> <span class="pre">redes</span> <span class="pre">cerebrales</span></code> asociadas <span id="id18">[<a class="reference internal" href="biblio.html#id41" title="Youngjoo Seo, Manuel Morante, Yannis Kopsinis, and Sergios Theodoridis. Unsupervised pre-training of the brain connectivity dynamic using residual d-net. In Neural Information Processing: 26th International Conference, ICONIP 2019, Sydney, NSW, Australia, December 12–15, 2019, Proceedings, Part III 26, 608–620. Springer, 2019.">Seo <em>et al.</em>, 2019</a>]</span>.</p></li>
<li><p>pronostico de <code class="docutils literal notranslate"><span class="pre">volatilidad</span> <span class="pre">de</span> <span class="pre">Bitcoin</span></code> <span id="id19">[<a class="reference internal" href="biblio.html#id42" title="Tiago E Pratas, Filipe R Ramos, and Lihki Rubio. Forecasting bitcoin volatility: exploring the potential of deep learning. Eurasian Economic Review, pages 1–21, 2023.">Pratas <em>et al.</em>, 2023</a>]</span></p></li>
</ul>
</li>
<li><p>Por ejemplo, en el <em><strong>procesamiento del lenguaje</strong></em> la <em><strong>entrada suele ser una secuencia de palabras, que se codifican como números</strong></em> (son punteros al diccionario disponible). La <em><strong>salida es la secuencia de palabras que hay que predecir</strong></em>. Durante el entrenamiento, se establece <span class="math notranslate nohighlight">\(\boldsymbol{y}_{n} = \boldsymbol{x}_{n+1}\)</span>. Es decir, <em><strong>la red se entrena como un predictor no lineal</strong></em>.</p></li>
</ul>
</div>
</section>
<section id="series-de-tiempo">
<h2><span class="section-number">4.12. </span>Series de Tiempo<a class="headerlink" href="#series-de-tiempo" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Hasta ahora, en este curso hemos descrito <em><strong>métodos estadísticos</strong></em> tradicionales para el análisis de series temporales. En los capítulos anteriores, hemos analizado varios métodos para predecir la serie en un momento futuro a partir de observaciones tomadas en el pasado. Uno de estos métodos para realizar predicciones es el <em><strong>modelo autorregresivo</strong></em> (<span class="math notranslate nohighlight">\(AR\)</span>), que expresa la serie en el tiempo <span class="math notranslate nohighlight">\(t\)</span> como una <em><strong>regresión lineal</strong></em> de <span class="math notranslate nohighlight">\(p\)</span> observaciones anteriores</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
y_{t}=\omega_{0}+\sum_{i=1}^{p}\omega_{i}y_{t-i}+\varepsilon_{t}.
\]</div>
<ul class="simple">
<li><p>Aquí, <span class="math notranslate nohighlight">\(\varepsilon_{t}\)</span> es el término de <em><strong>error residual</strong></em> del modelo <span class="math notranslate nohighlight">\(AR\)</span>. La idea que subyace al modelo lineal puede generalizarse en el sentido de que el objetivo de la predicción de series temporales es <em><strong>desarrollar una función</strong></em> <span class="math notranslate nohighlight">\(f\)</span> <em><strong>que prediga</strong></em> <span class="math notranslate nohighlight">\(y_{t}\)</span> <em><strong>en función de las observaciones en</strong></em> <span class="math notranslate nohighlight">\(p\)</span> <em><strong>valores previos en el tiempo</strong></em></p></li>
</ul>
<div class="math notranslate nohighlight">
\[
y_{t}=f(y_{t-1}, y_{t-2}, \dots, y_{t-p}).
\]</div>
<ul class="simple">
<li><p>En este capítulo, exploraremos tres <em><strong>métodos basados en redes neuronales para desarrollar la función</strong></em> <span class="math notranslate nohighlight">\(f\)</span>. Cada método incluye la definición de una <em><strong>arquitectura de red neuronal</strong></em> (en términos del <em><strong>número de capas ocultas, número de neuronas en cada capa oculta</strong></em>, etc.) y luego el algoritmo <em><strong>backpropagation</strong></em> o su variante adecuada para la arquitectura de red utilizada.</p></li>
</ul>
<ul class="simple">
<li><p>Los ejemplos de este capítulo serán implementados utilizando la <code class="docutils literal notranslate"><span class="pre">API</span> <span class="pre">Keras</span></code> para el aprendizaje profundo. <code class="docutils literal notranslate"><span class="pre">Keras</span></code> es una <code class="docutils literal notranslate"><span class="pre">API</span></code> de alto nivel que permite <em><strong>definir diferentes arquitecturas de redes neuronales y entrenarlas utilizando varios optimizadores basados en gradientes</strong></em>. En el <em><strong>backend, Keras</strong></em> utiliza un marco computacional de bajo nivel implementado en <code class="docutils literal notranslate"><span class="pre">C,</span> <span class="pre">C++</span></code> y <code class="docutils literal notranslate"><span class="pre">FORTRAN</span></code>. Varios de estos entornos de bajo nivel están disponibles en código abierto. <code class="docutils literal notranslate"><span class="pre">Keras</span></code> soporta los tres siguientes entornos: <em><strong>TensorFlow, que fue desarrollado por Google</strong></em> y es el <em><strong>backend</strong></em> por defecto de <em><strong>Keras, CNTK, un marco de código abierto de Microsoft</strong></em>, y <em><strong>Theano, desarrollado originalmente en la Universidad de Montreal, Canadá</strong></em>. Los ejemplos de este libro utilizan <code class="docutils literal notranslate"><span class="pre">TensorFlow</span></code> como backend. Por lo tanto, para ejecutar los ejemplos, necesitarás tanto <code class="docutils literal notranslate"><span class="pre">Keras</span></code> como <code class="docutils literal notranslate"><span class="pre">TensorFlow</span></code> instalados.</p></li>
</ul>
</section>
<section id="perceptrones-multicapa">
<h2><span class="section-number">4.13. </span>Perceptrones multicapa<a class="headerlink" href="#perceptrones-multicapa" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Los <em><strong>perceptrones multicapa (MLP)</strong></em> son las formas más básicas de las redes neuronales. Un <code class="docutils literal notranslate"><span class="pre">MLP</span></code> consta de tres componentes: una <em><strong>capa de entrada</strong></em>, varias <em><strong>capas ocultas</strong></em> y una <em><strong>capa de salida</strong></em>. Una <em><strong>capa de entrada representa un vector de regresores o características de entrada</strong></em>, por ejemplo, observaciones a partir de <span class="math notranslate nohighlight">\(p\)</span> puntos previos en el tiempo <span class="math notranslate nohighlight">\([y_{t-1}, y_{t-2}, \dots, y_{t-p}]\)</span>. Las <em><strong>características de entrada se introducen en una capa oculta</strong></em> que tiene <span class="math notranslate nohighlight">\(n\)</span> neuronas, cada una de las cuales aplica una <em><strong>transformación lineal</strong></em> y una <em><strong>activación no lineal</strong></em> a las características de entrada.</p></li>
</ul>
<ul>
<li><p>La salida de una neurona es <span class="math notranslate nohighlight">\(g_{i} = h(\boldsymbol{w}_{i}x + b_{i})\)</span>, donde <span class="math notranslate nohighlight">\(\boldsymbol{w}_{i}\)</span> y <span class="math notranslate nohighlight">\(b_{i}\)</span> son los <em><strong>pesos y el sesgo de la transformación lineal</strong></em>, y <span class="math notranslate nohighlight">\(h\)</span> es una <em><strong>función de activación no lineal</strong></em>. La función de activación no lineal permite a la red neuronal <em><strong>modelar no linealidades complejas de las relaciones subyacentes entre los regresores y la variable objetivo</strong></em>. Popularmente, <span class="math notranslate nohighlight">\(h\)</span> es la función <em><strong>sigmoid</strong></em></p>
<div class="math notranslate nohighlight">
\[
    h(z)=\displaystyle{\frac{1}{1-e^{-z}}},
    \]</div>
<p>que mapea cualquier número real al intervalo <span class="math notranslate nohighlight">\([0, 1]\)</span>.</p>
</li>
</ul>
<ul>
<li><p>Debido a esta propiedad, <em><strong>la función sigmoidea se utiliza para generar probabilidades de clase binarias y, por tanto, es comunmente usada en los modelos de clasificación</strong></em>. Otra opción de función de activación no lineal es la función <code class="docutils literal notranslate"><span class="pre">tanh</span></code></p>
<div class="math notranslate nohighlight">
\[
    h(z)=\displaystyle{\frac{1-e^{-z}}{1+e^{-z}}},
    \]</div>
<p>la cual mapea cualquier número real al intervalo <span class="math notranslate nohighlight">\([-1, 1]\)</span>. En algunos casos la función <span class="math notranslate nohighlight">\(h\)</span> es una <em><strong>función lineal o la identidad</strong></em>.</p>
</li>
</ul>
<ul class="simple">
<li><p>En el caso de una <em><strong>red neuronal de una sola capa oculta</strong></em>, como se muestra en <a class="reference internal" href="#single-hidden-layer"><span class="std std-numref">Fig. 4.16</span></a>, la salida de cada neurona se pasa a la capa de salida, que aplica una <em><strong>transformación lineal y función de activación para generar la predicción de la variable objetivo</strong></em>, que, en el caso de la predicción de series temporales, es el valor predicho de la serie en el tiempo <span class="math notranslate nohighlight">\(t\)</span></p></li>
</ul>
<figure class="align-center" id="single-hidden-layer">
<a class="reference internal image-reference" href="_images/single_hidden_layer.png"><img alt="_images/single_hidden_layer.png" src="_images/single_hidden_layer.png" style="width: 360.0px; height: 320.4px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.16 </span><span class="caption-text">Red neuronal con una sola capa oculta.</span><a class="headerlink" href="#single-hidden-layer" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>En un <code class="docutils literal notranslate"><span class="pre">MLP</span></code> (ver <a class="reference internal" href="#multiple-hidden-layer"><span class="std std-numref">Fig. 4.17</span></a>), se agrupan varias capas ocultas. <em><strong>La salida de las neuronas de una capa oculta se introduce en la siguiente capa oculta</strong></em>. Las neuronas de esta capa transforman la entrada y la pasan a la siguiente capa oculta. Finalmente, <em><strong>la última capa oculta alimenta la capa de salida</strong></em></p></li>
</ul>
<figure class="align-center" id="multiple-hidden-layer">
<a class="reference internal image-reference" href="_images/multiple_hidden_layer.png"><img alt="_images/multiple_hidden_layer.png" src="_images/multiple_hidden_layer.png" style="width: 570.0px; height: 335.16px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.17 </span><span class="caption-text">Red neuronal con múltiples capas ocultas.</span><a class="headerlink" href="#multiple-hidden-layer" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>Las capas ocultas del <code class="docutils literal notranslate"><span class="pre">MLP</span></code> también se denominan <em><strong>capas densas o, a veces, capas totalmente conectadas</strong></em>. El nombre denso tiene su significado en el hecho de que todas <em><strong>las neuronas de una capa densa están conectadas a todas las neuronas de la capa anterior y de la siguiente</strong></em>. Si la capa anterior es la capa de entrada, todas las características de entrada alimentan a cada una de las neuronas de la capa oculta.</p></li>
<li><p><em><strong>Debido a las conexiones múltiples entre la capa de entrada y la primera capa densa y entre las capas densas intermedias, un MLP tiene un número enorme de pesos entrenables</strong></em>. Por ejemplo, si el número de características de entrada es <span class="math notranslate nohighlight">\(p\)</span> y hay tres capas densas que tienen número de neuronas <span class="math notranslate nohighlight">\(n_{1}, n_{2}\)</span> y <span class="math notranslate nohighlight">\(n_{3}\)</span> respectivamente, entonces el número de pesos entrenables es</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
p\times n_{1}+n_{1}\times n_{2}+n_{2}\times n_{3}+n_{3}.
\]</div>
<ul class="simple">
<li><p>El último elemento de este cálculo es el número de pesos que conectan la tercera capa oculta y la capa de salida. Los <em><strong>MLP profundos tienen varias capas densas y cientos, incluso miles, de neuronas en cada capa</strong></em>. Por lo tanto, el número de pesos entrenables en los MLP profundos es muy grande.</p></li>
</ul>
</section>
<section id="entrenamiento-de-mlp">
<h2><span class="section-number">4.14. </span>Entrenamiento de MLP<a class="headerlink" href="#entrenamiento-de-mlp" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Los pesos <span class="math notranslate nohighlight">\(w\)</span> de una red neuronal se calculan mediante un algoritmo de optimización basado en el gradiente, como el <em><strong>gradiente decendiente estocástico, que minimiza iterativamente la función de pérdida o el error</strong></em> (<span class="math notranslate nohighlight">\(L\)</span>) en que incurre la red al hacer predicciones sobre los datos de entrenamiento. El <em><strong>error cuadrático medio</strong></em> (<span class="math notranslate nohighlight">\(MSE\)</span>) y el <em><strong>error absoluto medio</strong></em> (<span class="math notranslate nohighlight">\(MAE\)</span>) se utilizan para <em><strong>tareas de regresión</strong></em>, mientras que la pérdida binaria y categórica logarítmica son funciones de pérdida habituales en los problemas de clasificación.</p></li>
</ul>
<ul class="simple">
<li><p>Para la predicción de series temporales, <span class="math notranslate nohighlight">\(MSE\)</span> y <span class="math notranslate nohighlight">\(MAE\)</span> serían aptos para entrenar los modelos neuronales. <em><strong>Los algoritmos gradiente descendente funcionan moviendo los pesos, en iteraciones</strong></em> <span class="math notranslate nohighlight">\(i\)</span><em><strong>, a lo largo de su trayectoria de gradiente</strong></em>. El gradiente es la derivada parcial de la función de pérdida <span class="math notranslate nohighlight">\(L\)</span> con respecto al peso. La regla de actualización más sencilla para cambiar un peso <span class="math notranslate nohighlight">\(w\)</span> requiere los valores de los pesos, la derivada parcial de <span class="math notranslate nohighlight">\(L\)</span> con respecto a los pesos, y una tasa de aprendizaje <span class="math notranslate nohighlight">\(\alpha\)</span> que <em><strong>controla la rapidez con la que el punto desciende a lo largo del gradiente</strong></em></p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\omega_{i+1}=\omega_{i}-\alpha\left(\frac{\partial L}{\partial\omega}\right)_{\omega = \omega_{i}}
\]</div>
<ul class="simple">
<li><p>Esta regla básica de actualización tiene diversas variantes que influyen en la convergencia del algoritmo. Sin embargo, <em><strong>una entrada crucial para todos los algoritmos basados en el gradiente es la derivada parcial que debe calcularse para todos los pesos de la red</strong></em>. En redes neuronales profundas, algunas de las cuales tienen millones de pesos, el cálculo de la derivada puede ser una tarea computacional gigantesca. En esta dirección es exactamente donde entra en juego el famoso <em><strong>algoritmo backpropagation</strong></em> que resuelve este problema de forma eficiente.</p></li>
<li><p>Para entender el algoritmo backpropagation, primero hay que conocer los <em><strong>grafos computacionales y cómo se utilizan para realizar cálculos en una red neuronal</strong></em>. Consideremos una <em><strong>red neuronal simple de una sola capa oculta, que tiene dos unidades ocultas, cada una con una activación sigmoidea</strong></em>. La unidad de salida es una transformación lineal de sus entradas. La red se alimenta con dos variables de entrada, <span class="math notranslate nohighlight">\([y_{1},y_{2}]\)</span>. Los pesos se muestran a lo largo de los bordes de la red</p></li>
</ul>
<figure class="align-center" id="twoinput-single-hidden-layer">
<a class="reference internal image-reference" href="_images/twoinput_single_hidden_layer.png"><img alt="_images/twoinput_single_hidden_layer.png" src="_images/twoinput_single_hidden_layer.png" style="width: 300.0px; height: 207.60000000000002px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.18 </span><span class="caption-text">Red neuronal con una sola capa oculta y dos entradas.</span><a class="headerlink" href="#twoinput-single-hidden-layer" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>La red realiza una serie de <em><strong>sumas, multiplicaciones y un par evaluaciones de funciones sigmoidales para transformar la entrada en una predicción</strong></em>. La transformación de la entrada en una predicción se denomina paso hacia delante de la red neuronal. La <a class="reference internal" href="#forwardprop-single-hidden-layer"><span class="std std-numref">Fig. 4.19</span></a> muestra cómo se consigue un pase hacia adelante mediante un grafo computacional para un par de entrada <span class="math notranslate nohighlight">\([-1, 2]\)</span>. Cada cálculo da como resultado una salida intermedia <span class="math notranslate nohighlight">\(p_{i}\)</span>. <em><strong>Los resultados intermedios</strong></em> <span class="math notranslate nohighlight">\(p_{7}\)</span> <em><strong>y</strong></em> <span class="math notranslate nohighlight">\(p_{8}\)</span> <em><strong>son la salida de las neuronas ocultas</strong></em> <span class="math notranslate nohighlight">\(g_{1}\)</span> y <span class="math notranslate nohighlight">\(g_{2}\)</span>. Durante el entrenamiento, la función de pérdida <span class="math notranslate nohighlight">\(L\)</span> también se calcula en el paso hacia delante</p></li>
</ul>
<figure class="align-center" id="forwardprop-single-hidden-layer">
<img alt="_images/forwardprop_single_hidden_layer.png" src="_images/forwardprop_single_hidden_layer.png" />
<figcaption>
<p><span class="caption-number">Fig. 4.19 </span><span class="caption-text">Gráfico computacional de un perceptrón con una capa y dos neuronas ocultas.</span><a class="headerlink" href="#forwardprop-single-hidden-layer" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>En este punto, <em><strong>se aplica el algoritmo de backpropagation para calcular las derivadas parciales entre dos nodos conectados por una arista</strong></em>. El recorrido hacia atrás en el grafo para calcular la derivada parcial también se conoce como paso hacia atrás (<em><strong>backward pass</strong></em>). <em><strong>El operador de diferenciación parcial se aplica en cada nodo y las derivadas parciales se asignan a las respectivas aristas que conectan el nodo descendente a lo largo del grafo computacional</strong></em>.</p></li>
<li><p>Siguiendo la regla de la cadena la derivada parcial <span class="math notranslate nohighlight">\(\partial_{\omega} L\)</span> se calcula <em><strong>multiplicando las derivadas parciales en todas las aristas que conectan el nodo de peso y el nodo de pérdida</strong></em>. Si existen varios caminos entre un nodo de peso y el nodo de pérdida, las derivadas parciales a lo largo de cada camino se suman para obtener la <em><strong>derivada parcial total de la pérdida con respecto al peso</strong></em>. Esta técnica gráfica de implementar los pasos hacia delante y hacia atrás es la técnica computacional subyacente utilizada en potentes bibliotecas de aprendizaje profundo. El paso hacia atrás se ilustra en <a class="reference internal" href="#partialderivates-single-hidden-layer"><span class="std std-numref">Fig. 4.20</span></a></p></li>
</ul>
<figure class="align-center" id="partialderivates-single-hidden-layer">
<img alt="_images/partialderivates_single_hidden_layer.png" src="_images/partialderivates_single_hidden_layer.png" />
<figcaption>
<p><span class="caption-number">Fig. 4.20 </span><span class="caption-text">Cálculo de derivadas parciales en un grafo computacional.</span><a class="headerlink" href="#partialderivates-single-hidden-layer" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>Las derivadas parciales de la función de pérdida con respecto a los pesos se obtienen aplicando la <em><strong>regla de la cadena</strong></em>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\frac{\partial L}{\partial\omega_{5}} &amp;= -2(y-\hat{y})\times 1\times p_{7}\\
\frac{\partial L}{\partial\omega_{6}} &amp;= -2(y-\hat{y})\times 1\times p_{8}\\
\frac{\partial L}{\partial\omega_{1}} = \frac{\partial L}{\partial\hat{y}}\frac{\partial\hat{y}}{\partial p_{9}}\frac{\partial p_{9}}{\partial p_{7}}\frac{\partial p_{7}}{\partial p_{5}}\frac{\partial p_{5}}{\partial p_{1}}\frac{\partial p_{1}}{\partial\omega_{1}} &amp;= -2(y-\hat{y})\times 1\times\omega_{5}\times p_{7}^{2}e^{-p_{5}}\times 1\times -1\\
\frac{\partial L}{\partial\omega_{2}} = \frac{\partial L}{\partial\hat{y}}\frac{\partial\hat{y}}{\partial p_{9}}\frac{\partial p_{9}}{\partial p_{7}}\frac{\partial p_{7}}{\partial p_{5}}\frac{\partial p_{5}}{\partial p_{2}}\frac{\partial p_{2}}{\partial\omega_{2}} &amp;= -2(y-\hat{y})\times 1\times\omega_{5}\times p_{7}^{2}e^{-p_{5}}\times 1\times 2\\
\frac{\partial L}{\partial\omega_{3}} = \frac{\partial L}{\partial\hat{y}}\frac{\partial\hat{y}}{\partial p_{10}}\frac{\partial p_{10}}{\partial p_{8}}\frac{\partial p_{8}}{\partial p_{6}}\frac{\partial p_{6}}{\partial p_{3}}\frac{\partial p_{3}}{\partial\omega_{3}} &amp;= -2(y-\hat{y})\times 1\times\omega_{6}\times p_{8}^{2}e^{-p_{6}}\times 1\times -1\\
\frac{\partial L}{\partial\omega_{4}} = \frac{\partial L}{\partial\hat{y}}\frac{\partial\hat{y}}{\partial p_{10}}\frac{\partial p_{10}}{\partial p_{8}}\frac{\partial p_{8}}{\partial p_{6}}\frac{\partial p_{6}}{\partial p_{4}}\frac{\partial p_{4}}{\partial\omega_{4}} &amp;= -2(y-\hat{y})\times 1\times\omega_{6}\times p_{8}^{2}e^{-p_{6}}\times 1\times 2.
\end{align*}
\end{split}\]</div>
<ul class="simple">
<li><p>Durante el entrenamiento, <em><strong>los pesos se inicializan con números aleatorios comúnmente muestreados a partir de una distribución uniforme con límites superior e inferior en</strong></em> <span class="math notranslate nohighlight">\([-1, 1]\)</span> o una <em><strong>distribución normal que tiene media cero y varianza unitaria</strong></em>. Estos esquemas de inicialización aleatoria tienen algunas variantes que mejoran la convergencia de la optimización. En este caso, vamos a suponer que los pesos son inicializados a partir de una distribución aleatoria uniforme y, por tanto, <span class="math notranslate nohighlight">\(w_{1} = -0.33, w_{2} = -0.33, w_{3} = 0.57, w_{4} = -0.01, w_{5}=0.07\)</span>, y <span class="math notranslate nohighlight">\(w_{6} = 0.82\)</span>.</p></li>
<li><p>Con estos valores, vamos a realizar pasos hacia adelante y hacia atrás sobre el grafo computacional. Actualizamos la figura anterior con los valores calculados durante la <em><strong>pasada hacia adelante en azul y los gradientes calculados durante la pasada hacia atrás en rojo</strong></em>. Para este ejemplo, fijamos el <em><strong>valor real de la variable objetivo como</strong></em> <span class="math notranslate nohighlight">\(y = 1\)</span></p></li>
</ul>
<figure class="align-center" id="forward-backward-single-hidden-layer">
<img alt="_images/forward_backward_single_hidden_layer.png" src="_images/forward_backward_single_hidden_layer.png" />
<figcaption>
<p><span class="caption-number">Fig. 4.21 </span><span class="caption-text">Pasos hacia delante (en azul) y hacia atrás (en rojo) sobre un grafo computacional.</span><a class="headerlink" href="#forward-backward-single-hidden-layer" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>Una vez calculados los gradientes a lo largo de las aristas, <em><strong>las derivadas parciales con respecto a los pesos no son más que una aplicación de la regla de la cadena</strong></em>, de la que ya hemos hablado. Los valores de las derivadas parciales son los siguientes:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\frac{\partial L}{\partial\omega_{5}} &amp;= -0.919\times 1\times 0.418 = -0.384\\
\frac{\partial L}{\partial\omega_{6}} &amp;= -0.919\times 1\times 0.357 = -0.328\\
\frac{\partial L}{\partial\omega_{1}} &amp;= -0.919\times 1\times 0.07\times 0.243\times 1\times -1 = 0.016\\
\frac{\partial L}{\partial\omega_{2}} &amp;= -0.919\times 1\times 0.07\times 0.243\times 1\times 2 = -0.032\\
\frac{\partial L}{\partial\omega_{3}} &amp;= -0.919\times 1\times 0.82\times 0.229\times 1\times -1 = 0.173\\
\frac{\partial L}{\partial\omega_{4}} &amp;= -0.919\times 1\times 0.82\times 0.229\times 1\times 2 = -0.346\\
\end{align*}
\end{split}\]</div>
<ul class="simple">
<li><p>El siguiente paso consiste en <em><strong>actualizar los pesos mediante el algoritmo de gradiente descendente</strong></em>. Así, con una <em><strong>tasa de aprendizaje de</strong></em> <span class="math notranslate nohighlight">\(\alpha = 0.01\)</span>, el nuevo valor de</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
w_{5} = 0.07 - 0.01\times -0.384 = 0.0738.
\]</div>
<ul class="simple">
<li><p>El resto de pesos también pueden actualizarse utilizando una regla de actualización similar. El proceso de actualización iterativa de los pesos se repite varias veces. <em><strong>El número de veces que se actualizan los pesos se conoce como número de épocas</strong></em> o pasadas sobre los datos de entrenamiento. Normalmente, un criterio de tolerancia sobre el cambio de la función de pérdida en comparación con la época anterior controla el número de épocas.</p></li>
<li><p><em><strong>Para determinar los pesos de una red neuronal se utiliza el algoritmo backpropagation junto con un optimizador basado en el gradiente</strong></em>. Afortunadamente, existen potentes bibliotecas de aprendizaje profundo, como <code class="docutils literal notranslate"><span class="pre">Tensorflow,</span> <span class="pre">Theano</span></code> y <code class="docutils literal notranslate"><span class="pre">CNTK</span></code> que implementan gráficos computacionales para entrenar redes neuronales de cualquier arquitectura y complejidad. Estas bibliotecas vienen con soporte para ejecutar los cálculos como operaciones matemáticas en matrices multidimensionales y también pueden aprovechar las <code class="docutils literal notranslate"><span class="pre">GPU</span></code> para realizar cálculos más rápidos.</p></li>
</ul>
</section>
<section id="mlp-para-la-prediccion-de-series-temporales">
<h2><span class="section-number">4.15. </span>MLP para la predicción de series temporales<a class="headerlink" href="#mlp-para-la-prediccion-de-series-temporales" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>En esta sección, utilizaremos <code class="docutils literal notranslate"><span class="pre">MLP</span></code> para desarrollar <em><strong>modelos de predicción de series temporales</strong></em>. El conjunto de datos utilizado para estos ejemplos es sobre la <em><strong>contaminación atmosférica medida por la concentración de partículas (PM) de diámetro inferior o igual a 2.5 micrómetros</strong></em>.</p></li>
<li><p>Hay otras variables, como la <em><strong>presión atmosférica, temperatura del aire, punto de rocío, etc</strong></em>. Se han desarrollado un par de modelos de series temporales, uno sobre la <em><strong>presión atmosférica</strong></em> <code class="docutils literal notranslate"><span class="pre">PRES</span></code> y otro sobre <code class="docutils literal notranslate"><span class="pre">pm</span> <span class="pre">2.5</span></code>. El conjunto de datos se ha descargado del repositorio de aprendizaje automático de la <a class="reference external" href="https://archive.ics.uci.edu/ml/datasets/Beijing+PM2.5+Data">UCI</a>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">datetime</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MinMaxScaler</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Input</span><span class="p">,</span> <span class="n">Dropout</span>
<span class="kn">from</span> <span class="nn">keras.optimizers</span> <span class="kn">import</span> <span class="n">SGD</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">load_model</span>
<span class="kn">from</span> <span class="nn">keras.callbacks</span> <span class="kn">import</span> <span class="n">ModelCheckpoint</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">r2_score</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_absolute_error</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">devices</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">list_physical_devices</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Available devices:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">device</span> <span class="ow">in</span> <span class="n">devices</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">gpus</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">list_physical_devices</span><span class="p">(</span><span class="s1">&#39;GPU&#39;</span><span class="p">)</span>
<span class="k">if</span> <span class="n">gpus</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;TensorFlow is using GPU.&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">gpu</span> <span class="ow">in</span> <span class="n">gpus</span><span class="p">:</span>
        <span class="n">gpu_details</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">get_device_details</span><span class="p">(</span><span class="n">gpu</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GPU details: </span><span class="si">{</span><span class="n">gpu_details</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;TensorFlow is not using GPU.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Available devices:
PhysicalDevice(name=&#39;/physical_device:CPU:0&#39;, device_type=&#39;CPU&#39;)
PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;)
TensorFlow is using GPU.
GPU details: {&#39;compute_capability&#39;: (8, 9), &#39;device_name&#39;: &#39;NVIDIA GeForce RTX 4070&#39;}
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>I0000 00:00:1728055391.390007   31222 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;datasets/PRSA_data_2010.1.1-2014.12.31.csv&#39;</span><span class="p">,</span> <span class="n">usecols</span><span class="o">=</span><span class="k">lambda</span> <span class="n">column</span><span class="p">:</span> <span class="n">column</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;DEWP&#39;</span><span class="p">,</span> <span class="s1">&#39;TEMP&#39;</span><span class="p">,</span> <span class="s1">&#39;cbwd&#39;</span><span class="p">,</span> <span class="s1">&#39;Iws&#39;</span><span class="p">,</span>	<span class="s1">&#39;Is&#39;</span><span class="p">,</span> <span class="s1">&#39;Ir&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Shape of the dataframe:&#39;</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shape of the dataframe: (43824, 7)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>No</th>
      <th>year</th>
      <th>month</th>
      <th>day</th>
      <th>hour</th>
      <th>pm2.5</th>
      <th>PRES</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>2010</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>NaN</td>
      <td>1021.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>2010</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>NaN</td>
      <td>1020.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>2010</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>NaN</td>
      <td>1019.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>2010</td>
      <td>1</td>
      <td>1</td>
      <td>3</td>
      <td>NaN</td>
      <td>1019.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>2010</td>
      <td>1</td>
      <td>1</td>
      <td>4</td>
      <td>NaN</td>
      <td>1018.0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<ul class="simple">
<li><p>Para asegurarse de que las filas están en el orden correcto de fecha y hora de las observaciones, <em><strong>se crea una nueva columna datetime a partir de las columnas relacionadas con la fecha y la hora del DataFrame</strong></em>. La nueva columna se compone de objetos <code class="docutils literal notranslate"><span class="pre">datetime.datetime</span> <span class="pre">de</span> <span class="pre">Python</span></code>. El DataFrame se ordena en orden ascendente sobre esta columna</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;datetime&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;year&#39;</span><span class="p">,</span> <span class="s1">&#39;month&#39;</span><span class="p">,</span> <span class="s1">&#39;day&#39;</span><span class="p">,</span> <span class="s1">&#39;hour&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">row</span><span class="p">:</span> <span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="p">(</span><span class="n">year</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">],</span> <span class="n">month</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;month&#39;</span><span class="p">],</span> <span class="n">day</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;day&#39;</span><span class="p">],</span>
                                                                                          <span class="n">hour</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;hour&#39;</span><span class="p">]),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;datetime&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>No</th>
      <th>year</th>
      <th>month</th>
      <th>day</th>
      <th>hour</th>
      <th>pm2.5</th>
      <th>PRES</th>
      <th>datetime</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>2010</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>NaN</td>
      <td>1021.0</td>
      <td>2010-01-01 00:00:00</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>2010</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>NaN</td>
      <td>1020.0</td>
      <td>2010-01-01 01:00:00</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>2010</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>NaN</td>
      <td>1019.0</td>
      <td>2010-01-01 02:00:00</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>2010</td>
      <td>1</td>
      <td>1</td>
      <td>3</td>
      <td>NaN</td>
      <td>1019.0</td>
      <td>2010-01-01 03:00:00</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>2010</td>
      <td>1</td>
      <td>1</td>
      <td>4</td>
      <td>NaN</td>
      <td>1018.0</td>
      <td>2010-01-01 04:00:00</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<ul class="simple">
<li><p>Dibujamos un diagrama de cajas para visualizar la <em><strong>tendencia central y la dispersión</strong></em> de por ejemplo la columna <code class="docutils literal notranslate"><span class="pre">PRES</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_fontsize</span> <span class="o">=</span> <span class="mi">18</span><span class="p">;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  
<span class="n">g1</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;PRES&#39;</span><span class="p">],</span> <span class="n">orient</span><span class="o">=</span><span class="s2">&quot;h&quot;</span><span class="p">)</span>  <span class="c1"># Removed palette</span>
<span class="n">g1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Box plot of Air Pressure&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>
<span class="n">g1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Air Pressure readings in hPa&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> 
<span class="n">g2</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;pm2.5&#39;</span><span class="p">],</span> <span class="n">orient</span><span class="o">=</span><span class="s2">&quot;h&quot;</span><span class="p">)</span>  <span class="c1"># Removed palette</span>
<span class="n">g2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Box plot of PM2.5&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>
<span class="n">g2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;PM2.5 readings in µg/m³&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/a26fe4d39f2bcf1fb450d60a84130ca8dd8ffe3cc9043222f85a2321ce86c089.png" src="_images/a26fe4d39f2bcf1fb450d60a84130ca8dd8ffe3cc9043222f85a2321ce86c089.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> 
<span class="n">g1</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;PRES&#39;</span><span class="p">])</span>  <span class="c1"># Using x and y parameters instead of data</span>
<span class="n">g1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Time series of Air Pressure&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>
<span class="n">g1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Index&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>
<span class="n">g1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Air Pressure readings in hPa&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  
<span class="n">g2</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;pm2.5&#39;</span><span class="p">])</span>  <span class="c1"># Using x and y parameters instead of data</span>
<span class="n">g2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Time series of PM2.5&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>
<span class="n">g2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Index&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>
<span class="n">g2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;PM2.5 readings in µg/m³&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/4fa480191fc9785a41e9b88fc7128db01c59d1cdbb51cadf20189438b19c3c65.png" src="_images/4fa480191fc9785a41e9b88fc7128db01c59d1cdbb51cadf20189438b19c3c65.png" />
</div>
</div>
<ul class="simple">
<li><p><em><strong>Los algoritmos de gradiente descendente funcionan mejor (por ejemplo, convergen más rápido) si las variables están dentro del intervalo</strong></em> <span class="math notranslate nohighlight">\([-1, 1]\)</span>. Muchas fuentes relajan el límite hasta <span class="math notranslate nohighlight">\([-3, 3]\)</span>. La variable <code class="docutils literal notranslate"><span class="pre">PRES</span></code> es escalada con <code class="docutils literal notranslate"><span class="pre">minmax</span></code> para limitar la variable transformada dentro de <span class="math notranslate nohighlight">\([0,1]\)</span>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">(</span><span class="n">feature_range</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;scaled_PRES&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;PRES&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="tip admonition">
<p class="admonition-title">Observación</p>
<ul class="simple">
<li><p>Antes de entrenar el modelo, el conjunto de datos se divide en dos partes: el conjunto de <em><strong>entrenamiento</strong></em> y el conjunto de <em><strong>validación</strong></em>. La red neuronal se entrena en el conjunto de entrenamiento. Esto significa que <em><strong>el cálculo de la función de pérdida, la propagación hacia atrás y los pesos actualizados mediante un algoritmo de gradiente descendente se realizan en el conjunto de entrenamiento</strong></em>.</p></li>
<li><p><em><strong>El conjunto de validación se utiliza para evaluar el modelo y determinar el número de épocas en su entrenamiento</strong></em>. Aumentar el número de épocas reducirá aún más la función de pérdida en el conjunto de entrenamiento, pero no necesariamente tendrá el mismo efecto en el conjunto de validación debido al sobreajuste en el conjunto de entrenamiento, por lo que <em><strong>el número de épocas se controla manteniendo una evaluación y verificación sobre la función de pérdida calculada para el conjunto de validación</strong></em>.</p></li>
</ul>
</div>
<ul class="simple">
<li><p>Utilizamos <code class="docutils literal notranslate"><span class="pre">Keras</span></code> con el backend <code class="docutils literal notranslate"><span class="pre">Tensorflow</span></code> para definir y entrenar el modelo. Todos los pasos implicados en el entrenamiento y validación del modelo se realizan llamando a las funciones apropiadas de la <code class="docutils literal notranslate"><span class="pre">API</span></code> de <code class="docutils literal notranslate"><span class="pre">Keras</span></code>.</p></li>
</ul>
<ul class="simple">
<li><p>Los cuatro primeros años, <em><strong>de 2010 a 2013, se utilizan como entrenamiento y
2014 se utiliza para la validación</strong></em></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">split_date</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="p">(</span><span class="n">year</span><span class="o">=</span><span class="mi">2014</span><span class="p">,</span> <span class="n">month</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">day</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hour</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">df_train</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;datetime&#39;</span><span class="p">]</span><span class="o">&lt;</span><span class="n">split_date</span><span class="p">]</span>
<span class="n">df_val</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;datetime&#39;</span><span class="p">]</span><span class="o">&gt;=</span><span class="n">split_date</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Shape of train:&#39;</span><span class="p">,</span> <span class="n">df_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Shape of test:&#39;</span><span class="p">,</span> <span class="n">df_val</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shape of train: (35064, 9)
Shape of test: (8760, 9)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_train</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>No</th>
      <th>year</th>
      <th>month</th>
      <th>day</th>
      <th>hour</th>
      <th>pm2.5</th>
      <th>PRES</th>
      <th>datetime</th>
      <th>scaled_PRES</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>2010</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>NaN</td>
      <td>1021.0</td>
      <td>2010-01-01 00:00:00</td>
      <td>0.545455</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>2010</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>NaN</td>
      <td>1020.0</td>
      <td>2010-01-01 01:00:00</td>
      <td>0.527273</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>2010</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>NaN</td>
      <td>1019.0</td>
      <td>2010-01-01 02:00:00</td>
      <td>0.509091</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>2010</td>
      <td>1</td>
      <td>1</td>
      <td>3</td>
      <td>NaN</td>
      <td>1019.0</td>
      <td>2010-01-01 03:00:00</td>
      <td>0.509091</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>2010</td>
      <td>1</td>
      <td>1</td>
      <td>4</td>
      <td>NaN</td>
      <td>1018.0</td>
      <td>2010-01-01 04:00:00</td>
      <td>0.490909</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_val</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>No</th>
      <th>year</th>
      <th>month</th>
      <th>day</th>
      <th>hour</th>
      <th>pm2.5</th>
      <th>PRES</th>
      <th>datetime</th>
      <th>scaled_PRES</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>35064</th>
      <td>35065</td>
      <td>2014</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>24.0</td>
      <td>1014.0</td>
      <td>2014-01-01 00:00:00</td>
      <td>0.418182</td>
    </tr>
    <tr>
      <th>35065</th>
      <td>35066</td>
      <td>2014</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>53.0</td>
      <td>1013.0</td>
      <td>2014-01-01 01:00:00</td>
      <td>0.400000</td>
    </tr>
    <tr>
      <th>35066</th>
      <td>35067</td>
      <td>2014</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>65.0</td>
      <td>1013.0</td>
      <td>2014-01-01 02:00:00</td>
      <td>0.400000</td>
    </tr>
    <tr>
      <th>35067</th>
      <td>35068</td>
      <td>2014</td>
      <td>1</td>
      <td>1</td>
      <td>3</td>
      <td>70.0</td>
      <td>1013.0</td>
      <td>2014-01-01 03:00:00</td>
      <td>0.400000</td>
    </tr>
    <tr>
      <th>35068</th>
      <td>35069</td>
      <td>2014</td>
      <td>1</td>
      <td>1</td>
      <td>4</td>
      <td>79.0</td>
      <td>1012.0</td>
      <td>2014-01-01 04:00:00</td>
      <td>0.381818</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<ul class="simple">
<li><p>Restablecemos los <em><strong>índices del conjunto de validación</strong></em></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_val</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_val</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>No</th>
      <th>year</th>
      <th>month</th>
      <th>day</th>
      <th>hour</th>
      <th>pm2.5</th>
      <th>PRES</th>
      <th>datetime</th>
      <th>scaled_PRES</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>35065</td>
      <td>2014</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>24.0</td>
      <td>1014.0</td>
      <td>2014-01-01 00:00:00</td>
      <td>0.418182</td>
    </tr>
    <tr>
      <th>1</th>
      <td>35066</td>
      <td>2014</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>53.0</td>
      <td>1013.0</td>
      <td>2014-01-01 01:00:00</td>
      <td>0.400000</td>
    </tr>
    <tr>
      <th>2</th>
      <td>35067</td>
      <td>2014</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>65.0</td>
      <td>1013.0</td>
      <td>2014-01-01 02:00:00</td>
      <td>0.400000</td>
    </tr>
    <tr>
      <th>3</th>
      <td>35068</td>
      <td>2014</td>
      <td>1</td>
      <td>1</td>
      <td>3</td>
      <td>70.0</td>
      <td>1013.0</td>
      <td>2014-01-01 03:00:00</td>
      <td>0.400000</td>
    </tr>
    <tr>
      <th>4</th>
      <td>35069</td>
      <td>2014</td>
      <td>1</td>
      <td>1</td>
      <td>4</td>
      <td>79.0</td>
      <td>1012.0</td>
      <td>2014-01-01 04:00:00</td>
      <td>0.381818</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<ul class="simple">
<li><p>También <em><strong>se grafican las series temporales de entrenamiento y validación normalizadas para PRES</strong></em>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> 
<span class="n">g1</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="s1">&#39;scaled_PRES&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>
<span class="n">g1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Time series of scaled Air Pressure in train set&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>
<span class="n">g1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Index&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>
<span class="n">g1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Scaled Air Pressure readings&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">);</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">g2</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">df_val</span><span class="p">[</span><span class="s1">&#39;scaled_PRES&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">g2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Time series of scaled Air Pressure in validation set&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>
<span class="n">g2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Index&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>
<span class="n">g2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Scaled Air Pressure readings&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">);</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/a782c59ba7cfcbb0f8f66eba761684df0a3ba4c8f36e54418df4a161108909e7.png" src="_images/a782c59ba7cfcbb0f8f66eba761684df0a3ba4c8f36e54418df4a161108909e7.png" />
</div>
</div>
<div class="tip admonition">
<p class="admonition-title">Regresores y Variable Objetivo</p>
<ul class="simple">
<li><p>Ahora necesitamos generar los <em><strong>regresores</strong></em> (<span class="math notranslate nohighlight">\(X\)</span>) y la <em><strong>variable objetivo</strong></em> (<span class="math notranslate nohighlight">\(y\)</span>) para el entrenamiento y la validación. <em><strong>La matriz bidimensional de regresores y la matriz unidimensional objetivo se crean a partir de la matriz unidimensional original de la columna standardized_PRES</strong></em> en el <em>DataFrame</em>.</p></li>
<li><p>Para el modelo de predicción de series temporales, de este ejemplo, <em><strong>se utilizan las observaciones de los últimos siete días para predecir el día siguiente</strong></em>. Esto equivale a un modelo <span class="math notranslate nohighlight">\(AR(7)\)</span>. Definimos una función que toma la serie temporal original y el número de pasos temporales en regresores como entrada para generar las matrices <span class="math notranslate nohighlight">\(X\)</span> e <span class="math notranslate nohighlight">\(y\)</span></p></li>
</ul>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">makeXy</span><span class="p">(</span><span class="n">ts</span><span class="p">,</span> <span class="n">nb_timesteps</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Input: </span>
<span class="sd">           ts: original time series</span>
<span class="sd">           nb_timesteps: number of time steps in the regressors</span>
<span class="sd">    Output: </span>
<span class="sd">           X: 2-D array of regressors</span>
<span class="sd">           y: 1-D array of target </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">X</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_timesteps</span><span class="p">,</span> <span class="n">ts</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="k">if</span> <span class="n">i</span><span class="o">-</span><span class="n">nb_timesteps</span> <span class="o">&lt;=</span> <span class="mi">4</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">i</span><span class="o">-</span><span class="n">nb_timesteps</span><span class="p">,</span> <span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
        <span class="n">X</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">ts</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="n">nb_timesteps</span><span class="p">:</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span> <span class="c1">#Regressors</span>
        <span class="n">y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ts</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="c1">#Target</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">makeXy</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="s1">&#39;scaled_PRES&#39;</span><span class="p">],</span> <span class="mi">7</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0 6 7
1 7 8
2 8 9
3 9 10
4 10 11
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Shape of train arrays:&#39;</span><span class="p">,</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shape of train arrays: (35057, 7) (35057,)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">makeXy</span><span class="p">(</span><span class="n">df_val</span><span class="p">[</span><span class="s1">&#39;scaled_PRES&#39;</span><span class="p">],</span> <span class="mi">7</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0 6 7
1 7 8
2 8 9
3 9 10
4 10 11
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Shape of validation arrays:&#39;</span><span class="p">,</span> <span class="n">X_val</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_val</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shape of validation arrays: (8753, 7) (8753,)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Ahora definimos la red <code class="docutils literal notranslate"><span class="pre">MLP</span></code> utilizando la <code class="docutils literal notranslate"><span class="pre">API</span></code> funcional de <code class="docutils literal notranslate"><span class="pre">Keras</span></code>. En este enfoque <em><strong>una capa puede ser declarada como la entrada de la siguiente capa en el momento de definir la siguiente</strong></em></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">input_layer</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>En este caso, <code class="docutils literal notranslate"><span class="pre">Input</span></code> es una función que se utiliza para <em><strong>crear una capa de entrada en un modelo de red neuronal</strong></em>. <code class="docutils literal notranslate"><span class="pre">shape=(7,)</span></code> específica la <em><strong>forma de los datos de entrada</strong></em>. En este caso, <em>significa que los datos de entrada tendrán 7 dimensiones</em>. <code class="docutils literal notranslate"><span class="pre">dtype='float32'</span></code> especifica el <em><strong>tipo de datos de los elementos de la capa de entrada</strong></em>. En este caso, son números de punto <em>flotante de 32 bits</em>.</p></li>
</ul>
<ul class="simple">
<li><p>Las capas densas las definimos en esta caso con <em><strong>activación</strong></em> <code class="docutils literal notranslate"><span class="pre">tanh</span></code>. Puede utilizar un <code class="docutils literal notranslate"><span class="pre">GridSearch</span></code> tal como se hizo en el curso de <em><strong>Machine Learning</strong></em> para encontrar <em><strong>hiperparámetros adecuados minimizando las métricas de regresión</strong></em>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;tanh&#39;</span><span class="p">)(</span><span class="n">input_layer</span><span class="p">)</span>
<span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;tanh&#39;</span><span class="p">)(</span><span class="n">dense1</span><span class="p">)</span>
<span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;tanh&#39;</span><span class="p">)(</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="tip admonition">
<p class="admonition-title"><strong>Dense</strong> e <strong>input_layer</strong></p>
<ul class="simple">
<li><p><em><strong><code class="docutils literal notranslate"><span class="pre">Dense</span></code></strong></em>: Correspone a una <em><strong>capa totalmente conectada</strong></em> (fully connected).</p></li>
<li><p><em><strong>Unidades (Neurons)</strong></em>: 32 neuronas.</p></li>
<li><p><em><strong><code class="docutils literal notranslate"><span class="pre">dense1</span></code></strong></em>: Esta capa <em><strong>toma como entrada</strong></em> <code class="docutils literal notranslate"><span class="pre">input_layer</span></code>, que puede ser la <em><strong>capa de entrada del modelo u otra capa anterior</strong></em>.</p></li>
<li><p><em><strong><code class="docutils literal notranslate"><span class="pre">dense2</span></code></strong></em>: Esta capa <em><strong>toma como entrada la salida de</strong></em> <code class="docutils literal notranslate"><span class="pre">dense1</span></code>. Esto significa que <em><strong>los 32 valores de salida de</strong></em> <code class="docutils literal notranslate"><span class="pre">dense1</span></code> <em><strong>se usan como entrada para</strong></em> <code class="docutils literal notranslate"><span class="pre">dense2</span></code>. Similarmente, ocurre con <code class="docutils literal notranslate"><span class="pre">dense3</span></code></p></li>
</ul>
</div>
<div class="admonition-observacion admonition">
<p class="admonition-title">Observación</p>
<ul class="simple">
<li><p>Las <em><strong>múltiples capas ocultas y el gran número de neuronas en cada capa oculta</strong></em> le dan a las redes neuronales la <em><strong>capacidad de modelar la compleja no linealidad de las relaciones subyacentes entre los regresores y el objetivo</strong></em>. Sin embargo, las redes neuronales profundas también <em><strong>pueden sobreajustar los datos de entrenamiento</strong></em> y dar malos resultados en el conjunto de validación o prueba. La función <code class="docutils literal notranslate"><span class="pre">Dropout</span></code> se ha utilizado eficazmente para <em><strong>regularizar las redes neuronales profundas</strong></em>.</p></li>
</ul>
</div>
<ul class="simple">
<li><p>En este ejemplo, se añade una capa <code class="docutils literal notranslate"><span class="pre">Dropout</span></code> antes de la capa de salida. <em><strong>Dropout aleatoriamente establece</strong></em> <span class="math notranslate nohighlight">\(p\)</span> <em><strong>fracción de neuronas de entrada a cero antes de pasar a la siguiente capa</strong></em>. La eliminación aleatoria de entradas actúa esencialmente como un tipo de ensamblaje de modelos de agregación <code class="docutils literal notranslate"><span class="pre">bootstrap</span></code>. Al <em><strong>apagar neuronas aleatoriamente</strong></em>, <code class="docutils literal notranslate"><span class="pre">Dropout</span></code> está <em><strong>forzando a la red a no depender excesivamente de ninguna unidad específica, mejorando la generalización</strong></em>.</p></li>
<li><p>Por ejemplo, <em><strong>el bosque aleatorio utiliza el ensamblaje mediante la construcción de árboles en subconjuntos aleatorios de características de entrada</strong></em>. Utilizamos <span class="math notranslate nohighlight">\(p=0.2\)</span> para <em>descartar el 20% de las características de entrada seleccionadas aleatoriamente</em>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dropout_layer</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)(</span><span class="n">dense3</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Por último, <em><strong>la capa de salida predice la presión atmosférica del día siguiente</strong></em></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">output_layer</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">)(</span><span class="n">dropout_layer</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Las capas de entrada, densa y de salida se empaquetarán ahora dentro de un modelo, que es una clase envolvente para entrenar y hacer predicciones. <em><strong>Como función de pérdida se utiliza el error cuadrático medio</strong></em> (MSE). Los pesos de la red se optimizan mediante el algoritmo <code class="docutils literal notranslate"><span class="pre">Adam</span></code>. <code class="docutils literal notranslate"><span class="pre">Adam</span></code> significa <em><strong>Estimación Adaptativa de Momentos</strong></em> y ha sido una opción popular para el entrenamiento de redes neuronales profundas.</p></li>
<li><p>A diferencia del <em><strong>Gradiente descendente Estocástico, Adam utiliza diferentes tasas de aprendizaje para cada peso y las actualiza por separado a medida que avanza el entrenamiento</strong></em>. La tasa de aprendizaje de un peso se actualiza basándose en medias móviles ponderadas exponencialmente de los gradientes del peso y los gradientes al cuadrado.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ts_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">input_layer</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">output_layer</span><span class="p">)</span>
<span class="n">ts_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;mean_squared_error&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">)</span>
<span class="n">ts_model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "functional_3"</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Layer (type)                    </span>┃<span style="font-weight: bold"> Output Shape           </span>┃<span style="font-weight: bold">       Param # </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ input_layer_3 (<span style="color: #0087ff; text-decoration-color: #0087ff">InputLayer</span>)      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">7</span>)              │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_9 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                 │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">32</span>)             │           <span style="color: #00af00; text-decoration-color: #00af00">256</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_10 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)             │           <span style="color: #00af00; text-decoration-color: #00af00">528</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_11 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)             │           <span style="color: #00af00; text-decoration-color: #00af00">272</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dropout_3 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)             │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)             │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_12 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)              │            <span style="color: #00af00; text-decoration-color: #00af00">17</span> │
└─────────────────────────────────┴────────────────────────┴───────────────┘
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">1,073</span> (4.19 KB)
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">1,073</span> (4.19 KB)
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div></div>
</div>
<div class="tip admonition">
<p class="admonition-title">Observación</p>
<ul class="simple">
<li><p>En este caso, <code class="docutils literal notranslate"><span class="pre">Params</span> <span class="pre">#</span></code> es calculado mediante la fórmula: <em><strong><code class="docutils literal notranslate"><span class="pre">Param</span> <span class="pre">#</span> <span class="pre">=</span> <span class="pre">#Entradas</span> <span class="pre">x</span> <span class="pre">#Neuronas</span> <span class="pre">+</span> <span class="pre">#Neuronas</span></code></strong></em>. Esto incluye los <em><strong>pesos de cada conexión entre las entradas y las neuronas</strong></em> y un <em><strong>bias para cada neurona</strong></em>. En este caso <code class="docutils literal notranslate"><span class="pre">Params</span> <span class="pre">#</span> <span class="pre">=</span> <span class="pre">7x32</span> <span class="pre">+</span> <span class="pre">32</span> <span class="pre">=</span> <span class="pre">256</span></code>.</p></li>
<li><p>El modelo se entrena llamando a la función <code class="docutils literal notranslate"><span class="pre">fit()</span></code> en el objeto modelo y pasándole <code class="docutils literal notranslate"><span class="pre">X_train</span></code> y <code class="docutils literal notranslate"><span class="pre">y_train</span></code>. El entrenamiento se realiza para un <em><strong>número predefinido de épocas</strong></em>. Además, <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> define el <em><strong>número de muestras del conjunto de entrenamiento que se utilizarán para una instancia de backpropagation</strong></em>.</p></li>
</ul>
</div>
<ul class="simple">
<li><p>El conjunto de datos de validación también se pasa para evaluar el modelo después de cada <code class="docutils literal notranslate"><span class="pre">epoch</span></code> completa. Un objeto <em><strong><code class="docutils literal notranslate"><span class="pre">ModelCheckpoint</span></code> rastrea la función de pérdida en el conjunto de validación y guarda el modelo para la época en la que la función de pérdida ha sido mínima</strong></em>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">save_weights_at</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;keras_models&#39;</span><span class="p">,</span> <span class="s1">&#39;PRSA_data_Air_Pressure_MLP_weights.</span><span class="si">{epoch:02d}</span><span class="s1">-</span><span class="si">{val_loss:.4f}</span><span class="s1">.keras&#39;</span><span class="p">)</span>
<span class="n">save_best</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">save_weights_at</span><span class="p">,</span> <span class="n">monitor</span><span class="o">=</span><span class="s1">&#39;val_loss&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                            <span class="n">save_best_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">save_weights_only</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;min&#39;</span><span class="p">,</span> <span class="n">save_freq</span><span class="o">=</span><span class="s1">&#39;epoch&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Aquí <code class="docutils literal notranslate"><span class="pre">val_loss</span></code> es el <em><strong>valor de la función de coste para los datos de validación cruzada</strong></em> y <code class="docutils literal notranslate"><span class="pre">loss</span></code> es el <em><strong>valor de la función de coste para los datos de entrenamiento</strong></em>. Con <code class="docutils literal notranslate"><span class="pre">verbose=0</span></code>, no se imprime ningún mensaje en la consola durante el proceso de guardado del modelo. <code class="docutils literal notranslate"><span class="pre">period=1</span></code> indica que <em><strong>el modelo se evaluará y potencialmente se guardará después de cada época</strong></em>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">joblib</span> <span class="kn">import</span> <span class="n">dump</span><span class="p">,</span> <span class="n">load</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">history_airp</span> <span class="o">=</span> <span class="kc">None</span>

<span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="s1">&#39;history_airp.joblib&#39;</span><span class="p">):</span>
    <span class="n">history_airp</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="s1">&#39;history_airp.joblib&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;El archivo &#39;history_airp.joblib&#39; ya existe. Se ha cargado el historial del entrenamiento.&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">history_airp</span> <span class="o">=</span> <span class="n">ts_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
             <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">save_best</span><span class="p">],</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">),</span>
             <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">);</span>
    <span class="n">dump</span><span class="p">(</span><span class="n">history_airp</span><span class="o">.</span><span class="n">history</span><span class="p">,</span> <span class="s1">&#39;history_airp.joblib&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;El entrenamiento se ha completado y el historial ha sido guardado en &#39;history_airp.joblib&#39;.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/20
2192/2192 - 4s - 2ms/step - loss: 0.0030 - val_loss: 2.0931e-04
Epoch 2/20
2192/2192 - 2s - 913us/step - loss: 9.7843e-04 - val_loss: 3.7588e-04
Epoch 3/20
2192/2192 - 2s - 918us/step - loss: 8.4390e-04 - val_loss: 2.4903e-04
Epoch 4/20
2192/2192 - 2s - 990us/step - loss: 8.0117e-04 - val_loss: 2.4213e-04
Epoch 5/20
2192/2192 - 2s - 1ms/step - loss: 7.8276e-04 - val_loss: 1.7230e-04
Epoch 6/20
2192/2192 - 2s - 995us/step - loss: 7.9531e-04 - val_loss: 8.4024e-04
Epoch 7/20
2192/2192 - 2s - 945us/step - loss: 8.0365e-04 - val_loss: 1.7579e-04
Epoch 8/20
2192/2192 - 2s - 1ms/step - loss: 7.8178e-04 - val_loss: 2.3215e-04
Epoch 9/20
2192/2192 - 2s - 998us/step - loss: 7.5834e-04 - val_loss: 1.5838e-04
Epoch 10/20
2192/2192 - 2s - 983us/step - loss: 7.5602e-04 - val_loss: 1.5858e-04
Epoch 11/20
2192/2192 - 2s - 949us/step - loss: 7.7194e-04 - val_loss: 3.1116e-04
Epoch 12/20
2192/2192 - 2s - 974us/step - loss: 7.7687e-04 - val_loss: 1.4221e-04
Epoch 13/20
2192/2192 - 2s - 968us/step - loss: 7.6399e-04 - val_loss: 2.1503e-04
Epoch 14/20
2192/2192 - 2s - 1ms/step - loss: 7.5355e-04 - val_loss: 2.1473e-04
Epoch 15/20
2192/2192 - 2s - 933us/step - loss: 7.6465e-04 - val_loss: 1.4276e-04
Epoch 16/20
2192/2192 - 2s - 927us/step - loss: 7.4503e-04 - val_loss: 3.0024e-04
Epoch 17/20
2192/2192 - 2s - 897us/step - loss: 7.6175e-04 - val_loss: 3.0910e-04
Epoch 18/20
2192/2192 - 2s - 949us/step - loss: 7.3964e-04 - val_loss: 3.1504e-04
Epoch 19/20
2192/2192 - 2s - 886us/step - loss: 7.6523e-04 - val_loss: 1.7569e-04
Epoch 20/20
2192/2192 - 2s - 969us/step - loss: 7.4868e-04 - val_loss: 2.6988e-04
El entrenamiento se ha completado y el historial ha sido guardado en &#39;history_airp.joblib&#39;.
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>En este caso, el modo <code class="docutils literal notranslate"><span class="pre">verbose=2</span></code> muestra una <em><strong>barra de progreso por cada época</strong></em>. Los modos posibles son: <code class="docutils literal notranslate"><span class="pre">0</span></code> para no mostrar nada, <code class="docutils literal notranslate"><span class="pre">1</span></code> para mostrar la barra de progreso, <code class="docutils literal notranslate"><span class="pre">2</span></code> para mostrar una línea por época. Las <em><strong>muestras se mezclan aleatoriamente antes de cada época</strong></em> (<code class="docutils literal notranslate"><span class="pre">shuffle=True</span></code>).</p></li>
</ul>
<ul class="simple">
<li><p>Se hacen predicciones para la presión atmosférica a partir del mejor modelo guardado. <em><strong>Las predicciones del modelo sobre la presión atmosférica escalada, se transforman inversamente para obtener predicciones sobre la presión atmosférica original</strong></em>. También se calcula la bondad de ajuste o <em><strong><code class="docutils literal notranslate"><span class="pre">R</span> <span class="pre">cuadrado</span></code></strong></em>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">load_model</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_dir</span> <span class="o">=</span> <span class="s1">&#39;keras_models&#39;</span>
<span class="n">files</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">model_dir</span><span class="p">)</span>
<span class="n">pattern</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;PRSA_data_Air_Pressure_MLP_weights\.(\d+)-([\d\.]+)\.keras&quot;</span>

<span class="n">best_val_loss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">)</span>
<span class="n">best_model_file</span> <span class="o">=</span> <span class="kc">None</span>
<span class="n">best_model</span> <span class="o">=</span> <span class="kc">None</span>

<span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">files</span><span class="p">:</span>
    <span class="n">match</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">pattern</span><span class="p">,</span> <span class="n">file</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">match</span><span class="p">:</span>
        <span class="n">epoch</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">match</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">val_loss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">match</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">val_loss</span> <span class="o">&lt;</span> <span class="n">best_val_loss</span><span class="p">:</span>
            <span class="n">best_val_loss</span> <span class="o">=</span> <span class="n">val_loss</span>
            <span class="n">best_model_file</span> <span class="o">=</span> <span class="n">file</span>

<span class="k">if</span> <span class="n">best_model_file</span><span class="p">:</span>
    <span class="n">best_model_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">model_dir</span><span class="p">,</span> <span class="n">best_model_file</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Cargando el mejor modelo: </span><span class="si">{</span><span class="n">best_model_file</span><span class="si">}</span><span class="s2"> con val_loss: </span><span class="si">{</span><span class="n">best_val_loss</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">best_model</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="n">best_model_path</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;No se encontraron archivos de modelos que coincidan con el patrón.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cargando el mejor modelo: PRSA_data_Air_Pressure_MLP_weights.09-0.0001.keras con val_loss: 0.0001
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">preds</span> <span class="o">=</span> <span class="n">best_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>
<span class="n">pred_PRES</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span>
<span class="n">pred_PRES</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">pred_PRES</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold">274/274</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">0s</span> 884us/step
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">r2</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">df_val</span><span class="p">[</span><span class="s1">&#39;PRES&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">7</span><span class="p">:],</span> <span class="n">pred_PRES</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;R-squared for the validation set:&#39;</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">r2</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>R-squared for the validation set: 0.9957
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">5.5</span><span class="p">,</span> <span class="mf">5.5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span> <span class="n">df_val</span><span class="p">[</span><span class="s1">&#39;PRES&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">7</span><span class="p">:</span><span class="mi">56</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;*&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span> <span class="n">pred_PRES</span><span class="p">[:</span><span class="mi">50</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;Actual&#39;</span><span class="p">,</span><span class="s1">&#39;Predicted&#39;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Actual vs Predicted Air Pressure&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Air Pressure&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Index&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/7f668204bf7016917bd95614cd1833077649102a763b564480666610c4ae95eb.png" src="_images/7f668204bf7016917bd95614cd1833077649102a763b564480666610c4ae95eb.png" />
</div>
</div>
<ul class="simple">
<li><p>Para <em><strong>predecir la variable</strong></em> <code class="docutils literal notranslate"><span class="pre">pm2.5</span></code> usando <code class="docutils literal notranslate"><span class="pre">MLP</span></code> usamos la implementación presentada a continuación</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;datasets/PRSA_data_2010.1.1-2014.12.31.csv&#39;</span><span class="p">,</span> <span class="n">usecols</span><span class="o">=</span><span class="k">lambda</span> <span class="n">column</span><span class="p">:</span> <span class="n">column</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;DEWP&#39;</span><span class="p">,</span> <span class="s1">&#39;TEMP&#39;</span><span class="p">,</span> <span class="s1">&#39;cbwd&#39;</span><span class="p">,</span> <span class="s1">&#39;Iws&#39;</span><span class="p">,</span>	<span class="s1">&#39;Is&#39;</span><span class="p">,</span> <span class="s1">&#39;Ir&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Shape of the dataframe:&#39;</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shape of the dataframe: (43824, 7)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>No</th>
      <th>year</th>
      <th>month</th>
      <th>day</th>
      <th>hour</th>
      <th>pm2.5</th>
      <th>PRES</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>2010</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>NaN</td>
      <td>1021.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>2010</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>NaN</td>
      <td>1020.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>2010</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>NaN</td>
      <td>1019.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>2010</td>
      <td>1</td>
      <td>1</td>
      <td>3</td>
      <td>NaN</td>
      <td>1019.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>2010</td>
      <td>1</td>
      <td>1</td>
      <td>4</td>
      <td>NaN</td>
      <td>1018.0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<ul class="simple">
<li><p>Las <em><strong>filas con valores NaN</strong></em> en la columna pm2.5 <em><strong>se eliminan</strong></em></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;pm2.5&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;datetime&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;year&#39;</span><span class="p">,</span> <span class="s1">&#39;month&#39;</span><span class="p">,</span> <span class="s1">&#39;day&#39;</span><span class="p">,</span> <span class="s1">&#39;hour&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">row</span><span class="p">:</span> <span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="p">(</span><span class="n">year</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">],</span> <span class="n">month</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;month&#39;</span><span class="p">],</span> <span class="n">day</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;day&#39;</span><span class="p">],</span>
                                                                                          <span class="n">hour</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;hour&#39;</span><span class="p">]),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;datetime&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> 
<span class="n">g1</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;pm2.5&#39;</span><span class="p">],</span> <span class="n">orient</span><span class="o">=</span><span class="s2">&quot;h&quot;</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="s2">&quot;Set2&quot;</span><span class="p">)</span>
<span class="n">g1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Box plot of pm2.5&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">);</span>
<span class="n">g1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;pm2.5&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>
<span class="n">g1</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">yticklabels</span><span class="o">=</span><span class="p">[])</span>
<span class="n">g1</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="kc">False</span><span class="p">);</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">g2</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;pm2.5&#39;</span><span class="p">])</span>
<span class="n">g2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Time series of pm2.5&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>
<span class="n">g2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Index&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>
<span class="n">g2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;pm2.5 readings&#39;</span><span class="p">);</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/d492abe47224e8c07be16807d3c2595090f2e3c65ddc7fa41b8e258a7eb813ef.png" src="_images/d492abe47224e8c07be16807d3c2595090f2e3c65ddc7fa41b8e258a7eb813ef.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">g1</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;pm2.5&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;datetime&#39;</span><span class="p">]</span><span class="o">&lt;=</span><span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="p">(</span><span class="n">year</span><span class="o">=</span><span class="mi">2010</span><span class="p">,</span><span class="n">month</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span><span class="n">day</span><span class="o">=</span><span class="mi">30</span><span class="p">)],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">)</span>
<span class="n">g1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;pm2.5 during 2010&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>
<span class="n">g1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Index&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>
<span class="n">g1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;pm2.5 readings&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">);</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;pm2.5&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;datetime&#39;</span><span class="p">]</span><span class="o">&lt;=</span><span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="p">(</span><span class="n">year</span><span class="o">=</span><span class="mi">2010</span><span class="p">,</span><span class="n">month</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">day</span><span class="o">=</span><span class="mi">31</span><span class="p">)],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">)</span>
<span class="n">g</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Zoom in on one month: pm2.5 during Jan 2010&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>
<span class="n">g</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Index&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>
<span class="n">g</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;pm2.5 readings&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">);</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/35938c40e57736463d7d1458d7b8da872f1822c07f18061b1da90c9b38463f58.png" src="_images/35938c40e57736463d7d1458d7b8da872f1822c07f18061b1da90c9b38463f58.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">(</span><span class="n">feature_range</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;scaled_pm2.5&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;pm2.5&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">split_date</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="p">(</span><span class="n">year</span><span class="o">=</span><span class="mi">2014</span><span class="p">,</span> <span class="n">month</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">day</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hour</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">df_train</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;datetime&#39;</span><span class="p">]</span><span class="o">&lt;</span><span class="n">split_date</span><span class="p">]</span>
<span class="n">df_val</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;datetime&#39;</span><span class="p">]</span><span class="o">&gt;=</span><span class="n">split_date</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Shape of train:&#39;</span><span class="p">,</span> <span class="n">df_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Shape of test:&#39;</span><span class="p">,</span> <span class="n">df_val</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shape of train: (33096, 9)
Shape of test: (8661, 9)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_train</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>No</th>
      <th>year</th>
      <th>month</th>
      <th>day</th>
      <th>hour</th>
      <th>pm2.5</th>
      <th>PRES</th>
      <th>datetime</th>
      <th>scaled_pm2.5</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>25</td>
      <td>2010</td>
      <td>1</td>
      <td>2</td>
      <td>0</td>
      <td>129.0</td>
      <td>1020.0</td>
      <td>2010-01-02 00:00:00</td>
      <td>0.129779</td>
    </tr>
    <tr>
      <th>1</th>
      <td>26</td>
      <td>2010</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>148.0</td>
      <td>1020.0</td>
      <td>2010-01-02 01:00:00</td>
      <td>0.148893</td>
    </tr>
    <tr>
      <th>2</th>
      <td>27</td>
      <td>2010</td>
      <td>1</td>
      <td>2</td>
      <td>2</td>
      <td>159.0</td>
      <td>1021.0</td>
      <td>2010-01-02 02:00:00</td>
      <td>0.159960</td>
    </tr>
    <tr>
      <th>3</th>
      <td>28</td>
      <td>2010</td>
      <td>1</td>
      <td>2</td>
      <td>3</td>
      <td>181.0</td>
      <td>1022.0</td>
      <td>2010-01-02 03:00:00</td>
      <td>0.182093</td>
    </tr>
    <tr>
      <th>4</th>
      <td>29</td>
      <td>2010</td>
      <td>1</td>
      <td>2</td>
      <td>4</td>
      <td>138.0</td>
      <td>1022.0</td>
      <td>2010-01-02 04:00:00</td>
      <td>0.138833</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_val</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>No</th>
      <th>year</th>
      <th>month</th>
      <th>day</th>
      <th>hour</th>
      <th>pm2.5</th>
      <th>PRES</th>
      <th>datetime</th>
      <th>scaled_pm2.5</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>33096</th>
      <td>35065</td>
      <td>2014</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>24.0</td>
      <td>1014.0</td>
      <td>2014-01-01 00:00:00</td>
      <td>0.024145</td>
    </tr>
    <tr>
      <th>33097</th>
      <td>35066</td>
      <td>2014</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>53.0</td>
      <td>1013.0</td>
      <td>2014-01-01 01:00:00</td>
      <td>0.053320</td>
    </tr>
    <tr>
      <th>33098</th>
      <td>35067</td>
      <td>2014</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>65.0</td>
      <td>1013.0</td>
      <td>2014-01-01 02:00:00</td>
      <td>0.065392</td>
    </tr>
    <tr>
      <th>33099</th>
      <td>35068</td>
      <td>2014</td>
      <td>1</td>
      <td>1</td>
      <td>3</td>
      <td>70.0</td>
      <td>1013.0</td>
      <td>2014-01-01 03:00:00</td>
      <td>0.070423</td>
    </tr>
    <tr>
      <th>33100</th>
      <td>35069</td>
      <td>2014</td>
      <td>1</td>
      <td>1</td>
      <td>4</td>
      <td>79.0</td>
      <td>1012.0</td>
      <td>2014-01-01 04:00:00</td>
      <td>0.079477</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_val</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">g1</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="s1">&#39;scaled_pm2.5&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>
<span class="n">g1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Time series of scaled pm2.5 in train set&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>
<span class="n">g1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Index&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>
<span class="n">g1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Scaled pm2.5 readings&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">);</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">g2</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">df_val</span><span class="p">[</span><span class="s1">&#39;scaled_pm2.5&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">g2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Time series of scaled pm2.5 in validation set&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>
<span class="n">g2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Index&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>
<span class="n">g2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Scaled pm2.5 readings&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">);</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/73112cb48201fbe34418cfd68bbb6269057957ea5aa05119dbd76a421377b504.png" src="_images/73112cb48201fbe34418cfd68bbb6269057957ea5aa05119dbd76a421377b504.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">makeXy</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="s1">&#39;scaled_pm2.5&#39;</span><span class="p">],</span> <span class="mi">7</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0 6 7
1 7 8
2 8 9
3 9 10
4 10 11
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Shape of train arrays:&#39;</span><span class="p">,</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shape of train arrays: (33089, 7) (33089,)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">makeXy</span><span class="p">(</span><span class="n">df_val</span><span class="p">[</span><span class="s1">&#39;scaled_pm2.5&#39;</span><span class="p">],</span> <span class="mi">7</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0 6 7
1 7 8
2 8 9
3 9 10
4 10 11
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Shape of validation arrays:&#39;</span><span class="p">,</span> <span class="n">X_val</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_val</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shape of validation arrays: (8654, 7) (8654,)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Definimos la <em><strong>capa de entrada</strong></em> que tiene forma <code class="docutils literal notranslate"><span class="pre">(None,</span> <span class="pre">7)</span></code> y de tipo <code class="docutils literal notranslate"><span class="pre">float32</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">input_layer</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">input_layer</span><span class="p">)</span>
<span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">dense1</span><span class="p">)</span>
<span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dropout_layer</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)(</span><span class="n">dense3</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">output_layer</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">)(</span><span class="n">dropout_layer</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ts_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">input_layer</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">output_layer</span><span class="p">)</span>
<span class="n">ts_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;mean_absolute_error&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">)</span>
<span class="n">ts_model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "functional_4"</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Layer (type)                    </span>┃<span style="font-weight: bold"> Output Shape           </span>┃<span style="font-weight: bold">       Param # </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ input_layer_4 (<span style="color: #0087ff; text-decoration-color: #0087ff">InputLayer</span>)      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">7</span>)              │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_13 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">32</span>)             │           <span style="color: #00af00; text-decoration-color: #00af00">256</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_14 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)             │           <span style="color: #00af00; text-decoration-color: #00af00">528</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_15 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)             │           <span style="color: #00af00; text-decoration-color: #00af00">272</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dropout_4 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)             │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)             │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_16 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)              │            <span style="color: #00af00; text-decoration-color: #00af00">17</span> │
└─────────────────────────────────┴────────────────────────┴───────────────┘
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">1,073</span> (4.19 KB)
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">1,073</span> (4.19 KB)
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">save_weights_at</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;keras_models&#39;</span><span class="p">,</span> <span class="s1">&#39;PRSA_data_PM2.5_MLP_weights.</span><span class="si">{epoch:02d}</span><span class="s1">-</span><span class="si">{val_loss:.4f}</span><span class="s1">.keras&#39;</span><span class="p">)</span>
<span class="n">save_best</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">save_weights_at</span><span class="p">,</span> <span class="n">monitor</span><span class="o">=</span><span class="s1">&#39;val_loss&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                            <span class="n">save_best_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">save_weights_only</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;min&#39;</span><span class="p">,</span> <span class="n">save_freq</span><span class="o">=</span><span class="s1">&#39;epoch&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">history_pm25</span> <span class="o">=</span> <span class="kc">None</span>

<span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="s1">&#39;history_pm25.joblib&#39;</span><span class="p">):</span>
    <span class="n">history_pm25</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="s1">&#39;history_pm25.joblib&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;El archivo &#39;history_pm25.joblib&#39; ya existe. Se ha cargado el historial del entrenamiento.&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">history_pm25</span> <span class="o">=</span> <span class="n">ts_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
             <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">save_best</span><span class="p">],</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">),</span>
             <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">);</span>
    <span class="n">dump</span><span class="p">(</span><span class="n">history_pm25</span><span class="o">.</span><span class="n">history</span><span class="p">,</span> <span class="s1">&#39;history_pm25.joblib&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;El entrenamiento se ha completado y el historial ha sido guardado en &#39;history_pm25.joblib&#39;.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/20
2069/2069 - 4s - 2ms/step - loss: 0.0214 - val_loss: 0.0159
Epoch 2/20
2069/2069 - 2s - 1ms/step - loss: 0.0195 - val_loss: 0.0163
Epoch 3/20
2069/2069 - 2s - 967us/step - loss: 0.0192 - val_loss: 0.0154
Epoch 4/20
2069/2069 - 2s - 1ms/step - loss: 0.0191 - val_loss: 0.0136
Epoch 5/20
2069/2069 - 2s - 1ms/step - loss: 0.0190 - val_loss: 0.0126
Epoch 6/20
2069/2069 - 2s - 1ms/step - loss: 0.0189 - val_loss: 0.0153
Epoch 7/20
2069/2069 - 2s - 1ms/step - loss: 0.0188 - val_loss: 0.0131
Epoch 8/20
2069/2069 - 2s - 1ms/step - loss: 0.0189 - val_loss: 0.0146
Epoch 9/20
2069/2069 - 2s - 1ms/step - loss: 0.0188 - val_loss: 0.0150
Epoch 10/20
2069/2069 - 2s - 985us/step - loss: 0.0188 - val_loss: 0.0136
Epoch 11/20
2069/2069 - 2s - 1ms/step - loss: 0.0186 - val_loss: 0.0128
Epoch 12/20
2069/2069 - 2s - 1ms/step - loss: 0.0185 - val_loss: 0.0135
Epoch 13/20
2069/2069 - 2s - 1ms/step - loss: 0.0184 - val_loss: 0.0125
Epoch 14/20
2069/2069 - 2s - 994us/step - loss: 0.0186 - val_loss: 0.0138
Epoch 15/20
2069/2069 - 2s - 1ms/step - loss: 0.0184 - val_loss: 0.0144
Epoch 16/20
2069/2069 - 2s - 1000us/step - loss: 0.0184 - val_loss: 0.0134
Epoch 17/20
2069/2069 - 2s - 1ms/step - loss: 0.0184 - val_loss: 0.0130
Epoch 18/20
2069/2069 - 2s - 1ms/step - loss: 0.0185 - val_loss: 0.0135
Epoch 19/20
2069/2069 - 2s - 1ms/step - loss: 0.0185 - val_loss: 0.0121
Epoch 20/20
2069/2069 - 2s - 1ms/step - loss: 0.0185 - val_loss: 0.0124
El entrenamiento se ha completado y el historial ha sido guardado en &#39;history_pm25.joblib&#39;.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_dir</span> <span class="o">=</span> <span class="s1">&#39;keras_models&#39;</span>
<span class="n">files</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">model_dir</span><span class="p">)</span>
<span class="n">pattern</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;PRSA_data_PM2.5_MLP_weights\.(\d+)-([\d\.]+)\.keras&quot;</span>
    
<span class="n">best_val_loss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">)</span>
<span class="n">best_model_file</span> <span class="o">=</span> <span class="kc">None</span>
<span class="n">best_model</span> <span class="o">=</span> <span class="kc">None</span>

<span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">files</span><span class="p">:</span>
    <span class="n">match</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">pattern</span><span class="p">,</span> <span class="n">file</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">match</span><span class="p">:</span>
        <span class="n">epoch</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">match</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">val_loss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">match</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">val_loss</span> <span class="o">&lt;</span> <span class="n">best_val_loss</span><span class="p">:</span>
            <span class="n">best_val_loss</span> <span class="o">=</span> <span class="n">val_loss</span>
            <span class="n">best_model_file</span> <span class="o">=</span> <span class="n">file</span>

<span class="k">if</span> <span class="n">best_model_file</span><span class="p">:</span>
    <span class="n">best_model_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">model_dir</span><span class="p">,</span> <span class="n">best_model_file</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Cargando el mejor modelo: </span><span class="si">{</span><span class="n">best_model_file</span><span class="si">}</span><span class="s2"> con val_loss: </span><span class="si">{</span><span class="n">best_val_loss</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">best_model</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="n">best_model_path</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;No se encontraron archivos de modelos que coincidan con el patrón.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cargando el mejor modelo: PRSA_data_PM2.5_MLP_weights.16-0.0118.keras con val_loss: 0.0118
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">preds</span> <span class="o">=</span> <span class="n">best_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>
<span class="n">pred_pm25</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span>
<span class="n">pred_pm25</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">pred_pm25</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold">271/271</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">0s</span> 879us/step
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mae</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">df_val</span><span class="p">[</span><span class="s1">&#39;pm2.5&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">7</span><span class="p">:],</span> <span class="n">pred_pm25</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;MAE for the validation set:&#39;</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">mae</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>MAE for the validation set: 11.7255
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Trazamos los 50 primeros valores reales y predichos de <code class="docutils literal notranslate"><span class="pre">pm2.5</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">5.5</span><span class="p">,</span> <span class="mf">5.5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span> <span class="n">df_val</span><span class="p">[</span><span class="s1">&#39;pm2.5&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">7</span><span class="p">:</span><span class="mi">56</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;*&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span> <span class="n">pred_pm25</span><span class="p">[:</span><span class="mi">50</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;Actual&#39;</span><span class="p">,</span><span class="s1">&#39;Predicted&#39;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Actual vs Predicted pm2.5&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;pm2.5&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Index&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/cd3eb414d43524e8f9514a2e186b0de454abb6dfd2b4a4ba8d3f47f58a07fcd4.png" src="_images/cd3eb414d43524e8f9514a2e186b0de454abb6dfd2b4a4ba8d3f47f58a07fcd4.png" />
</div>
</div>
</section>
<section id="lstm-para-la-prediccion-de-series-de-tiempo">
<h2><span class="section-number">4.16. </span>LSTM para la predicción de series de tiempo<a class="headerlink" href="#lstm-para-la-prediccion-de-series-de-tiempo" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Continuaremos usando el conjunto de datos sobre contaminación del aire (pm2.5) usando LSTM. La lectura y el <em><strong>preprocesamiento de los datos se mantienen igual que en los ejemplos de <code class="docutils literal notranslate"><span class="pre">MLPs</span></code></strong></em>. El conjunto de datos original se divide en dos conjuntos: <em>entrenamiento y validación</em>, que se usan para el entrenamiento y validación del modelo respectivamente.</p></li>
</ul>
<ul class="simple">
<li><p>La función <code class="docutils literal notranslate"><span class="pre">makeXy</span></code> se utiliza para generar arreglos de regresores y objetivos: <code class="docutils literal notranslate"><span class="pre">X_train,</span> <span class="pre">X_val,</span> <span class="pre">y_train</span></code> y <code class="docutils literal notranslate"><span class="pre">y_val</span></code>. <code class="docutils literal notranslate"><span class="pre">X_train</span> <span class="pre">y</span> <span class="pre">X_val</span></code>, generados por la función <code class="docutils literal notranslate"><span class="pre">makeXy</span></code>, son <em><strong>arreglos 2D de forma (número de muestras, número de pasos de tiempo)</strong></em>. Sin embargo, la <em><strong>entrada a las capas RNN debe ser de forma (número de muestras, número de pasos de tiempo, número de características por paso de tiempo)</strong></em>.</p></li>
<li><p>En este caso, estamos tratando solo con <code class="docutils literal notranslate"><span class="pre">pm2.5</span></code>, por lo tanto, el <em><strong>número de características por paso de tiempo es uno</strong></em>. El número de pasos de tiempo es siete y el número de muestras es el mismo que el número de muestras en <code class="docutils literal notranslate"><span class="pre">X_train</span></code> y <code class="docutils literal notranslate"><span class="pre">X_val</span></code>, que se transforman a arreglos 3D</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_val</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">X_val</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">X_val</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X_val</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Shape of 3D arrays:&#39;</span><span class="p">,</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X_val</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shape of 3D arrays: (33089, 7, 1) (8654, 7, 1)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">X_train</span></code> y <code class="docutils literal notranslate"><span class="pre">X_val</span></code> se han <em><strong>convertido en matrices 3D</strong></em> y sus nuevas formas se ven en la salida de la instrucción de impresión anterior</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Input</span><span class="p">,</span> <span class="n">Dropout</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">LSTM</span>
<span class="kn">from</span> <span class="nn">keras.optimizers</span> <span class="kn">import</span> <span class="n">SGD</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">load_model</span>
<span class="kn">from</span> <span class="nn">keras.callbacks</span> <span class="kn">import</span> <span class="n">ModelCheckpoint</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>La red neuronal para desarrollar el modelo de predicción de series temporales tiene una <em><strong>capa de entrada, que alimenta a la capa LSTM</strong></em>. La capa LSTM tiene <em><strong>siete pasos temporales</strong></em>, que es el mismo número de observaciones históricas tomadas para hacer la predicción de la presión atmosférica para el día siguiente. Solo <em><strong>el último paso temporal de la LSTM devuelve una salida</strong></em>.</p></li>
<li><p>Hay <em><strong>sesenta y cuatro neuronas ocultas en cada paso temporal de la capa LSTM</strong></em>. Por lo tanto, la salida de la LSTM tiene sesenta y cuatro
características:</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">input_layer</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Aquí <code class="docutils literal notranslate"><span class="pre">shape=(7,1)</span></code> forma de la <em><strong>entrada a la LSTM</strong></em>. <code class="docutils literal notranslate"><span class="pre">7</span></code>: Número de <em><strong>pasos de tiempo (timesteps)</strong></em>. <code class="docutils literal notranslate"><span class="pre">1</span></code>: Número de <em><strong>características (features) por cada paso de tiempo</strong></em>. <code class="docutils literal notranslate"><span class="pre">return_sequences=True</span></code> se usa cuando se necesita <em><strong>pasar la secuencia completa de salidas a la siguiente capa de la red</strong></em>, que también puede ser una capa recurrente como otra LSTM</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lstm_layer1</span> <span class="o">=</span> <span class="n">LSTM</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">)(</span><span class="n">input_layer</span><span class="p">)</span>
<span class="n">lstm_layer2</span> <span class="o">=</span> <span class="n">LSTM</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">64</span><span class="p">),</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">lstm_layer1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>A continuación, <em><strong>la salida de LSTM pasa a una capa de exclusión que elimina aleatoriamente el 20% de la entrada antes de pasar a la capa de salida</strong></em>, que tiene una única neurona oculta con una función de <em><strong>activación lineal</strong></em></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dropout_layer</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)(</span><span class="n">lstm_layer2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">output_layer</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">)(</span><span class="n">dropout_layer</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Finalmente, todas las capas se envuelven en un <code class="docutils literal notranslate"><span class="pre">keras.models.Model</span></code> y se entrenan durante <em><strong>veinte epochs para minimizar el MSE</strong></em> utilizando el <em><strong>optimizador Adam</strong></em></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ts_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">input_layer</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">output_layer</span><span class="p">)</span>
<span class="n">ts_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;mean_absolute_error&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">)</span> <span class="c1">#SGD(lr=0.001, decay=1e-5))</span>
<span class="n">ts_model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "functional_5"</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Layer (type)                    </span>┃<span style="font-weight: bold"> Output Shape           </span>┃<span style="font-weight: bold">       Param # </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ input_layer_5 (<span style="color: #0087ff; text-decoration-color: #0087ff">InputLayer</span>)      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">7</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)           │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ lstm_2 (<span style="color: #0087ff; text-decoration-color: #0087ff">LSTM</span>)                   │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">7</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)          │        <span style="color: #00af00; text-decoration-color: #00af00">16,896</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ lstm_3 (<span style="color: #0087ff; text-decoration-color: #0087ff">LSTM</span>)                   │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">32</span>)             │        <span style="color: #00af00; text-decoration-color: #00af00">12,416</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dropout_5 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)             │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">32</span>)             │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_17 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)              │            <span style="color: #00af00; text-decoration-color: #00af00">33</span> │
└─────────────────────────────────┴────────────────────────┴───────────────┘
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">29,345</span> (114.63 KB)
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">29,345</span> (114.63 KB)
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">save_weights_at</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;keras_models&#39;</span><span class="p">,</span> <span class="s1">&#39;PRSA_data_PM2.5_LSTM_weights.</span><span class="si">{epoch:02d}</span><span class="s1">-</span><span class="si">{val_loss:.4f}</span><span class="s1">.keras&#39;</span><span class="p">)</span>
<span class="n">save_best</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">save_weights_at</span><span class="p">,</span> <span class="n">monitor</span><span class="o">=</span><span class="s1">&#39;val_loss&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                            <span class="n">save_best_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">save_weights_only</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;min&#39;</span><span class="p">,</span> <span class="n">save_freq</span><span class="o">=</span><span class="s1">&#39;epoch&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">history_pm25_LSTM</span> <span class="o">=</span> <span class="kc">None</span>

<span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="s1">&#39;history_pm25_LSTM.joblib&#39;</span><span class="p">):</span>
    <span class="n">history_pm25_LSTM</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="s1">&#39;history_pm25_LSTM.joblib&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;El archivo &#39;history_pm25_LSTM.joblib&#39; ya existe. Se ha cargado el historial del entrenamiento.&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">history_pm25_LSTM</span> <span class="o">=</span> <span class="n">ts_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
             <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">save_best</span><span class="p">],</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">),</span>
             <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">);</span>
    <span class="n">dump</span><span class="p">(</span><span class="n">history_pm25_LSTM</span><span class="o">.</span><span class="n">history</span><span class="p">,</span> <span class="s1">&#39;history_pm25_LSTM.joblib&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;El entrenamiento se ha completado y el historial ha sido guardado en &#39;history_pm25_LSTM.joblib&#39;.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/20
2069/2069 - 17s - 8ms/step - loss: 0.0191 - val_loss: 0.0117
Epoch 2/20
2069/2069 - 15s - 7ms/step - loss: 0.0151 - val_loss: 0.0125
Epoch 3/20
2069/2069 - 14s - 7ms/step - loss: 0.0151 - val_loss: 0.0127
Epoch 4/20
2069/2069 - 15s - 7ms/step - loss: 0.0149 - val_loss: 0.0129
Epoch 5/20
2069/2069 - 16s - 8ms/step - loss: 0.0150 - val_loss: 0.0118
Epoch 6/20
2069/2069 - 14s - 7ms/step - loss: 0.0149 - val_loss: 0.0126
Epoch 7/20
2069/2069 - 15s - 7ms/step - loss: 0.0147 - val_loss: 0.0117
Epoch 8/20
2069/2069 - 15s - 7ms/step - loss: 0.0148 - val_loss: 0.0125
Epoch 9/20
2069/2069 - 15s - 7ms/step - loss: 0.0148 - val_loss: 0.0117
Epoch 10/20
2069/2069 - 15s - 7ms/step - loss: 0.0147 - val_loss: 0.0120
Epoch 11/20
2069/2069 - 15s - 7ms/step - loss: 0.0147 - val_loss: 0.0123
Epoch 12/20
2069/2069 - 15s - 7ms/step - loss: 0.0147 - val_loss: 0.0122
Epoch 13/20
2069/2069 - 15s - 7ms/step - loss: 0.0146 - val_loss: 0.0117
Epoch 14/20
2069/2069 - 15s - 7ms/step - loss: 0.0146 - val_loss: 0.0121
Epoch 15/20
2069/2069 - 15s - 7ms/step - loss: 0.0147 - val_loss: 0.0120
Epoch 16/20
2069/2069 - 15s - 7ms/step - loss: 0.0146 - val_loss: 0.0117
Epoch 17/20
2069/2069 - 14s - 7ms/step - loss: 0.0146 - val_loss: 0.0129
Epoch 18/20
2069/2069 - 15s - 7ms/step - loss: 0.0147 - val_loss: 0.0123
Epoch 19/20
2069/2069 - 15s - 7ms/step - loss: 0.0146 - val_loss: 0.0121
Epoch 20/20
2069/2069 - 16s - 8ms/step - loss: 0.0147 - val_loss: 0.0117
El entrenamiento se ha completado y el historial ha sido guardado en &#39;history_pm25_LSTM.joblib&#39;.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_dir</span> <span class="o">=</span> <span class="s1">&#39;keras_models&#39;</span>
<span class="n">files</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">model_dir</span><span class="p">)</span>
<span class="n">pattern</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;PRSA_data_PM2.5_LSTM_weights\.(\d+)-([\d\.]+)\.keras&quot;</span>
    
<span class="n">best_val_loss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">)</span>
<span class="n">best_model_file</span> <span class="o">=</span> <span class="kc">None</span>
<span class="n">best_model</span> <span class="o">=</span> <span class="kc">None</span>

<span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">files</span><span class="p">:</span>
    <span class="n">match</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">pattern</span><span class="p">,</span> <span class="n">file</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">match</span><span class="p">:</span>
        <span class="n">epoch</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">match</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">val_loss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">match</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">val_loss</span> <span class="o">&lt;</span> <span class="n">best_val_loss</span><span class="p">:</span>
            <span class="n">best_val_loss</span> <span class="o">=</span> <span class="n">val_loss</span>
            <span class="n">best_model_file</span> <span class="o">=</span> <span class="n">file</span>

<span class="k">if</span> <span class="n">best_model_file</span><span class="p">:</span>
    <span class="n">best_model_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">model_dir</span><span class="p">,</span> <span class="n">best_model_file</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Cargando el mejor modelo: </span><span class="si">{</span><span class="n">best_model_file</span><span class="si">}</span><span class="s2"> con val_loss: </span><span class="si">{</span><span class="n">best_val_loss</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">best_model</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="n">best_model_path</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;No se encontraron archivos de modelos que coincidan con el patrón.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cargando el mejor modelo: PRSA_data_PM2.5_LSTM_weights.07-0.0116.keras con val_loss: 0.0116
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">preds</span> <span class="o">=</span> <span class="n">best_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>
<span class="n">pred_pm25</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span>
<span class="n">pred_pm25</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">pred_pm25</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold">271/271</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 2ms/step
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_absolute_error</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mae</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">df_val</span><span class="p">[</span><span class="s1">&#39;pm2.5&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">7</span><span class="p">:],</span> <span class="n">pred_pm25</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;MAE for the validation set:&#39;</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">mae</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>MAE for the validation set: 11.5552
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Hemos utilizado <code class="docutils literal notranslate"><span class="pre">keras.callbacks.ModelCheckpoint</span></code> como callback para rastrear el <code class="docutils literal notranslate"><span class="pre">MSE</span></code> del modelo en el conjunto de validación y <em><strong>guardar los pesos de la epoch que da el mínimo MSE</strong></em>. El <code class="docutils literal notranslate"><span class="pre">R-cuadrado</span></code> del mejor modelo para el conjunto de validación es de 0.9959. Los primeros cincuenta valores reales y predichos se muestran en la siguiente figura</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">5.5</span><span class="p">,</span> <span class="mf">5.5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span> <span class="n">df_val</span><span class="p">[</span><span class="s1">&#39;pm2.5&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">7</span><span class="p">:</span><span class="mi">56</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;*&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span> <span class="n">pred_pm25</span><span class="p">[:</span><span class="mi">50</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;Actual&#39;</span><span class="p">,</span><span class="s1">&#39;Predicted&#39;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Actual vs Predicted pm2.5&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;pm2.5&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Index&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/ff08101d80d618b234816755c333c2ed13bc9cf1552bec74894bf1a5201a660d.png" src="_images/ff08101d80d618b234816755c333c2ed13bc9cf1552bec74894bf1a5201a660d.png" />
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "tf"
        },
        kernelOptions: {
            name: "tf",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'tf'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="arima_model.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">3. </span>Modelos autorregresivos integrados de media móvil</p>
      </div>
    </a>
    <a class="right-next"
       href="biblio.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">5. </span>Bibliografía</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#redes-neuronales">4.1. Redes neuronales</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradiente-descendente">4.2. Gradiente descendente</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#el-perceptron">4.3. El perceptrón</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#redes-totalmente-conectadas">4.4. Redes Totalmente Conectadas</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#el-algoritmo-de-backpropagation">4.5. El Algoritmo De Backpropagation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#el-esquema-de-backpropagation-para-gradiente-descendente">4.6. El Esquema De Backpropagation Para Gradiente descendente</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#calculo-de-gradientes">4.7. Cálculo de gradientes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#calculo-de-delta-nj-r">4.8. Cálculo de <span class="math notranslate nohighlight">\(\delta_{nj}^{r}\)</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#las-capas-ocultas">4.9. Las capas ocultas</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#redes-neuronales-recurrentes">4.10. Redes Neuronales Recurrentes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#backpropagation-en-tiempo">4.10.1. Backpropagation en tiempo</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#desvanecimiento-y-explosion-de-gradientes">4.10.2. Desvanecimiento y explosión de gradientes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementacion">4.10.3. Implementación</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#red-de-memoria-a-largo-plazo-lstm">4.11. Red de memoria a largo plazo (LSTM)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#series-de-tiempo">4.12. Series de Tiempo</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#perceptrones-multicapa">4.13. Perceptrones multicapa</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#entrenamiento-de-mlp">4.14. Entrenamiento de MLP</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mlp-para-la-prediccion-de-series-temporales">4.15. MLP para la predicción de series temporales</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lstm-para-la-prediccion-de-series-de-tiempo">4.16. LSTM para la predicción de series de tiempo</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Lihki Rubio, Ph.D.
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <p>Lihki Rubio, Ph.D. All rights reserved.</p>
</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>